[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Risk Management and Financial Institutions",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Risk Management and Financial Institutions",
    "section": "",
    "text": "Course Overview\nThis course provides a comprehensive introduction to risk management principles and their practical applications in banking and insurance sectors. Students will gain deep insights into how financial institutions identify, assess, and mitigate various types of risk while exploring the critical role risk management plays in ensuring organizational stability and success.\nThrough a blend of theoretical frameworks and real-world case studies, participants will develop the analytical skills and strategic thinking necessary to navigate the complex risk landscape of modern financial services.\n\n\nLearning Outcomes\nUpon successful completion of this course, students will be able to:\n\nMaster fundamental risk management concepts, terminology, and theoretical frameworks\nUnderstand regulatory requirements and industry best practices in financial risk management\nApply quantitative and qualitative methods to measure and evaluate risk profiles\nCompare and contrast various risk identification methodologies\nEvaluate existing risk management systems and processes within financial institutions\nPropose innovative risk mitigation strategies using financial market instruments\nAssess risk transfer mechanisms including derivatives, securitization, and reinsurance\nIntegrate risk management considerations into broader business decision-making\n\n\n\nKey Topics Covered\nThe course explores risk management from both banking and insurance perspectives, examining how these industries approach common challenges while addressing their unique risk exposures. Students will engage with current industry developments, emerging risks, and evolving regulatory landscapes that shape modern risk management practices.\n\n\nReferences\nThe whole course is based on the following textbooks:\n\nHull, J. C. 2023. Risk Management and Financial Institutions. Wiley Finance.\nPirie, W. L., and M. P. Kritzman. 2017. Derivatives. CFA Institute Investment Series.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "week_01.html",
    "href": "week_01.html",
    "title": "1  Introduction to Risk Management",
    "section": "",
    "text": "1.1 Foundational Concepts",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_01.html#foundational-concepts",
    "href": "week_01.html#foundational-concepts",
    "title": "1  Introduction to Risk Management",
    "section": "",
    "text": "1.1.1 What is Risk?\nRisk is the exposure to uncertainty that can lead to variation in outcomes, particularly adverse outcomes that result in losses or failure to achieve objectives. Simply put: risk is the possibility that bad things might happen.\nThe key distinction to understand:\n\nUncertainty means we don’t know what will happen\nRisk means we can estimate the probabilities of different outcomes\n\nWhen we talk about risk in practice, we’re actually referring to three interconnected components:\n\nRisk Driver - The underlying source of uncertainty (what could change?)\nRisk Position - The amount exposed to that uncertainty (how much is at stake?)\nRisk Exposure - The potential impact from that position (what could we gain or lose?)\n\n\n\n\n\n\n\nExample\n\n\n\nConsider a company holding ¥1,000,000 when news from Japan could move the currency by ±1%:\n\nThe risk driver is the ±1% currency movement\nThe risk position is ¥1,000,000\nThe risk exposure is ±¥10,000 potential impact\n\n\n\n\n\n1.1.2 Risk Management: Optimization, Not Elimination\nRisk management is the process of defining acceptable risk levels, measuring current risk, and adjusting exposure to achieve organizational objectives. The critical insight: risk management aims to optimize value, not minimize risk.\nWhy? Because risk and return are inseparable. We cannot directly control returns—we can only manage risk to influence expected outcomes. This leads to the fundamental principle of finance: there is no free lunch. Higher returns require accepting higher risks.\n\n\n\n\n\n\nExample\n\n\n\nConsider these investment options:\n\nGovernment Bonds: ~2-3% annual return with minimal risk\nCorporate Bonds: ~5-7% return with moderate default risk\nStartup Equity: 20%+ potential returns with high probability of total loss\n\nThe goal isn’t to avoid the startup investment—it’s to understand the risk-return tradeoff and choose appropriately based on objectives and risk tolerance.\n\n\n\n\n1.1.3 The Risk Landscape: Financial and Non-Financial Risks\nOrganizations face two broad categories of risk that frequently interact and amplify each other.\n\nFinancial Risks\nMarket Risk stems from changes in market prices and rates. Market risk includes equity price movements, interest rate changes, foreign exchange fluctuations, and commodity price volatility.\nCredit Risk is the possibility that a counterparty won’t meet their obligations. The challenge with credit risk is its rarity—defaults are infrequent events with limited historical data, requiring estimation through credit spreads, rating models, and financial ratio analysis.\nLiquidity Risk manifests in two forms:\n\nFunding liquidity risk: inability to meet payment obligations\nMarket liquidity risk: inability to exit positions without significant losses\n\nThe 2008 crisis demonstrated this dramatically when mortgage-backed securities became essentially unsellable at any reasonable price. The risk isn’t just wider bid-ask spreads (a known cost) but the uncertainty of spreads widening unpredictably, especially during stress periods.\n\n\nNon-Financial Risks\nOperational Risk arises from failed processes, people, systems, or external events. These events are typically low-frequency but high-impact, making them difficult to predict and quantify.\nStrategic Risk emerges from poor business decisions or flawed execution.\nReputation Risk can destroy value rapidly, especially in our social media age. It often results from other risk events and spreads faster than organizations can respond.\nModel Risk occurs when we use wrong models or misapply correct ones. All models simplify reality—the danger comes when we forget these limitations. Models fail in predictable ways.\nTail Risk represents extreme “black swan” events that occur more frequently than normal distributions suggest. These outlier events in the probability distribution’s tails can devastate unprepared organizations.\n\n\nThe Critical Distinction: Systematic vs. Idiosyncratic Risk\nSystematic risk affects entire markets or economies—interest rate changes, recessions, geopolitical events. It cannot be diversified away.\nIdiosyncratic risk is specific to individual companies or assets. While diversification can reduce it, the 2008 crisis showed how idiosyncratic risks can evolve into systematic ones when individual firm failures trigger system-wide contagion.\n\n\n\n1.1.4 Risk Interactions: When 1+1 = 3\nRisks rarely occur in isolation. They interact, amplify each other, and cascade through organizations in non-linear ways. The 2008 Financial Crisis provides a textbook cascade:\n\nHousing prices decline (market risk)\nMortgage defaults surge (credit risk)\n\nMBS securities become illiquid (liquidity risk)\nBanks face funding crises (funding liquidity risk)\nCounterparty concerns explode (credit risk amplification)\nSystem-wide crisis emerges (systemic risk)\n\nThis cascade illustrates wrong-way risk—when exposure increases precisely as counterparty credit quality deteriorates. Banks buying credit protection from AIG, which was itself exposed to the same mortgage risks, exemplified this fatal correlation.\n\n\n\n\n\n\nImportant\n\n\n\nThe lesson: risk models often fail to capture these interactions, making scenario planning and stress testing essential for identifying potential risk cascades before they occur.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_01.html#the-risk-management-process",
    "href": "week_01.html#the-risk-management-process",
    "title": "1  Introduction to Risk Management",
    "section": "1.2 The Risk Management Process",
    "text": "1.2 The Risk Management Process\n\n1.2.1 The Five-Step Risk Management Cycle\nRisk management isn’t a one-time exercise—it’s a continuous cycle that integrates risk considerations into every organizational decision. The process flows through five interconnected steps, each building on the previous while feeding back into a continuous loop of improvement.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis cycle operates within a broader framework that provides structure and support:\n\nRisk Governance establishes the top-down direction, ensuring risk management aligns with organizational objectives.\nRisk Infrastructure provides the people, systems, and analytical tools needed to track and quantify risks.\nPolicies and Processes translate high-level risk appetite into operational guidelines and limits.\nThe Three Lines of Defense create organizational accountability—business operations own and manage risks, risk management provides oversight, and internal audit offers independent assurance.\n\n\n\n\n\n\n\nImportant\n\n\n\nA critical insight: while organizations face countless potential risks, they’re typically driven by a small number of key risk factors. Effective risk management focuses on identifying and managing these critical drivers.\n\n\n\n\n1.2.2 Risk Identification—Finding What Could Hurt You\nThe first challenge is discovering what risks you face. This requires systematic exploration beyond the obvious, because the risks that destroy organizations are often the ones they didn’t see coming.\nEffective identification combines multiple approaches. Risk workshops bring together diverse perspectives to brainstorm potential threats. Historical loss analysis examines past failures—both internal and at peer organizations—to identify patterns. Scenario analysis asks “what could go wrong?” and follows the implications through to their logical conclusions. Risk registers provide systematic taxonomies of known risk types, while external benchmarking reveals risks other organizations have identified.\nThe challenge lies in navigating three categories of risk knowledge:\n\nKnown Knowns: Risks we understand and can quantify (market volatility, credit defaults)\nKnown Unknowns: Risks we’re aware of but can’t fully measure (cyber attack severity, pandemic impacts)\nUnknown Unknowns: “Black Swan” events we haven’t imagined\n\nThe goal isn’t to identify every conceivable risk—that’s impossible. It’s to cast a wide enough net to catch the risks that could materially impact objectives while remaining alert to signals of emerging unknown risks.\n\n\n1.2.3 Risk Assessment—Prioritizing What Matters\nNot all risks deserve equal attention. Assessment evaluates both the likelihood and potential impact of identified risks to focus resources where they matter most.\nThe classic risk matrix provides a simple but powerful prioritization tool:\n\n\n\nLikelihood\nLow Impact\nMedium Impact\nHigh Impact\n\n\n\n\nHigh\nMedium\nHigh\nCritical\n\n\nMedium\nLow\nMedium\nHigh\n\n\nLow\nLow\nLow\nMedium\n\n\n\nHigh-likelihood, high-impact risks demand immediate attention. Low-likelihood, low-impact risks might be accepted as a cost of doing business. The challenging decisions involve the corners: high-impact but unlikely events (prepare contingency plans) and likely but low-impact events (automate controls).\nAssessment isn’t purely mechanical. It requires judgment about probability and impact, consideration of risk velocity (how quickly a risk materializes), and understanding of risk interconnections that could amplify impacts.\n\n\n1.2.4 Risk Measurement—Quantifying the Unquantifiable\nMeasurement transforms vague concerns into actionable metrics. Different risk types require different measurement approaches, and choosing the right metric is crucial for effective management.\nFoundation metrics apply across risk types:\n\nProbability distributions show the range of potential outcomes\nStandard deviation measures dispersion, though it assumes normal distributions that rarely exist in practice\nValue-at-Risk (VaR) estimates maximum likely loss at a confidence level\nConditional VaR reveals average losses beyond the VaR threshold\nStress testing explores performance under extreme scenarios\n\nSpecialized metrics target specific risks:\n\nFor market risk, we use beta to measure systematic risk exposure, duration for interest rate sensitivity, and the “Greeks” for derivatives—delta (price sensitivity), gamma (delta’s rate of change), vega (volatility sensitivity), theta (time decay), and rho (interest rate sensitivity).\nFor credit risk, we estimate Probability of Default (PD), Loss Given Default (LGD), and Exposure at Default (EAD). Credit ratings synthesize multiple factors into standardized risk grades.\n\nKey Risk Indicators (KRIs) serve as early warning signals—metrics that change before risks materialize, providing time to respond.\nThe challenge in measurement is balancing precision with practicality. Perfect measurement is impossible, but approximate measurement that acknowledges uncertainty is far better than no measurement at all.\n\n\n1.2.5 Risk Management—Choosing Your Response\nOnce risks are identified, assessed, and measured, organizations must decide how to respond. The four fundamental strategies can be combined for optimal results:\nRisk Avoidance eliminates risk by not engaging in the activity. A company might avoid entering politically unstable markets or decline to develop products with uncertain liability. This provides certainty but foregoes potential opportunities. The key question: is what we’re giving up worth the risk we’re avoiding?\nRisk Acceptance consciously retains risk, either because it’s unavoidable or because the cost of mitigation exceeds potential losses. This takes two forms. Self-insurance means bearing losses as they occur or setting aside reserves. Diversification spreads risk across uncorrelated exposures, reducing concentration while accepting that some risk remains.\nRisk Transfer shifts risk to parties better equipped to bear it. Insurance is the classic mechanism—insurers pool uncorrelated risks from many sources, using the law of large numbers to make aggregate losses predictable. The limitation: insurance becomes expensive or unavailable for risks that affect many parties simultaneously (pandemic, financial crisis).\nRisk Shifting uses derivatives to alter risk distribution without transferring ownership. A company might use currency forwards to lock in exchange rates or buy options to limit downside while preserving upside. The choice between forwards (obligation) and options (right but not obligation) depends on whether you want certainty or flexibility.\nThe decision process starts with a fundamental question: is the risk within our appetite? If yes, we optimize management costs. If no, we must reduce exposure through some combination of the four strategies, considering costs, benefits, risk tolerance, and operational capabilities.\n\n\n1.2.6 Risk Monitoring—Maintaining Vigilance\nRisk management isn’t “set and forget.” Continuous monitoring ensures controls remain effective and strategies stay aligned with changing conditions.\nEffective monitoring tracks KRIs against predetermined limits, providing early warning when risks approach unacceptable levels. It regularly updates risk assessments as business conditions change, tests control effectiveness through audits and reviews, runs stress scenarios to verify resilience, and adjusts strategies based on lessons learned.\nRisk reporting transforms monitoring data into actionable intelligence. Effective reports share six characteristics:\n\nTimely—delivered frequently enough for decision-making, which varies by risk type (market risk might need daily reports, strategic risk quarterly)\nAccurate—based on validated data and calculations stakeholders can trust\nRelevant—focused on material risks for the specific audience, avoiding information overload\nClear—understandable by intended recipients, avoiding unnecessary jargon\nActionable—enabling informed decisions with clear implications and recommendations\nComprehensive—covering all material risk areas without gaps that could hide emerging problems\n\nThe monitoring phase feeds back into identification, creating a continuous cycle of improvement. New risks emerge, existing risks evolve, and yesterday’s controls become today’s vulnerabilities. Only through constant vigilance can organizations maintain resilience in an uncertain world.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_01.html#governance-and-organization",
    "href": "week_01.html#governance-and-organization",
    "title": "1  Introduction to Risk Management",
    "section": "1.3 Governance and Organization",
    "text": "1.3 Governance and Organization\nRisk governance transforms risk management from compliance burden into strategic advantage through clear organizational structure and accountability. The hierarchy ensures risk awareness flows from boardroom to operations while information travels upward to inform decisions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRisk appetite defines what risks an organization willingly accepts pursuing objectives, while risk tolerance sets acceptable variation boundaries. Organizations operationalize these through cascading limits:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmart organizations set appetite well below capacity, maintaining buffers for unexpected shocks.\nRisk budgeting transforms abstract risk appetite into concrete allocations across the organization. Like financial budgeting allocates scarce capital, risk budgeting allocates scarce risk capacity to its most productive uses.\nRisk culture—shared attitudes and behaviors around risk—determines whether governance structures actually influence behavior. Strong culture exhibits four pillars: leadership tone from the top, clear accountability with consequences, effective challenge without retaliation, and risk-aligned incentives favoring long-term value over short-term profits. Poor culture reveals itself through profit-at-any-cost focus, suppressed concerns, and risk management viewed as impediment rather than enabler.\nModern risk governance creates value by optimizing risk-return trade-offs, enabling profitable activities others fear, and building competitive advantage through superior risk management. Success requires integration with business strategy, dynamic adaptation to changing conditions, and investment in capabilities matching strategic importance. The goal: risk management so embedded it becomes invisible yet omnipresent, influencing every decision without constraining innovation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_01.html#modern-applications-and-enterprise-risk-management",
    "href": "week_01.html#modern-applications-and-enterprise-risk-management",
    "title": "1  Introduction to Risk Management",
    "section": "1.4 Modern Applications and Enterprise Risk Management",
    "text": "1.4 Modern Applications and Enterprise Risk Management\nEnterprise Risk Management (ERM) represents a paradigm shift from isolated risk silos to holistic risk management. Traditional approaches created blind spots at intersections where risks interact and amplify—credit crises become liquidity crises become market crises. ERM acknowledges that organizations are driven by a small number of key risk factors cutting across traditional boundaries.\nERM rests on five interconnected pillars:\n\nTargets align risk-taking with strategic objectives through enterprise-wide appetite statements and risk-adjusted metrics.\nStructure creates accountability through board oversight and cross-functional committees.\nIdentification and Metrics establish common risk language and aggregate measures.\nStrategies optimize risk-return trade-offs at portfolio level.\nCulture embeds risk awareness as organizational mindset rather than isolated function.\n\nModern risk management looks forward through stress testing that explores extreme conditions. Sensitivity analysis changes single factors, scenario analysis combines multiple changes, and reverse stress testing identifies what could cause failure. Testing uses both historical scenarios (2008 crisis, COVID-19) and hypothetical events (cyber attacks, climate catastrophes) to reveal vulnerabilities that normal metrics miss.\nNew risk categories demand attention as the world evolves. Climate risk operates through physical damage, transition costs, and stranded assets. Cyber risk grows more sophisticated with cascading systemic impacts. Artificial intelligence creates accountability gaps and amplifies model risk. Regulatory evolution accelerates with dynamic requirements across multiple frameworks (Basel III, Solvency II, COSO).\n\n\n\n\n\n\nImportant\n\n\n\nThe fundamental insight: ERM transforms risk from threat to opportunity, from cost to investment, from necessary evil to competitive advantage. Organizations that master ERM don’t eliminate uncertainty—they harness it for sustainable value creation through intelligent risk-taking.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_01.html#practice-questions-and-problems",
    "href": "week_01.html#practice-questions-and-problems",
    "title": "1  Introduction to Risk Management",
    "section": "1.5 Practice Questions and Problems",
    "text": "1.5 Practice Questions and Problems\n\nExercise 1: Risk Components\nIdentify the risk components:\n\nRisk Driver: What uncertain factor is creating the risk?\nRisk Position: What is your stake/exposure in this situation?\nRisk Exposure: What is the dollar amount potentially at stake?\n\n\n\n\n\n\n\nScenario A: Currency Risk\n\n\n\nYou work for a German company that just signed a contract to receive $1,000,000 from a US client in 3 months. Today’s exchange rate is €1 = $1.10, but it could move to anywhere between $1.00 and $1.20.\n\n\n\n\n\n\n\n\nScenario B: Interest Rate Risk\n\n\n\nYour company has a €5,000,000 loan with a variable interest rate, currently at 4%. The rate is reviewed every 6 months and could move ±2%.\n\n\nBased on these two examples, how do the three risk components (driver, position, exposure) work together to create overall risk?\n\n\nExercise 2: Risk Identification\nReview the real events from recent years provided below. For each event, complete the following:\n\nIdentify all types of risks present in each event. Consider multiple perspectives - what could potentially go wrong for various stakeholders involved?\nClassify the identified risks into these four categories:\n\nFinancial/Market Risks\nOperational/Process Risks\nHuman/Behavioral Risks\nTechnology/System Risks\n\n\nThis exercise helps develop pattern recognition skills for identifying and categorizing different risk types across real-world scenarios.\n\n\n\n\n\n\nEvent 1: Silicon Valley Bank Collapse (March 2023)\n\n\n\nSVB focused heavily on tech startups and held many long-term government bonds. When interest rates rose, bond values fell. The bank announced losses, customers panicked, and $42 billion was withdrawn in one day via mobile banking.\n\n\n\n\n\n\n\n\nEvent 2: Ever Given Ship (March 2021)\n\n\n\nA container ship got stuck in the Suez Canal for 6 days, blocking 12% of global trade. Oil prices rose, supply chains were disrupted, and some companies couldn’t deliver products.\n\n\n\n\n\n\n\n\nEvent 3: GameStop Stock Surge (January 2021)\n\n\n\nReddit users coordinated to buy GameStop stock, driving the price from $20 to $347. Hedge funds that bet against the stock lost billions. Some trading apps restricted purchases.\n\n\n\n\nExercise 3: Risk Dominos\nPick ONE of the scenarios below and trace how problems might spread. Think about how one problem leads to the next:\n\nWeek 1: Original event happens\nWeek 2: What happens next?\nWeek 3: What new problems emerge?\nWeek 4: How does it spread further?\nWeek 5: What’s the final impact?\n\nCritical Thinking Questions:\n\nWhich step was hardest to predict?\nAt what point could the company have stopped the cascade?\nWhat does this tell you about managing risk?\n\n\n\n\n\n\n\nScenario A: Cyber Attack\n\n\n\nA major bank’s computer systems are hacked, and customer data is stolen.\n\n\n\n\n\n\n\n\nScenario B: CEO Scandal\n\n\n\nThe CEO of a major company is arrested for fraud.\n\n\n\n\n\n\n\n\nScenario C: Factory Fire\n\n\n\nYour company’s main manufacturing plant burns down.\n\n\n\n\nExercise 4: The Coffee Shop Dilemma\nYou’re opening a coffee shop near campus. You’ve identified these potential problems:\n\nCompetitor opens next door: High chance, medium impact\nCoffee prices double: Low chance, high impact\nStudents switch to energy drinks: Medium chance, medium impact\nYour espresso machine breaks: High chance, low impact\nCampus shuts down (like COVID): Very low chance, very high impact\n\nSuggest suitable solutions:\n\nRank these risks from 1-5 (1 = deal with first, 5 = worry about last)\nFor your top 3 risks, what would you do and why? For each one, you can:\n\nAvoid it (don’t do things that create the risk)\nAccept it (live with it, maybe save money just in case)\nTransfer it (get someone else to bear the risk - like insurance)\nReduce it (take action to make it less likely or less damaging)\n\n\n\n\nExercise 5: The Risk Management System\nImagine you’re hired as “Chief Risk Officer” for a tech startup. The CEO says: “Just make sure nothing bad happens to us.”\n\nWhat would you tell the CEO about their request? What’s realistic and what isn’t?\n\nThe CEO is more reasonable now. They want you to “manage risks systematically.” Design your approach:\n\nWhat’s the very first thing you’d do?\nWhat comes next?\nHow would you measure things?\nWhat actions would you take?\nHow would you keep watching?\n\nWho should be responsible for risk management in the company?\n\nJust the risk manager\nSenior management\nAll employees\nBoard of directors\nExternal consultants\n\n\n\nExercise 6: Putting It All Together\nThink about everything you’ve worked through today. In your own words, define:\n\nRisk\nRisk Management\n\nThink of a major financial decision you’ve made (or will make soon) - buying a car, choosing a university, taking a job, etc.\n\nWhat risks did/do you face?\nHow did/will you manage them?\n\nYou’re in a job interview for a finance position. The interviewer asks: “Why should companies care about risk management?” Explain your opinion in 60s.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_02.html",
    "href": "week_02.html",
    "title": "2  Revision of Basic Concepts",
    "section": "",
    "text": "2.1 Introduction\nThis session reviews core concepts from your previous coursework, now examined specifically through a risk management lens. These aren’t just academic theories—they’re the foundation of how practitioners assess portfolio exposures, set capital requirements, and evaluate systemic vulnerabilities. Understanding these concepts deeply is crucial for grasping why certain risks materialize and how traditional models can fail under stress.\nDon’t just find answers—understand why these concepts matter and how they break down. Be able to explain your findings to the class, including points of disagreement or uncertainty. Recent events (from the 2008 crisis to SVB’s 2023 collapse) show that even well-understood principles can fail spectacularly.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#introduction",
    "href": "week_02.html#introduction",
    "title": "2  Revision of Basic Concepts",
    "section": "",
    "text": "Use available resources: Textbook, your previous notes, online sources\nAI tools can help with general concepts, but verify specific facts and formulas\nFocus on connections between concepts and their practical limitations\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember: In risk management, questioning assumptions is essential. The most dangerous phrase remains: “We’ve always done it this way!”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#risk-return-trade-offs-and-portfolio-theory",
    "href": "week_02.html#risk-return-trade-offs-and-portfolio-theory",
    "title": "2  Revision of Basic Concepts",
    "section": "2.2 Risk-Return Trade-offs and Portfolio Theory",
    "text": "2.2 Risk-Return Trade-offs and Portfolio Theory\n\nHow does the correlation between assets affect portfolio risk, and why is perfect negative correlation rare in practice?\nWhat changes when we introduce a risk-free asset to the efficient frontier? How does this lead to the concept of the market portfolio?\nIn practice, why might investors choose portfolios that are not on the efficient frontier?\nHow do leverage constraints affect the risk-return choices available to different types of investors?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#the-capital-asset-pricing-model",
    "href": "week_02.html#the-capital-asset-pricing-model",
    "title": "2  Revision of Basic Concepts",
    "section": "2.3 The Capital Asset Pricing Model",
    "text": "2.3 The Capital Asset Pricing Model\n\nWhat distinguishes systematic from nonsystematic risk? Why can only systematic risk command a risk premium?\nHow should we interpret negative beta assets, and what role might they play in portfolio construction?\nIf CAPM holds perfectly, what should be the weighted average alpha across all investors? Why?\nWhat are the main violations of CAPM assumptions you observe in real markets, and do they invalidate the model’s usefulness?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#banking-and-risk-management",
    "href": "week_02.html#banking-and-risk-management",
    "title": "2  Revision of Basic Concepts",
    "section": "2.4 Banking and Risk Management",
    "text": "2.4 Banking and Risk Management\n\nCompare the risks faced by commercial banks versus investment banks. How did the repeal of Glass-Steagall change these risk profiles?\nWhat is the “originate-to-distribute” model, and how did it contribute to the 2007-2008 crisis?\nHow do Basel III capital requirements differ from Basel II? What gaps were they designed to address?\nWhy might a bank choose to use internal models versus standardized approaches for calculating regulatory capital?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#insurance-and-pension-plans",
    "href": "week_02.html#insurance-and-pension-plans",
    "title": "2  Revision of Basic Concepts",
    "section": "2.5 Insurance and Pension Plans",
    "text": "2.5 Insurance and Pension Plans\n\nHow do life insurance companies manage the opposing risks of mortality and longevity across their product lines?\nWhat makes property-casualty insurance fundamentally different from life insurance in terms of risk management?\nHow do CAT bonds work, and why might investors be interested in them despite their risks?\nWhat is the definition of moral hazard and adverse selection?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#fund-management",
    "href": "week_02.html#fund-management",
    "title": "2  Revision of Basic Concepts",
    "section": "2.6 Fund Management",
    "text": "2.6 Fund Management\n\nWhat are the key differences in risk profile between open-end funds, closed-end funds, and ETFs?\nHow do hedge fund fee structures (e.g., “2 and 20”) create option-like payoffs for managers? What conflicts might this create?\nWhat is the evidence on persistence in fund manager performance? What does this imply for investors?\nCompare the risks of late trading, market timing, and front running. How do regulations attempt to prevent these?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#derivatives-markets-and-central-clearing",
    "href": "week_02.html#derivatives-markets-and-central-clearing",
    "title": "2  Revision of Basic Concepts",
    "section": "2.7 Derivatives Markets and Central Clearing",
    "text": "2.7 Derivatives Markets and Central Clearing\n\nHow does daily settlement in futures markets differ from end-of-period settlement in forward markets? What are the risk implications?\nWhat is the role of central counterparties (CCPs), and how do they change counterparty risk?\nHow has netting evolved from bilateral agreements to multilateral clearing? What are the benefits and limitations?\nPost-2008, what OTC derivatives must be centrally cleared, and why might some market participants prefer bilateral clearing?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#systemic-risk-and-the-2007-2008-crisis",
    "href": "week_02.html#systemic-risk-and-the-2007-2008-crisis",
    "title": "2  Revision of Basic Concepts",
    "section": "2.8 Systemic Risk and the 2007-2008 Crisis",
    "text": "2.8 Systemic Risk and the 2007-2008 Crisis\n\nWhat factors transformed the U.S. housing bubble into a global financial crisis?\nHow did securitization and CDOs amplify rather than distribute risk?\nWhat role did credit rating agencies play, and what conflicts of interest existed?\nCompare “too big to fail” banks with “too interconnected to fail” - which poses greater systemic risk?\nWhat key regulatory changes (Dodd-Frank, Volcker Rule, Basel III) emerged from the crisis, and what risks might they have missed?\n\n\n\n\n\n\n\nCase Study Exercise\n\n\n\nSelect one major financial institution failure from 2007-2008 (e.g., Lehman Brothers, Bear Stearns, AIG, Northern Rock). Analyze:\n\nWhat risks were inadequately managed?\nHow did their failure impact other institutions?\nWhat regulatory changes specifically addressed their type of failure?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_03.html",
    "href": "week_03.html",
    "title": "3  Market Risk I: Value at Risk",
    "section": "",
    "text": "Block A: What Risk Number Do We Need?\nYou’ve just been hired as a risk analyst at an equity trading desk. Your manager hands you a CSV file with the last 250 days of S&P 500 returns and says:\nDownload Dataset (250 Days)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Market Risk I: Value at Risk</span>"
    ]
  },
  {
    "objectID": "week_03.html#block-a-what-risk-number-do-we-need",
    "href": "week_03.html#block-a-what-risk-number-do-we-need",
    "title": "3  Market Risk I: Value at Risk",
    "section": "",
    "text": "The CEO will ask me tomorrow morning: ‘How much could we lose on our $100 million SPY position in a day?’ I need a single number by 4 PM.\n\n\n\nActivity 1: The Variance Approach\nLoad the SPY returns data (SPY_returns_250.csv) - one year of daily returns, small enough to inspect in Excel:\nTask 1.1: Calculate the standard deviation of all 250 returns. What is the daily volatility?\nTask 1.2: If we define risk as “2-sigma events,” what loss level should we report for our $100M position?\nTask 1.3: Find the best and worst daily returns in the dataset. Are they roughly symmetric around the mean? What does this tell you about using standard deviation for risk measurement?\n\n\n\n\n\n\nP/L Convention\n\n\n\nWe use Profit/Loss (P/L) data where:\n\nPositive = profits (good)\nNegative = losses (bad)\n\nFor a $100M position:\n\nA -2% return means we lost $2 million\nA +1.5% return means we gained $1.5 million\n\nThis matches trader thinking and system reports.\n\n\n\n\nActivity 2: Discovery Through Sorting\nTask 2.1: Open SPY_returns_250.csv and sort the returns column from worst to best. What are the 5 worst daily returns?\nTask 2.2: For 95% confidence with 250 observations:\n\nHow many days would you allow as “exceptional bad days”?\nWhat loss value sits at this cutoff?\nFor our $100M position, what dollar loss does this represent?\n\nTask 2.3: Compare your answer from Task 2.2 with the “2-sigma” approach from Task 1.2. Which gives a larger loss estimate? Why might they differ?\n\n\nTheory: Formalizing Value-at-Risk\n\n\n\n\n\n\nVaR Definition\n\n\n\nValue-at-Risk (VaR) is the loss threshold that will not be exceeded with \\(\\alpha\\)% probability over a specified time horizon \\(h\\).\n\n\nMathematical Definition:\nFor confidence level \\(\\alpha \\in (0,1)\\) and loss random variable \\(L\\): \\[\\text{VaR}_\\alpha = \\inf\\{l \\in \\mathbb{R} : P(L &gt; l) \\leq 1-\\alpha\\}\\]\n\nVaR is the smallest loss value \\(l\\) such that the probability of exceeding \\(l\\) is at most \\((1-\\alpha)\\)\n\nEquivalently, using the loss distribution’s quantile function: \\[\\text{VaR}_\\alpha = F_L^{-1}(1-\\alpha)\\]\nWhere \\(F_L^{-1}\\) is the inverse cumulative distribution function (CDF), also called the quantile function.\n\nVaR is the \\((1-\\alpha)\\)-quantile of the loss distribution\n\nThree essential components:\n\nConfidence level (\\(\\alpha\\)): The probability that losses will not exceed VaR\nTime horizon (\\(h\\)): The period over which losses are measured\nLoss amount: The monetary value at risk\n\nInterpretation:\n\n“1-day 95% VaR = $X” means we expect to lose less than $X on 95% of days\nOn 5% of days (about once per month), we expect to lose more than $X.\n\n\n\nActivity 3: Quick Application\nTask 3.1: Interpretation Check:\n\nIf your 1-day 99% VaR is $4M, how often do you expect to exceed this loss?\nDoes VaR tell us the maximum possible loss?\nWhy might the CEO prefer one number (VaR) rather than the full distribution?\n\nTask 3.2: Using your sorted SPY returns, calculate the 99% VaR for the $100M position:\n\nWhich observation should you use?\nWhat is the corresponding return?\nWhat is the VaR in dollars?\n\nTask 3.3: Look at your 99% VaR calculation:\n\nIs this enough data to be confident about tail risk?\nWhat could go wrong with such limited tail observations?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Market Risk I: Value at Risk</span>"
    ]
  },
  {
    "objectID": "week_03.html#block-b-two-ways-to-calculate-var",
    "href": "week_03.html#block-b-two-ways-to-calculate-var",
    "title": "3  Market Risk I: Value at Risk",
    "section": "Block B: Two Ways to Calculate VaR",
    "text": "Block B: Two Ways to Calculate VaR\nNow we’ll use 4 years of SPY data (SPY_returns_1000.csv) for a more robust analysis.\n  Download Dataset (1000 Days) \n\nTheory: Historical Simulation vs. Parametric Approach\nLet’s calculate VaR using the historical simulation method.\n\n\n\n\n\n\nHistorical Simulation\n\n\n\nStep 1: Sort all daily returns from worst to best\nStep 2: Determine the position for your chosen confidence level \\(\\alpha\\)%:\n\nNumber of exceptions allowed: \\(n \\times (1-\\alpha)\\)\nRound conservatively to find the position \\(k\\)\nThe \\(k\\)-th worst observation is your VaR\n\nStep 3: Convert the return to dollar loss for your portfolio value\n\n\nNow let’s calculate VaR assuming returns follow a normal distribution.\n\n\n\n\n\n\nParametric Approach - Normal Distribution\n\n\n\nStep 1: Calculate the distribution parameters from your data:\n\nSample mean: \\(\\mu = \\frac{1}{n}\\sum_{i=1}^{n} r_i\\)\nSample standard deviation: \\(\\sigma = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(r_i - \\mu)^2}\\)\n\nStep 2: Apply the parametric VaR formula using the appropriate z-score: \\[\\text{VaR}_\\alpha = -(\\mu + \\sigma \\times z_{1-\\alpha})\\]\nwhere \\(z_{1-\\alpha}\\) is the standard normal quantile\nTo find \\(z_{1-\\alpha}\\), you can use the Excel function NORM.INV(1-α, 0, 1)\n\n\n\n\nActivity 4: When Methods Diverge\nLet’s systematically compare the two main VaR approaches.\nTask 4.1: Create a comparison table:\n\nConfidence levels = 90%, 95%, 97.5%, 99%, 99.9%\nCalculate VaR using historical simulation and parametric approach\n\nTask 4.2: Discussion Questions\n\nAt which confidence levels do the methods diverge most?\nDoes the parametric 99.9% VaR exceed the worst historical loss? What does this imply?\nWhich method would you trust more for extreme events?\n\n\n\nTheory: Time Scaling and the Square-Root Rule\n\n\n\n\n\n\nScaling VaR Across Time Horizons\n\n\n\nThe Square-Root Rule: Under specific assumptions, VaR scales with the square root of time:\n\\[\\text{VaR}_{h\\text{-day}} = \\text{VaR}_{1\\text{-day}} \\times \\sqrt{h}\\]\nRequired assumptions:\n\nReturns are independent across days (no autocorrelation)\nReturns are identically distributed (stable volatility)\nNo mean reversion or momentum\n\n\n\n\n\nActivity 5: Square-Root Rule\nTask 5.1: Test the square-root rule with real data\n\nCalculate 10-day returns\nCalculate actual 10-day 99% VaR\nCalculate 10-day 99% VaR using the square-root rule from daily returns\nWhen might this scaling break down?\n\nTask 5.2: A risk manager says:\n\nOur 1-day VaR is $5M, so our annual VaR is \\(\\$5M \\times \\sqrt{250} = \\$79M\\).\n\n\nWhat’s wrong with this logic?\n\n\n\n\n\n\n\nKey Insight\n\n\n\nThe square-root rule is widely used but fragile. In crisis periods when you need VaR most, the independence assumption fails spectacularly as correlations spike and volatility clusters.\n\n\n\n\nActivity 6: Discussion\nTask 6.1: You now have several VaR numbers from different methods. How do you decide which to report to the CEO?\nTask 6.2: What key information is still missing from any VaR number?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Market Risk I: Value at Risk</span>"
    ]
  },
  {
    "objectID": "week_03.html#block-c-when-var-fails",
    "href": "week_03.html#block-c-when-var-fails",
    "title": "3  Market Risk I: Value at Risk",
    "section": "Block C: When VaR Fails",
    "text": "Block C: When VaR Fails\n\nActivity 7: The Diversification Paradox\nConsider two corporate bonds in your portfolio:\n\n\n\n\nBond A\nBond B\n\n\n\n\nNotional\n$100M\n$100M\n\n\nDefault Probability\n0.6%\n0.6%\n\n\nRecovery if Default\n$0\n$0\n\n\nMutual Correlation\n0 (independent)\n0 (independent)\n\n\n\nTask 7.1: Calculate 99% VaR for Bonds A and B separately\n\nIn 99% of scenarios, what happens?\nIn 1% of scenarios, what is the loss?\nWhat is the 99% VaR value?\n\nTask 7.2: Calculate 99% VaR for the combined portfolio (A + B)\n\nProbability that neither defaults?\nProbability that at least one defaults?\nProbability that both default?\nWhat is the 99% VaR value?\n\nTask 7.3: Compare your results\nTask 7.4: Your bank could split the portfolio into two separate legal entities, each holding one bond. What happens to regulatory capital requirements if capital is based on VaR?\nTask 7.5: A trader discovers that selling deep out-of-the-money options shows zero VaR most days. Should you be concerned? Why?\nTask 7.6: You’re allocating risk limits to two trading desks. If Desk A has VaR limit of $10M and Desk B has $8M, can you be sure the combined risk is less than $18M?\n\n\n\n\n\n\nReal-world consideration\n\n\n\nThis property contributed to the 2008 crisis. Banks created structures that minimized VaR while concentrating tail risk. When the “impossible” happened, multiple firms failed simultaneously.\n\n\n\n\nTheory: Coherent Risk Measure\n\n\n\n\n\n\nCoherent Risk Measure Axioms\n\n\n\nA risk measure \\(R\\) is called coherent if it satisfies four axioms:\n\nSubadditivity (Diversification should reduce risk) \\[R(L_1 + L_2) \\leq R(L_1) + R(L_2)\\]\nPositive Homogeneity (Scaling positions scales risk proportionally) \\[R(\\lambda L) = \\lambda R(L), \\text{ for every } \\lambda &gt; 0\\]\nMonotonicity (Bigger losses mean bigger risk) \\[R(L_1) \\leq R(L_2) \\text{ if } L_1 \\leq L_2\\]\nTranslation Invariance (Adding cash reduces risk by that amount) \\[R(L + a) = R(L) - a\\]\n\n\n\nVaR violation: VaR can fail subadditivity, especially for:\n\nPortfolios with fat-tailed distributions\nConcentrated credit exposures\nNon-linear derivatives positions\n\nKey insight: VaR satisfies only 3 of the 4 axioms. This single failure means VaR is not “coherent” in the mathematical sense.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Market Risk I: Value at Risk</span>"
    ]
  },
  {
    "objectID": "week_03.html#block-d-real-applications-and-limitations",
    "href": "week_03.html#block-d-real-applications-and-limitations",
    "title": "3  Market Risk I: Value at Risk",
    "section": "Block D: Real Applications and Limitations",
    "text": "Block D: Real Applications and Limitations\n\nActivity 8: Reflection Questions\nTask 8.1: If VaR can show that diversification increases risk, should we still use it?\nTask 8.2: How might traders “game” a VaR-based risk limit system?\nTask 8.3: What additional information would you want beyond VaR?\n\n\nCase Study: JP Morgan’s London Whale (2012)\nJP Morgan’s $6.2 billion loss illustrates how VaR model changes can mask growing risks.\nLate 2011 - Original Model:\n\nHistorical simulation VaR using full dataset\nResult: VaR = $95-132 million\nStatus: Approaching risk limits\nCIO position size: ~$157 billion notional\n\nJanuary 2012 - New “Enhanced” Model:\n\nSwitched to parametric (Gaussian) VaR approach\nUsed shorter, calmer data window\n“Simplified” correlation calculations\nResult: VaR dropped to $54-67 million overnight\nStatus: Comfortably within limits again\n\nThe Key Changes:\n\n\n\n\n\n\n\n\n\nAspect\nOld Model\nNew Model\nImpact\n\n\n\n\nMethod\nHistorical simulation\nParametric/Gaussian\nAssumes normality\n\n\nData period\nMultiple years\nShorter, recent only\nExcluded crisis volatility\n\n\nWeighting\nTime-weighted\nEqual-weighted\nReduced recent volatility impact\n\n\nImplementation\nEstablished system\nExcel spreadsheet\nIntroduced formula errors\n\n\n\nWhat Actually Happened:\n\nThe Excel error: Divided by sum instead of average of volatilities (understated risk by ~50%)\nVaR breaches: The desk exceeded its VaR limit on 71 days in Q1 2012\nActual losses: Daily losses exceeded $200M multiple times (3x the reported VaR)\nModel reversion: In April 2012, switched back to old model → VaR jumped to $129M\n\nThe Governance Failures:\n\nModel change approved without proper validation\nRisk committee not informed of methodology change\nBreaches explained away as “technical” or “model issues”\nTraders influenced risk measurement process",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Market Risk I: Value at Risk</span>"
    ]
  },
  {
    "objectID": "week_03.html#key-takeaways",
    "href": "week_03.html#key-takeaways",
    "title": "3  Market Risk I: Value at Risk",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nVaR is simple and intuitive but has critical limitations\nDifferent stakeholders need different VaR parameters\nVaR fails to capture:\n\nTail risk severity\nLiquidity risk\nModel risk\nCorrelation breakdowns\n\nReal failures (London Whale, 2008 crisis) show VaR’s limitations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Market Risk I: Value at Risk</span>"
    ]
  },
  {
    "objectID": "week_04.html",
    "href": "week_04.html",
    "title": "4  Market Risk II: Expected Shortfall",
    "section": "",
    "text": "Block A: Beyond VaR - What Happens in the Tail?\nLast week, you reported to your CEO that the 1-day 99% VaR for your $100M SPY position was $4.2M. This morning, she calls:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Risk II: Expected Shortfall</span>"
    ]
  },
  {
    "objectID": "week_04.html#block-a-beyond-var---what-happens-in-the-tail",
    "href": "week_04.html#block-a-beyond-var---what-happens-in-the-tail",
    "title": "4  Market Risk II: Expected Shortfall",
    "section": "",
    "text": "We just lost $8.7 million yesterday! You said we’d only exceed $4.2M once in 100 days. Should I fire you, or is this normal?\n\n\nActivity 1: The VaR Breach Investigation\nOpen your SPY_returns_1000.csv file from last week.\n  Download Dataset (1000 Days) \nTask 1.1: Find all daily returns worse than your 99% VaR threshold\n\nHow many observations are there?\nList the worst return in your dataset?\nWhat’s the average loss on days when VaR is breached?\n\nTask 1.2: Your colleague says:\n\nDon’t worry, VaR breaches are rare - only 1% of the time! The actual loss doesn’t matter.\n\n\nWhat critical information is your colleague missing?\n\nTask 1.3: Two portfolios both have 1-day 99% VaR of $10M:\n\nPortfolio A: Government bonds\nPortfolio B: Written put options on SPY\nOn a day when both breach their VaR, which likely loses more? Why?\n\n\n\nActivity 2: Tail Risk Discovery\nLet’s examine what happens “beyond VaR” using your SPY data.\nTask 2.1: For the worst 5% of days:\n\nCalculate the minimum loss\nCalculate the maximum loss\nCalculate the average loss\nHow does the average compare to your 95% VaR?\n\nTask 2.2: Risk Reporting Dilemma\nYou need to report risk to three stakeholders. Using your calculations:\n\n\n\n\n\n\n\n\n\nStakeholder\nTheir Question\nVaR Answer\nWhat’s Missing?\n\n\n\n\nCEO\n“What’s our typical bad day loss?”\n$4.2M\n?\n\n\nCFO\n“If things go bad, how much cash do I need?”\n$4.2M\n?\n\n\nRegulator\n“Can you survive a tail event?”\n$4.2M\n?\n\n\n\nTask 2.3: VaR Breach Scenarios\nYour risk report for last year shows:\n\n\n\n\nBank A\nBank B\n\n\n\n\n99% VaR\n$10M\n$10M\n\n\nBreach 1\n-$11M\n-$10.5M\n\n\nBreach 2\n-$12M\n-$11M\n\n\nBreach 3\n-$10.5M\n-$45M\n\n\n\n\nIs it acceptable for 99% confidence?\nWhich bank had worse tail risk?\nWhat does this tell you about VaR as a risk measure?\n\n\n\n\n\n\n\nKey Insight\n\n\n\nVaR tells you the frequency of bad events and the threshold but nothing about the severity when that threshold is breached.\nIn crises, what kills firms isn’t the frequency of bad days—it’s the magnitude of losses on those days.\n\n\n\n\nTheory: Conditional Expectation in the Tail\n\n\n\n\n\n\nExpected Shortfall Definition\n\n\n\nExpected Shortfall (ES) measures the average loss during bad days when things go worse than your VaR threshold.\nAlso called Conditional VaR (CVaR) or Expected Tail Loss (ETL), ES answers a critical question: When we have a really bad day that exceeds our VaR limit, how bad should we expect it to be?\nES is always worse than (or equal to) VaR because it’s the average of all the losses in the tail, including the very worst outcomes.\n\n\nMathematically, for confidence level \\(\\alpha\\): \\[ES_\\alpha = E[L \\mid L \\geq VaR_\\alpha]\\]\nThis is the average of all losses in the worst \\((1-\\alpha)\\) portion of the distribution, including the VaR observation itself.\nMore precisely for continuous distributions: \\[ES_\\alpha = \\frac{1}{1-\\alpha} \\int_{VaR_\\alpha}^{\\infty} L \\cdot f(L) \\, dL\\]\nFor discrete historical data with \\(n\\) observations: \\[ES_\\alpha = \\frac{1}{k} \\sum_{i=1}^{k} L_i\\] where:\n\n\\(k = \\lceil n(1-\\alpha) \\rceil\\) is the number of worst losses to average\n\\(L_1 \\geq L_2 \\geq ... \\geq L_k\\) are losses sorted from worst to best\n\\(L_k = VaR_\\alpha\\) (the k-th worst loss is the VaR)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Risk II: Expected Shortfall</span>"
    ]
  },
  {
    "objectID": "week_04.html#block-b-expected-shortfall---measuring-tail-risk",
    "href": "week_04.html#block-b-expected-shortfall---measuring-tail-risk",
    "title": "4  Market Risk II: Expected Shortfall",
    "section": "Block B: Expected Shortfall - Measuring Tail Risk",
    "text": "Block B: Expected Shortfall - Measuring Tail Risk\n\nTheory: Calculating Expected Shortfall\n\n\n\n\n\n\nHistorical Simulation Method for ES\n\n\n\nStep 1: Sort all daily returns from worst to best\nStep 2: Determine how many tail observations to include:\n\\[k = \\lceil n \\times (1-\\alpha) \\rceil\\]\nStep 3: Calculate the average of these \\(k\\) worst observations: \\[ES_\\alpha = \\frac{1}{k} \\sum_{i=1}^{k} L_i\\]\nStep 4: Convert to dollar loss for your portfolio value\n\n\n\n\n\n\n\n\nParametric Approach for ES (Normal Distribution)\n\n\n\nFor normally distributed returns:\nStep 1: Calculate distribution parameters from your data:\n\nSample mean: \\(\\mu\\)\nSample standard deviation: \\(\\sigma\\)\n\nStep 2: Apply the ES formula for normal distribution: \\[ES_\\alpha = -(\\mu - \\sigma \\times \\frac{\\phi(z_\\alpha)}{1-\\alpha})\\]\nwhere:\n\n\\(z_\\alpha\\) is the standard normal quantile at confidence \\(\\alpha\\)\n\\(\\phi(z_\\alpha)\\) is the standard normal PDF evaluated at \\(z_\\alpha\\)\n\nThe PDF can be calculated as: \\[\\phi(z_\\alpha) = \\frac{e^{-z_\\alpha^2/2}}{\\sqrt{2\\pi}}\\]\nExcel implementation:\n\nFor PDF: NORM.DIST(z_α, 0, 1, FALSE)\nFor z-score: NORM.INV(1-α, 0, 1)\nComplete ES formula: =-1*(mean - stdev*NORM.DIST(NORM.INV(1-α,0,1),0,1,FALSE)/(1-α))\n\nNote: The negative sign converts P/L returns (where losses are negative) to positive loss values, matching our VaR convention.\nStep 3: Convert to dollar loss for your portfolio value\n\n\n\n\nActivity 3: Comparing ES Methods\nUsing your SPY_returns_1000.csv data, let’s compare ES calculation methods.\nTask 3.1: Create a comprehensive risk comparison table with the following structure:\n\nConfidence levels = 90%, 95%, 97.5%, 99%, 99.9%\nCalculate ES using historical simulation and parametric approach\nES/VaR ratio for each method\n\nTask 3.2: Discussion Questions\n\nAt which confidence levels do the methods diverge most?\nHow does the ES/VaR ratio change as confidence level increases?\nFor 99.9% confidence, does parametric ES exceed the worst historical loss?\nWhich method would you recommend for setting risk limits?\n\n\n\nActivity 4: The Coherence Property - Subadditivity\nLet’s verify that ES satisfies the subadditivity property that VaR violates.\nConsider two corporate bonds in your portfolio:\n\n\n\n\nBond A\nBond B\n\n\n\n\nNotional\n$100M\n$100M\n\n\nDefault Probability\n0.6%\n0.6%\n\n\nRecovery if Default\n$0\n$0\n\n\nMutual Correlation\n0 (independent)\n0 (independent)\n\n\n\nTask 4.1: Calculate 99% ES for Bonds A and B separately\n\nIn 99% of scenarios, what happens?\nIn 1% of scenarios, what is the loss?\nWhat is the 99% ES value?\n\nTask 4.2: Calculate 99% ES for the combined portfolio (A + B)\n\nProbability that neither defaults?\nProbability that at least one defaults?\nProbability that both default?\nWhat is the 99% ES value?\n\nTask 4.3: Compare your results\n\n\n\n\n\n\nReal-world Implications\n\n\n\nThe subadditivity property matters because:\n\nIt ensures diversification is always rewarded\nRisk limits can be allocated consistently across desks\nPortfolio risk is never understated when aggregating positions\nCapital requirements properly reflect risk reduction from diversification\n\n\n\n\n\nTheory: Regulatory Context - From VaR to ES\n\n\n\n\n\n\nBasel III’s Shift to Expected Shortfall\n\n\n\nThe regulatory evolution:\n\nBasel II (until 2016): Market Risk Capital based on VaR(99%, 10-day) with multiplier of 3\nBasel III (2016-present): Market Risk Capital based on ES(97.5%, stressed) with multiplier of 1.5\n\n\n\nKey change: After the 2008 crisis, regulators switched from VaR to ES because:\n\nVaR missed tail severity (banks failed despite “acceptable” VaR)\nES is mathematically coherent (rewards diversification)\nES is harder to manipulate\n\nStressed calibration requirement:\nBasel III requires banks to calculate ES using both:\n\nRecent period: Last 250 trading days\nStressed period: The historical 250-day window that produces highest ES for current portfolio\n\n\\[\\text{Capital} = 1.5 \\times \\max(ES_{recent}, ES_{stressed})\\]\nThe stressed period is found by testing rolling 250-day windows across historical data (typically capturing crisis periods like 2008 or March 2020).\nNote: We’ll explore the full regulatory framework in detail in a later seminar. For now, these exercises demonstrate why regulators made this shift.\n\n\nActivity 5: Practical Applications\nTask 5.1: In 2016, Basel III shifted from VaR to ES for market risk capital. Using your SPY data, compare the old and new requirements:\n\nCalculate 99% VaR (old Basel II requirement)\nCalculate 97.5% ES (new Basel III requirement)\nDespite the lower confidence level, is the new requirement more or less conservative?\n\nTask 5.2: Calculate 99% VaR and 97.5% ES for all 250-day rolling windows.\n\nWhich 250-day window produces the highest VaR and ES?\nCompare the results with the most recent 250-day window.\nIf you had to hold capital for the worst-case historical period, how much more would you need?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Risk II: Expected Shortfall</span>"
    ]
  },
  {
    "objectID": "week_04.html#block-c-monte-carlo-simulation-for-risk-measurement",
    "href": "week_04.html#block-c-monte-carlo-simulation-for-risk-measurement",
    "title": "4  Market Risk II: Expected Shortfall",
    "section": "Block C: Monte Carlo Simulation for Risk Measurement",
    "text": "Block C: Monte Carlo Simulation for Risk Measurement\n\nTheory: From Historical Data to Simulated Futures\n\n\n\n\n\n\nMonte Carlo Methods in Risk Management\n\n\n\nWhy simulate?\n\nHistorical data is limited (only one path of history)\nNeed to stress test scenarios that haven’t happened yet\nCan incorporate complex dependencies and dynamics\n\nBasic Monte Carlo for VaR/ES:\n\nSpecify a distribution for returns (normal, student-t, etc.)\nGenerate thousands of possible scenarios\nCalculate portfolio value for each scenario\nExtract VaR and ES from simulated distribution\n\nIndustry practice uses more sophisticated models:\n\nStochastic Volatility: Volatility itself is random (GARCH, Heston)\nJump Diffusion: Sudden jumps in addition to normal movements\nRegime Switching: Different distributions in calm vs stressed periods\nCopulas: Complex dependency structures between assets\n\n\n\n\n\nActivity 6: Basic Monte Carlo in Excel\nTask 6.1: Create 5000 simulated daily returns for your SPY position:\n\nIn Excel: =NORM.INV(RAND(), mean, stdev)\nUse mean and stdev from your historical data\nCalculae 95%/99% VaR and ES using historical method\n\nTask 6.2: Real returns have fatter tails than normal. Let’s use Student-t distribution and generate another 5000 simlated daily returns\n\n=T.INV(RAND(), degrees_of_freedom) * stdev + mean\nUse 5 degrees of freedom for fat tails\nCalculae 95%/99% VaR and ES using historical method\n\nTask 6.3: Compare results from historical simulation (actual), parametric method (theoretical), and Monte Carlo simulations. Which methods give similar results? Why might they differ?\n\n\nActivity 7: Convergence and Stability\nTask 7.1: Using Student-t distribution, calculate 97.5% ES with different numbers of scenarios:\n\nE.g., 200, 600, 1000, 2000, 5000, 10000 scenarios\nPlot ES estimate vs number of scenarios. At what point does it stabilize?\n\nTask 7.2: Run your 1000-scenario simulation 5 times (press F9 to recalculate) and write down the results for 99% VaR and 97.5% ES. How stable are your estimates? What does this imply for risk management?\n\n\n\n\n\n\nReal-World Implementation\n\n\n\nBanks typically use:\n\n10,000+ scenarios minimum\nMultiple risk factors (not just single asset)\nHistorical simulation with bootstrapping\nFiltered Historical Simulation (combining historical data with volatility models)\n\nThe trade-off: More scenarios = more accuracy but slower calculation. For intraday risk, banks often use fewer scenarios for speed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Risk II: Expected Shortfall</span>"
    ]
  },
  {
    "objectID": "week_04.html#block-d-case-study---ubs-and-the-2007-2008-financial-crisis",
    "href": "week_04.html#block-d-case-study---ubs-and-the-2007-2008-financial-crisis",
    "title": "4  Market Risk II: Expected Shortfall",
    "section": "Block D: Case Study - UBS and the 2007-2008 Financial Crisis",
    "text": "Block D: Case Study - UBS and the 2007-2008 Financial Crisis\nUBS, Switzerland’s largest bank, suffered the biggest losses of any European bank during the 2007-2008 financial crisis. Despite sophisticated risk models and AAA-rated assets, the bank lost over $37 billion on mortgage-related securities. This case illustrates how VaR models can catastrophically fail and why Expected Shortfall matters.\nBefore the Crisis (2005-2007):\n\nUBS held $50+ billion in “Super Senior” CDO tranches (the safest part of mortgage securities)\nThese were rated AAA - supposedly safer than US government bonds\nRisk models (like VaR) showed minimal danger because:\n\nHistorical data (2003-2006) showed tiny default rates\nMortgages across different regions were assumed independent\nThe bank bought insurance from monoline insurers as protection\nAssumed normal distributions and stable correlations\n\n\nDuring the Crisis (2007-2008):\n\nOctober 2007: UBS announces $3.4 billion loss (first major bank to report)\nFull year 2007: $18.7 billion in writedowns\nQ1 2008: Another $19 billion in losses\nStock price fell 70%, CEO and entire senior management resigned\n\nWhy Everything Failed:\n\nUS housing collapsed nationwide (not regionally as models assumed)\nAAA-rated securities became worthless\nInsurance companies failed (they insured the same mortgages)\nMarket liquidity vanished - couldn’t sell at any price\nLosses exceeded any historical scenario by 10-20x\n\n\nActivity 8: Discussion Questions\nTask 8.1: Model Assumptions\n\nIf historical data shows no defaults in AAA securities, what should your VaR be?\nShould risk models include events that have never happened?\nHow do you model risk for new products with limited history?\n\nTask 8.2: Risk Aggregation\n\nUBS claimed positions were “diversified” across different mortgages. What went wrong?\nIf correlation changes from 0.2 to 0.9 in crisis, how does this affect portfolio risk?\nCan diversification make risk disappear, or just hide it?\n\nTask 8.3: Lessons Learned\n\nWould higher capital requirements based on ES have prevented these losses?\nIf everyone uses similar risk models, what systemic risks emerge?\nWhy might a model that works for 99% of days still destroy a bank?\n\n\n\n\n\n\n\nThe Key Lesson\n\n\n\nVaR’s blind spot: It tells you the threshold you won’t exceed 99% of the time, but nothing about what happens in that 1%.\nUBS’s fate: The 1% tail event wasn’t just slightly worse than VaR - it was catastrophically worse. This is why regulators switched to Expected Shortfall after the crisis.\nRemember: Risk models work well in normal times. They fail precisely when you need them most - during crises when correlations spike, liquidity vanishes, and “impossible” events happen.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Risk II: Expected Shortfall</span>"
    ]
  },
  {
    "objectID": "week_04.html#key-takeaways",
    "href": "week_04.html#key-takeaways",
    "title": "4  Market Risk II: Expected Shortfall",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nVaR’s fatal flaw: It tells you nothing about severity when breached—ES measures what actually happens in the tail\nCoherence matters: ES always rewards diversification (subadditive), while VaR can punish it, leading to perverse incentives\nDistribution choice is critical: Normal distributions underestimate tail risk; fat-tailed distributions (Student-t) better capture extreme events\nMonte Carlo extends our toolkit: When historical data is limited, simulation helps explore scenarios that haven’t happened yet\nRegulatory evolution has a reason: Basel III’s shift to ES wasn’t arbitrary—it directly addressed failures revealed in 2008\nModels fail when you need them most: UBS lost $37B despite “safe” VaR—risk models work in normal times but break in crises when correlations spike and liquidity vanishes\nThe practical bottom line: Always ask “If things go bad, HOW bad?”—that’s ES, and it might save your firm",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Risk II: Expected Shortfall</span>"
    ]
  },
  {
    "objectID": "week_05.html",
    "href": "week_05.html",
    "title": "5  Market Risk III: Model Validation and Regulatory Capital",
    "section": "",
    "text": "Block A: Backtesting VaR Models",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Market Risk III: Model Validation and Regulatory Capital</span>"
    ]
  },
  {
    "objectID": "week_05.html#block-a-backtesting-var-models",
    "href": "week_05.html#block-a-backtesting-var-models",
    "title": "5  Market Risk III: Model Validation and Regulatory Capital",
    "section": "",
    "text": "Activity 1: Testing Your VaR Model\nYour risk committee asks:\n\nAre the VaR models we built last week actually working?\n\nLet’s use proper model validation with a train-test split.\nUsing your SPY_returns_1000.csv file:\n  Download Dataset (1000 Days) \n\nTraining period: Days 501-750 (250 days) - calculate VaR on this period\nTesting period: Days 751-1000 (250 days) - backtest your model on this period\n\nTask 1.1: Using only days 501-750 (training set):\n\nCalculate 95% and 99% VaR using historical simulation\nCalculate 95% and 99% VaR using parametric approach\n\nTask 1.2: Apply your VaR thresholds to the test period (days 751-1000):\n\nCount how many days returns exceeded your 95% VaR\nCount how many days returns exceeded your 99% VaR\nCalculate the actual violation rates (violations/250)\nCompare to expected rates (5% and 1%)\n\n\n\nTheory: The Backtesting Framework\nBacktesting is the statistical process of comparing predicted VaR with actual outcomes to validate model accuracy.\nKey concept: If your \\(\\alpha\\)% VaR model is correctly calibrated:\n\nExpected exceptions = \\(n \\times (1 - \\alpha)\\)\nViolations should be independent (no clustering)\nActual exceptions should follow \\(\\text{Binomial}(n, 1-\\alpha)\\)\n\n\n\n\n\n\n\nBasel Traffic Light Zones (250 days, 99% VaR)\n\n\n\n\n\n\n\n\n\n\n\n\nZone\nViolations\nMultiplier\nInterpretation\n\n\n\n\nGreen\n0-4\n3.00\nModel acceptable\n\n\nYellow\n5-9\n3.40-3.85\nPotential problems, review required\n\n\nRed\n10+\n4.00\nModel inadequate\n\n\n\n\n\n\n\nActivity 2: Basel’s Traffic Light System\nTask 2.1: Using your violation counts from Activity 1, determine your zone:\n\nWhat zone are you in for 99% VaR?\nIf your daily VaR is $10M, what’s your capital requirement at each zone?\nWhy do you think Basel uses these specific cutoffs?\n\n\n\nTheory: The Binomial Test\nFor 250 test observations with 95% VaR:\n\nExpected violations: \\(250 \\times 0.05 = 12.5\\)\n\nWe use a binomial distribution to calculate the acceptable range of violations. Choosing a 95% confidence level for our test:\n\nLower bound: =BINOM.INV(250, 0.05, 0.025) = 6\nUpper bound: =BINOM.INV(250, 0.05, 0.975) = 20\nAcceptable range at 95% confidence: 6 to 20 violations\n\nNote: The 0.025 and 0.975 give us a two-tailed 95% confidence interval (2.5% in each tail).\n\n\nActivity 3: VaR Violations Range\nTask 3.1: Answer the following questions\n\nAre your violations within the acceptable range?\nIf you had 20 violations, what would this suggest about your model?\nIf you had only 3 violations, is this good news or bad news?\nWhat would the acceptable range be if you used 99% confidence for your test?\n\n\n\nTheory: The Kupiec Proportion of Failures Test\n\n\n\n\n\n\nUnderstanding the Kupiec Test\n\n\n\nPurpose: The Kupiec POF test (1995) formally tests whether the observed frequency of VaR violations is statistically consistent with the model’s confidence level.\n\nNull Hypothesis (H₀): The model is correctly calibrated - violation rate equals expected rate (p)\nAlternative Hypothesis (H₁): The model is misspecified - violation rate differs from p\n\n\n\nThe Test Statistic:\nThe test uses a likelihood ratio to compare the model’s promised violation rate against what actually occurred:\n\\[LR_{POF} = -2 \\ln\\left(\\frac{(1-p)^{n-v} \\times p^v}{(1-\\hat{p})^{n-v} \\times \\hat{p}^v}\\right)\\]\nWhere:\n\n\\(p\\) = expected violation rate (0.01 for 99% VaR, 0.05 for 95% VaR)\n\\(v\\) = observed violations\n\\(n\\) = total observations (250 in our case)\n\\(\\hat{p} = v/n\\) (observed violation rate)\n\nThis LR statistic follows a chi-squared distribution with 1 degree of freedom.\nDecision Rule:\n\nIf \\(LR_{POF} &gt; \\chi^2_{0.95}(1) = 3.84\\), reject the model at 5% significance (In Excel: =CHISQ.INV(0.95,1))\nIf \\(LR_{POF} &gt; \\chi^2_{0.99}(1) = 6.63\\), reject the model at 1% significance (In Excel: =CHISQ.INV(0.99,1))\n\nInterpretation:\n\nThe test is two-sided: rejects for both too many AND too few violations\nSmall sample problem: With few expected violations (like 2.5 for 99% VaR), the test has low statistical power - meaning it often fails to detect truly bad models\nThe test assumes violations are independent - it only checks frequency, not clustering\n\nNote: More advanced tests like Christoffersen’s Conditional Coverage combine frequency and independence testing in one statistic.\n\n\nActivity 4: Kupiec’s Test Implementation\nTask 4.1: Calculate the Kupiec test statistic for your 99% VaR model.\nTask 4.2: Test the sensitivity - calculate LR for different violation counts: 0, 2, 5, 7, and 10. What’s the minimum violations needed to reject? Maximum allowed?\nTask 4.3: Repeat for your 95% VaR model. Which test is more conclusive? Why?\nTask 4.4: Summary:\n\nDo Basel traffic lights and Kupiec test agree for your model?\nWith only 2.5 expected violations, how useful is the Kupiec test for 99% VaR?\nWould you trust this model for setting trading limits?\n\n\n\n\n\n\n\nKey Insight\n\n\n\nThe Kupiec test often “passes” 99% VaR models even when they’re bad - you need 7+ violations (almost 3× expected) to reject! This is why banks also test at multiple confidence levels.\n\n\n\n\nActivity 5: Independence - Are Violations Clustering?\nTask 5.1: List the dates of your violations. Calculate days between consecutive violations. Are they evenly spread or clustered?\nTask 5.2: For violation clustering, count:\n\nHow many violations are followed by another violation next day?\nHow many violations are followed by no violation?\nIf you see several clustered violations, what does this suggest?\n\nTask 5.3: What could cause violation clustering?\n\n\n\n\n\n\nWhy This Matters\n\n\n\nDuring the 2008 crisis, banks saw weeks of consecutive VaR breaches. Models assuming independent violations drastically underestimated risk during stressed periods.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Market Risk III: Model Validation and Regulatory Capital</span>"
    ]
  },
  {
    "objectID": "week_05.html#block-b-the-es-backtesting-challenge",
    "href": "week_05.html#block-b-the-es-backtesting-challenge",
    "title": "5  Market Risk III: Model Validation and Regulatory Capital",
    "section": "Block B: The ES Backtesting Challenge",
    "text": "Block B: The ES Backtesting Challenge\n\nTheory: Why ES Backtesting is Difficult\n\n\n\n\n\n\nThe ES Backtesting Problem\n\n\n\nUnlike VaR (a single threshold), ES is the average of all tail losses. To test it properly, you need many tail observations - but tail events are rare by definition.\n\n\nKey issues:\n\nFew observations: At 97.5% confidence, expect only ~6 violations per year\nStatistical power: Can’t reliably test if the average of 6 observations matches prediction\nNot elicitable: ES lacks a simple scoring function that VaR has (binary: breach or not)\n\nTwo practical approaches:\n\nIndirect validation: Test VaR at multiple confidence levels (90%, 95%, 97.5%)\n\nIf all levels calibrated → ES likely correct\nMore observations at lower confidence = better statistical power\n\nDirect tests: Compare realized tail losses to predicted ES\n\nAcerbi-Székely test (2014, 2019)\nMcNeil-Frey test (2000)\nBut require strong assumptions and large samples\n\n\nRegulatory stance: Basel III accepts indirect validation methods because direct ES backtesting remains statistically challenging even with sophisticated tests.\n\n\nActivity 6: Comprehensive Backtesting - VaR and ES\nTask 6.1: Using your training period (days 1-750), calculate all risk measures you’ll need:\n\n\n\nConfidence Level\nVaR\nES\n\n\n\n\n90%\n?\n?\n\n\n92.5%\n?\n?\n\n\n95%\n?\n?\n\n\n97.5%\n?\n?\n\n\n99%\n?\n?\n\n\n\nTask 6.2: Apply ALL thresholds to test period (days 751-1000):\n\n\n\nConfidence\nExpected Violations\nActual Violations\n\n\n\n\n90%\n?\n?\n\n\n92.5%\n?\n?\n\n\n95%\n?\n?\n\n\n97.5%\n?\n?\n\n\n99%\n?\n?\n\n\n\nTask 6.3: For each confidence level, calculate average loss on violation days:\n\n\n\n\n\n\n\n\n\n\nConfidence\nPredicted ES\nRealized Avg Loss\nDifference\nReliable?\n\n\n\n\n90%\n?\n?\n?\n?\n\n\n92.5%\n?\n?\n?\n?\n\n\n95%\n?\n?\n?\n?\n\n\n97.5%\n?\n?\n?\n?\n\n\n99%\n?\n?\n?\n?\n\n\n\nTask 6.4: Validation conclusions:\n\nVaR validation: Which levels pass binomial test?\nES validation (direct): Can you trust ES given violation counts?\nES validation (indirect): If all VaR levels calibrated correctly, what about ES?\nOverall assessment: Would you use this model for risk management?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Market Risk III: Model Validation and Regulatory Capital</span>"
    ]
  },
  {
    "objectID": "week_05.html#block-c-stressed-risk-measures",
    "href": "week_05.html#block-c-stressed-risk-measures",
    "title": "5  Market Risk III: Model Validation and Regulatory Capital",
    "section": "Block C: Stressed Risk Measures",
    "text": "Block C: Stressed Risk Measures\n\nTheory: Stressed Risk Measures\n\n\n\n\n\n\nUnderstanding Stressed Calibration\n\n\n\nContext: During the 2007-2008 crisis, banks’ VaR models showed low risk because they were calibrated to recent calm periods. When markets crashed, actual losses far exceeded predictions.\nRegulatory Response:\n\nBasel 2.5 (2011): Introduced Stressed VaR (SVaR) alongside regular VaR\nBasel III (2016): Extended concept to Stressed ES\n\n\n\nHow it works:\n\nRegular measures: Calibrated to recent period (typically last 250 days)\nStressed measures: Calibrated to historical period with highest risk\nCapital requirement: Based on both regular AND stressed measures\n\nFinding the stressed period:\n\nTest all rolling 250-day windows in your historical data\nIdentify window producing highest VaR/ES for current portfolio\nThis becomes your “stressed period”\n\nWhy it matters:\n\nForces banks to hold crisis-level capital even during calm periods\nPrevents procyclicality (risk models becoming optimistic just before crashes)\nTypical stressed measures are 2-4× higher than current measures\n\nFormula evolution:\n\nBasel II: Capital = 3 × VaR(99%, 10-day)\nBasel 2.5: Capital = 3 × VaR(99%, 10-day) + 3 × SVaR(99%, 10-day)\nBasel III: Capital = 1.5 × max(ES(97.5%, 10-day), Stressed ES(97.5%, 10-day))\n\n\n\nActivity 7: Stressed Period and Capital Impact\n  Download Dataset (2006-2012) \nTask 7.1: For each possible 250-day window, calculate 99% VaR and 97.5% ES using historical simulation.\nTask 7.2: Identify stressed periods:\n\n\n\n\n\n\n\n\n\n\n\nMeasure\nWorst 250-day Window\nValue\nRecent Window\nValue\nRatio\n\n\n\n\n99% VaR\nDates ?-?\n?\nDates ?-?\n?\n?\n\n\n97.5% ES\nDates ?-?\n?\nDates ?-?\n?\n?\n\n\n\nTask 7.3: Calculate capital requirements for $100M portfolio:\n\n\n\n\n\n\n\n\n\n\nFramework\nFormula\nRecent Period\nStressed Period\nTotal Capital\n\n\n\n\nBasel II\n3 × VaR(99%)\n?\nN/A\n?\n\n\nBasel 2.5\n3 × VaR + 3 × SVaR\n?\n?\n?\n\n\nBasel III\n1.5 × max(ES, SES)\n?\n?\n?\n\n\n\n\nHow much did capital increase from Basel II to 2.5?\nWhy did Basel III reduce the multiplier but still maintain high capital?\n\n\n\n\n\n\n\nLimitation\n\n\n\nYour dataset might not contain a true crisis period. Real stressed measures often use 2008 or March 2020 data, showing 3-5× higher risk than calm periods.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Market Risk III: Model Validation and Regulatory Capital</span>"
    ]
  },
  {
    "objectID": "week_05.html#case-study-archegos-capital-management-2021",
    "href": "week_05.html#case-study-archegos-capital-management-2021",
    "title": "5  Market Risk III: Model Validation and Regulatory Capital",
    "section": "Case Study: Archegos Capital Management (2021)",
    "text": "Case Study: Archegos Capital Management (2021)\nBackground: In March 2021, Archegos Capital, a family office managing $10 billion, collapsed in 48 hours, causing over $10 billion in losses to banks including Credit Suisse ($5.5B), Nomura ($2.85B), and Morgan Stanley ($911M).\nThe Hidden Leverage Strategy: Archegos used total return swaps to secretly build massive positions:\n\nBanks bought and held the shares, Archegos got profits/losses\nArchegos put up only 10-20% margin, controlled 5-8× more\nNo disclosure required (banks owned shares, not Archegos)\nSame positions held at multiple banks - none knew about the others\n\nThe Systemic Blind Spot: Each bank thought they had manageable exposure:\n\nCredit Suisse: “We have $10B exposure, acceptable risk”\nMorgan Stanley: “We have $8B exposure, acceptable risk”\nNomura: “We have $5B exposure, acceptable risk”\n\nReality: $30B+ total street exposure to the same 8 stocks.\nWhy Risk Models Failed:\n\n\n\n\n\n\n\nWhat VaR Showed\nWhat Actually Happened\n\n\n\n\nNormal volatility in liquid stocks\n30% drop when all banks sold together\n\n\n99% VaR &lt; $50M daily loss\n$5.5B loss in 2 days\n\n\nAdequate 15% margin\nMargin calls triggered death spiral\n\n\nDiversified prime brokerage business\nOne client destroyed the quarter\n\n\n\nThe Collapse:\n\nViacomCBS announced share offering → stock dropped 10%\nArchegos couldn’t meet margin calls at ANY bank\nAll banks started selling the same stocks simultaneously\nFire sale drove stocks down 30-50%\nBanks discovered they all had the same positions\n\nDiscussion Questions:\n\nHow would you modify VaR to capture hidden systemic exposure?\nShould banks be required to disclose large swap positions to each other?\nIf your backtesting shows perfect results but you lose $5B overnight, what assumption was wrong?\nCould stressed VaR have caught this?\n\n\n\n\n\n\n\nThe Core Lesson\n\n\n\nIndividual models can be perfect and still fail systemically. Each bank’s VaR was “correct” for their known exposure. But swaps created hidden concentration risk across the entire street. When everyone sells the same positions simultaneously, historical correlations and liquidity assumptions collapse.\nThis wasn’t a failure of math - it was a failure to see the whole picture.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Market Risk III: Model Validation and Regulatory Capital</span>"
    ]
  },
  {
    "objectID": "week_05.html#key-takeaways",
    "href": "week_05.html#key-takeaways",
    "title": "5  Market Risk III: Model Validation and Regulatory Capital",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nBacktesting VaR: Use train-test split, Basel traffic lights (0-4 green, 5-9 yellow, 10+ red), and Kupiec test - but 99% VaR needs 7+ violations to reject (nearly 3× expected)\nES Challenge: Can’t backtest ES directly with few tail events - use indirect validation via multiple VaR levels (90%, 95%, 97.5%)\nStressed Measures: Calculate risk using worst historical 250-day window - typically 2-4× higher than recent period, forcing crisis-level capital in calm times\nBasel Evolution: Each crisis increased requirements: Basel II (3×VaR) → Basel 2.5 (3×VaR + 3×SVaR) → Basel III (1.5×max(ES, Stressed ES))\nCritical Lesson: Models that pass all backtests can still fail catastrophically (Archegos) - individual models miss systemic risk, and new crisis types won’t match historical patterns",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Market Risk III: Model Validation and Regulatory Capital</span>"
    ]
  },
  {
    "objectID": "week_06.html",
    "href": "week_06.html",
    "title": "6  Test 1: Market Risk Management",
    "section": "",
    "text": "Test Overview\nDuration: 70 minutes\nTotal Points: 25 points",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Test 1: Market Risk Management</span>"
    ]
  },
  {
    "objectID": "week_06.html#test-structure",
    "href": "week_06.html#test-structure",
    "title": "6  Test 1: Market Risk Management",
    "section": "Test Structure",
    "text": "Test Structure\n\nPart A: Multiple Choice (13 points)\n\n26 questions × 0.5 points each\nFour options (a, b, c, d), only one correct\nNo negative points for wrong answers\nCovers theoretical concepts from course topics 1-5\n\n\n\nPart B: Excel Calculations (12 points)\n\n6 calculation problems × 2 points each\nFocus on VaR and ES methods\nDataset file will be provided: exam_1.xlsx\nAll calculations must be performed in Excel\nExcel file will be uploaded to the information system showing all calculations with formulas",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Test 1: Market Risk Management</span>"
    ]
  },
  {
    "objectID": "week_06.html#allowed-materials",
    "href": "week_06.html#allowed-materials",
    "title": "6  Test 1: Market Risk Management",
    "section": "Allowed Materials",
    "text": "Allowed Materials\n✅ Handwritten cheatsheet: One side of A4 paper, handwritten (not printed), original work only\n✅ Excel: Use for calculations, must show formulas (no hardcoded values)\n✅ Calculator: Basic or scientific calculator\n❌ Not allowed: Printed materials, lecture notes, textbooks, communication devices, internet browsing, AI tools",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Test 1: Market Risk Management</span>"
    ]
  },
  {
    "objectID": "week_06.html#important-notes",
    "href": "week_06.html#important-notes",
    "title": "6  Test 1: Market Risk Management",
    "section": "Important Notes",
    "text": "Important Notes\n\nExpress all VaR/ES results as positive decimal numbers (e.g., 0.03456)\nRound calculations to 5 decimal places\nShow all work in Excel - formulas must be visible for potential partial credit\nAttempting all questions is better than leaving blanks (no penalties)\n\nGood luck!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Test 1: Market Risk Management</span>"
    ]
  },
  {
    "objectID": "week_07.html",
    "href": "week_07.html",
    "title": "7  Reading Week",
    "section": "",
    "text": "Tip\n\n\n\nNo classes this week! Take some rest and use this time to catch up on readings, review course materials, and prepare for the second half of the semester.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reading Week</span>"
    ]
  },
  {
    "objectID": "week_08.html",
    "href": "week_08.html",
    "title": "8  Liquidity Risk",
    "section": "",
    "text": "Block A: Understanding Liquidity Risk\nYou manage a $500M equity portfolio for a mid-sized hedge fund. This morning, you need to liquidate positions in three stocks. The Excel file contains full order books (10 bid levels, 5 ask levels) for three stocks with different liquidity characteristics:\nDownload Order Book Data",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Liquidity Risk</span>"
    ]
  },
  {
    "objectID": "week_08.html#block-a-understanding-liquidity-risk",
    "href": "week_08.html#block-a-understanding-liquidity-risk",
    "title": "8  Liquidity Risk",
    "section": "",
    "text": "TechCorp: 200,000 share position, 5,000,000 daily volume (highly liquid blue-chip)\nRegionalBank: 500,000 share position, 1,200,000 daily volume (moderately liquid mid-cap)\nBioStartup: 100,000 share position, 150,000 daily volume (illiquid small-cap)\n\n\nActivity 1: Discovering the Two Faces of Liquidity\nTask 1.1: Small Trade Analysis\nSell 10,000 shares of each stock at the best available bid price. For each stock, calculate:\n\nProceeds from the sale\nTotal liquidity cost in $ and % (compared to mid-price)\nWhich stock has the highest percentage liquidity cost for this small trade? Why?\n\nTask 1.2: Full Position Liquidation\nNow liquidate your entire position in each stock by selling at market (hitting successive bid levels in the order book). For each stock, calculate:\n\nAverage execution price\nTotal proceeds\nTotal liquidity cost in $ and % (compared to mid-price)\nPosition size as % of daily volume\n\nTask 1.3: Analysis and Reflection\nCompare and analyze your results from Tasks 1.1 and 1.2:\n\nRank the three stocks by liquidity cost (%) for both small trades and full liquidation. Did the ranking change? Why?\nFor each stock, how did your liquidity cost (%) change when you went from 10,000 shares to your full position? Which stock saw the biggest increase?\nWhat pattern emerges between position size (as % of daily volume) and total liquidity cost?\n\nTask 1.4: Decomposing Liquidity Cost\nNow quantify the two sources of liquidity cost. For each stock, separate your total liquidity cost into:\n\nSpread Cost (Exogenous): The cost if you could execute your entire position at the best bid price\nMarket Impact Cost (Endogenous): The additional cost from walking down the order book\nDoes this decomposition confirm your insights from Task 1.3? How?\n\n\n\nTheory: The Two Faces of Liquidity Risk\nLet’s formalize what you discovered: exogenous (market-driven) and endogenous (position-driven) liquidity risk.\n\n\n\n\n\n\nLiquidity Risk\n\n\n\n\nExogenous Liquidity Risk (Market-Driven): The liquidity conditions that exist in the market regardless of your actions.\n\nBid-ask spread (tightness)\nMarket depth at best quotes\nDaily trading volume\n\nEndogenous Liquidity Risk (Position-Driven): The liquidity impact caused by your own position size.\n\nPrice impact beyond the spread\nYour position as % of daily volume\nMarket depth consumption\n\n\n\n\n\\[\n\\text{Total Liquidity Cost} = \\text{Spread Cost} + \\text{Market Impact Cost}\n\\]\n\nSpread Cost = Exogenous (market-driven, constant per share)\nMarket Impact Cost = Endogenous (position-driven, increasing in position size)\n\n\\[\n\\text{Market Impact Cost} = f\\!\\left(\\frac{\\text{Position Size}}{\\text{Market Depth}}\\right)\n\\]\nRisk Management Implications: Never assume you can exit at current market prices—the market is a participant, not a vending machine.\nKey Takeaway: Small trades face the market as it is (exogenous), while large trades change the market itself (endogenous). The pattern you observed—cost increasing non-linearly with position size relative to daily volume—is the fundamental reality of liquidity risk.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Liquidity Risk</span>"
    ]
  },
  {
    "objectID": "week_08.html#block-b-time-horizons-and-liquidation-strategies",
    "href": "week_08.html#block-b-time-horizons-and-liquidation-strategies",
    "title": "8  Liquidity Risk",
    "section": "Block B: Time Horizons and Liquidation Strategies",
    "text": "Block B: Time Horizons and Liquidation Strategies\nUsing the same three positions from Block A, you now need to analyze different liquidation strategies. Your risk manager has given you a constraint: maximum daily participation cannot exceed 20% of normal daily volume to avoid market disruption.\n\nActivity 2: The Liquidation Tradeoff\nTask 2.1: Minimum Liquidation Time\nCalculate the minimum number of days needed to liquidate each position under the 20% participation constraint. What does this tell you about position sizing in illiquid markets?\nTask 2.2: Patient Liquidation Cost\nAssume you use sophisticated execution algorithms that allow the order book to refresh to its normal levels each day. You will liquidate up to 20% of daily volume per day.\n\nUse the Block A order book to calculate total cost over liquidation period\nHow does total liquidity cost compare with immediate liquidation (Block A Task 1.2)?\n\n\nNote: This “refresh” assumption is optimistic but helps isolate the benefit of patient liquidation. In reality, the market may react to your selling, which is why execution algorithms and careful timing matter.\n\nTask 2.3: Downside Risk During Liquidation\nRisk managers must consider worst-case scenarios. If market volatility is 2% daily for all three stocks, calculate the potential downside risk (1 std dev) from holding positions during gradual liquidation.\nFor each stock:\n\\[\n\\text{Downside Risk (1 std dev)} = \\text{Position Value} \\times \\sigma_{\\text{daily}} \\times \\sqrt{\\text{Liquidation Days}}\n\\]\nwhere\n\\[\n\\text{Position Value} = \\text{Position Size} \\times \\text{Mid Price (from Block A)}\n\\]\nThis represents the potential loss if the market moves against you during the liquidation period (note: markets can also move in your favor, but prudent risk management focuses on downside protection).\nCalculate:\n\nDownside risk exposure for each stock\nLiquidity cost savings from patient liquidation: (Block A Task 1.2 cost) - (Block B Task 2.2 cost)\nRisk-adjusted benefit: Cost Savings - Downside Risk (if positive, patient liquidation remains beneficial even in the worst-case scenario)\n\nTask 2.4: Analysis - The Liquidation Tradeoff\nCompare your results from Tasks 2.1, 2.2, and 2.3:\n\nWhich stock has the largest liquidity cost savings from patient liquidation (in absolute $ and % terms)?\nFor which stock(s) is the risk-adjusted benefit positive? For which is it negative? What does this tell you about when patient liquidation makes sense?\nIf you were the portfolio manager facing potential adverse market movements, which liquidation strategy (immediate vs patient) would you choose for each stock and why?\nHow do your Block A findings (exogenous vs endogenous costs from Task 1.4) explain why patient liquidation’s risk-adjusted benefit varies across stocks? (Hint: Which type of cost is reduced by patient liquidation?)\n\n\n\nTheory: The Liquidation Timeline Framework\n\nImmediate Liquidation: Eliminates market risk quickly — but incurs maximum liquidity costs (spread + severe market impact).\nGradual Liquidation: Reduces market impact — but increases exposure to adverse price movements over the liquidation horizon.\n\n\\[\n\\text{Minimum Liquidation Time} = \\frac{\\text{Position Size}}{\\text{Daily Volume} \\times \\text{Max Participation Rate}}\n\\]\nPractical Guidelines\n\n\n\nParticipation Rate\nMarket Signal\nTypical Impact\n\n\n\n\n&lt; 5%\nInvisible\nMinimal\n\n\n5-10%\nNormal trading\nSlight\n\n\n10-20%\nLarge player\nNoticeable\n\n\n20-30%\nMarket mover\nSignificant\n\n\n&gt; 30%\nDistressed seller\nSevere\n\n\n\nDuring market stress:\n\nDaily volumes can drop 50-70%\nBid-ask spreads widen 3-10x\nMarket depth evaporates\nResult: A 2-day liquidation becomes a 2-week ordeal\n\nKey Insight: Liquidity risk is non-linear and procyclical - it’s minimal when you don’t need it and severe when you do.\n\nLiquidity-Adjusted VaR\nTraditional VaR assumes positions can be liquidated instantaneously at market prices. Liquidity-adjusted VaR (LVaR) incorporates the cost of unwinding positions over a realistic time horizon:\n\\[\\text{LVaR} = \\text{VaR} + \\text{Liquidation Cost}\\]\nBanks use internal models to estimate liquidation costs based on historical bid-ask spreads, market depth, and trading volumes. These models typically:\n\nAssess position size relative to market liquidity - larger positions relative to daily volume face higher market impact\nDetermine realistic liquidation horizon - the time needed to unwind without excessive market disruption (often capped at 10-20% daily participation rate)\nEstimate total execution cost - combining immediate spread costs with projected market impact over the liquidation period\n\nThe liquidation cost component in LVaR reflects the expected cost under the chosen liquidation strategy (immediate vs gradual), calibrated to historical market conditions and stressed scenarios.\nKey insight: A portfolio may have acceptable VaR but unacceptable LVaR if positions are large relative to market liquidity. Liquidity risk dominates market risk for illiquid positions.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Liquidity Risk</span>"
    ]
  },
  {
    "objectID": "week_08.html#block-c-funding-liquidity-risk-and-the-liquidity-coverage-ratio-lcr",
    "href": "week_08.html#block-c-funding-liquidity-risk-and-the-liquidity-coverage-ratio-lcr",
    "title": "8  Liquidity Risk",
    "section": "Block C: Funding Liquidity Risk and the Liquidity Coverage Ratio (LCR)",
    "text": "Block C: Funding Liquidity Risk and the Liquidity Coverage Ratio (LCR)\nSo far, we’ve focused on asset liquidity - can you sell assets without major price impact? Now we turn to funding liquidity - can you meet cash obligations as they come due?\nBanks face a unique challenge: liabilities (deposits) can flee rapidly during stress, but assets (loans) cannot be recalled quickly. If depositors demand $10B in withdrawals today, you cannot say “wait 30 years for these mortgages to mature.” You need liquid assets - a buffer - to meet sudden outflows.\nThe 2008 financial crisis revealed that many banks held insufficient liquid assets relative to their unstable funding. Within days, wholesale funding markets froze, and banks faced severe funding runs. In response, Basel III introduced the Liquidity Coverage Ratio (LCR) to ensure banks maintain adequate high-quality liquid assets to survive a 30-day acute stress scenario.\n\n\n\n\n\n\nKey Concepts for This Activity\n\n\n\nHigh-Quality Liquid Assets (HQLA): Assets that can be quickly converted to cash with minimal loss of value, even during market stress.\n\nLevel 1 (0% haircut): Cash, central bank reserves, government bonds - the most liquid, can be sold at full value even in crisis\nLevel 2A (15% haircut): High-quality corporate bonds - liquid but might sell at 85 cents on the dollar in stress\n\nHaircuts: Percentage reduction in market value to reflect stressed liquidation conditions. A 15% haircut means a $100 bond counts as only $85 of available liquidity.\nLiquidity Coverage Ratio (LCR): Basel III requirement that banks hold sufficient HQLA to cover 30 days of stressed net cash outflows:\n\\[\\text{LCR} = \\frac{\\text{HQLA}}{\\text{Net Cash Outflows over 30 days}} \\times 100\\% \\geq 100\\%\\]\nStressed Cash Outflows: Basel III assumes liability outflow rates calibrated to the 2008 crisis - stable retail deposits are sticky (5% outflow), but wholesale funding runs immediately (100% outflow).\n\n\n\nActivity 3: Computing the Liquidity Coverage Ratio\nYou’re the treasurer of MidSize Bank. Your simplified balance sheet shows:\n\n\n\n\n\n\n\nAssets\nLiabilities\n\n\n\n\nCash & Central Bank Reserves: $5B\nStable Retail Deposits: $60B\n\n\nGovernment Bonds (AAA): $15B\nLess Stable Retail Deposits: $20B\n\n\nCorporate Bonds (Investment Grade): $20B\nCorporate Deposits (operational): $15B\n\n\nCorporate Loans: $50B\nCorporate Deposits (non-operational): $10B\n\n\nRetail Mortgages: $40B\nWholesale Funding (&lt; 30 days): $25B\n\n\n\nTask 3.1: Calculate High-Quality Liquid Assets (HQLA)\nCalculate the total HQLA by applying the appropriate haircuts to the balance sheet assets.\nTask 3.2: Calculate 30-Day Stressed Cash Outflows\nCalculate expected cash outflows over 30 days using these outflow rates:\n\nStable Retail Deposits: 5%\nLess Stable Retail: 10%\nCorporate Operational: 25%\nCorporate Non-operational: 40%\nWholesale Funding: 100%\n\nTask 3.3: Calculate the LCR\nCalculate MidSize Bank’s LCR and determine:\n\nDoes it meet the regulatory minimum of 100%?\nWhat is the buffer (or shortfall)?\n\nTask 3.4: Crisis Stress Test\nNow assume a severe market crisis occurs:\n\nGovernment bonds lose 10% of market value\nCorporate bond haircuts increase from 15% to 50%\n\nRecalculate:\n\nNew HQLA value\nNew LCR\nIf LCR &lt; 100%, what is the funding gap in dollars?\n\nTask 3.5: Funding Liquidity vs Asset Liquidity\nCompare your Block C findings with Blocks A and B:\n\nHow does funding liquidity risk (Block C) differ from asset liquidity risk (Blocks A & B)?\nIn Task 3.4, which asset class deterioration caused the biggest HQLA decline? Why does this matter for bank risk management?\nMidSize Bank has $25B in wholesale funding (19% of liabilities). How is reliance on wholesale funding similar to holding large illiquid positions (like BioStartup in Block A)?\nCan a bank have good asset liquidity but still fail from funding liquidity crisis? Explain using your findings.\n\n\n\nTheory: Funding Liquidity Risk and Basel III LCR\n\nFunding Liquidity vs Asset Liquidity\nAsset Liquidity (Blocks A & B)\n\nCan I sell my assets without major price impact?\nFocus: Bid-ask spreads, market depth, position size\n\nFunding Liquidity (Block C)\n\nCan I meet my cash obligations as they come due?\nFocus: Liability stability, rollover risk\n\nThe Connection: Funding stress forces asset fire sales (selling at Block A prices with high market impact), which depletes HQLA, which triggers more funding stress.\n\n\nLiquidity Metrics\n\\[\n\\mathrm{LCR} = \\frac{\\mathrm{HQLA}}{\\text{Net Cash Outflows over 30 days}} \\times 100\\% \\ge 100\\%\n\\]\nThe LCR ensures banks can survive acute stress for 30 days. Basel III also requires the Net Stable Funding Ratio (NSFR) for long-term structural stability:\n\\[\\text{NSFR} = \\frac{\\text{Available Stable Funding}}{\\text{Required Stable Funding}} \\geq 100\\%\\]\nTwo dimensions of funding liquidity:\n\nLCR: Can you survive a 30-day crisis? (Short-term shock absorption)\nNSFR: Is your funding stable for your asset maturity? (Long-term structural stability)\n\nA bank might pass LCR but fail NSFR if it funds long-term assets (e.g., 30-year mortgages) with short-term liabilities (e.g., overnight wholesale funding). LCR helps survive the crisis; NSFR helps avoid the crisis.\n\n\nHQLA Hierarchy\n\nLevel 1: Cash, central bank reserves, sovereigns (0% haircut)\nLevel 2A: High-rated corporates, covered bonds (15% haircut)\nLevel 2B: Lower-rated corporates, equities (25-50% haircut)\n\n\n\nThe Death Spiral\n\nRumors of problems → Wholesale funding dries up\nSell liquid assets → HQLA depletes\nFire sale illiquid assets → Losses mount\nMore rumors → Retail deposit run\nBank failure\n\nKey Insight: Banks don’t fail from bad loans (that takes years). They fail from funding runs (that takes days). Your Task 3.4 stress test showed how quickly this can happen—HQLA evaporates when you need it most.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Liquidity Risk</span>"
    ]
  },
  {
    "objectID": "week_08.html#block-d-case-study---the-gamestop-short-squeeze-january-2021",
    "href": "week_08.html#block-d-case-study---the-gamestop-short-squeeze-january-2021",
    "title": "8  Liquidity Risk",
    "section": "Block D: Case Study - The GameStop Short Squeeze (January 2021)",
    "text": "Block D: Case Study - The GameStop Short Squeeze (January 2021)\nIn January 2021, GameStop (GME) became the center of an unprecedented liquidity crisis that combined all the elements you’ve studied: asset liquidity evaporation, funding liquidity stress, forced liquidation, and systemic feedback loops.\nThe Setup: GameStop, a struggling video game retailer, had become heavily shorted by hedge funds—short interest exceeded 100% of the available shares. Retail traders on Reddit’s WallStreetBets forum coordinated buying, triggering a massive short squeeze. Between January 13-28, the stock surged from $20 to $483.\nWhat happened to liquidity? As the price soared, multiple liquidity crises unfolded simultaneously:\n\nAsset liquidity vanished: Bid-ask spreads that were normally $0.01-0.05 widened to $10-50 during peak volatility. Market makers stepped back, and trading volume that was normally 7 million shares/day spiked to over 175 million shares/day—yet liquidity actually decreased as spreads exploded.\nFunding liquidity crisis: Robinhood, the popular trading app used by retail traders, faced a liquidity crisis when the Depository Trust & Clearing Corporation (DTCC) increased margin requirements from $700 million to $3 billion overnight. Unable to meet this demand immediately, Robinhood restricted trading, triggering outrage and congressional hearings.\nHedge fund forced liquidation: Short sellers like Melvin Capital faced catastrophic losses and margin calls. To close their short positions, they had to buy shares—but each purchase pushed the price higher, triggering more margin calls. They couldn’t use patient liquidation because margin calls demanded immediate action.\n\n\nActivity 4: Analyzing the GameStop Liquidity Crisis\nReflect on the GameStop case and connect it to your learnings from Blocks A, B, and C:\nTask 4.1: Asset Liquidity Breakdown (Block A Connection)\n\nDuring normal times, GME had tight spreads and reasonable depth. During the squeeze, spreads widened 100-1000x and depth evaporated. Which type of liquidity risk dominated initially (exogenous or endogenous)? How did it change as hedge funds were forced to cover shorts?\n\nTask 4.2: The Impossible Liquidation Timeline (Block B Connection)\n\nIn Block B, you learned that patient liquidation reduces costs but exposes you to market risk. Why couldn’t short sellers use patient liquidation for GME? What happens to the Block B tradeoff when you face margin calls?\nThe squeeze lasted about 2 weeks. If short sellers had time to liquidate over 30 days at 20% of daily volume (Block B constraint), would this have prevented the crisis? Why or why not?\n\nTask 4.3: Funding Liquidity Crisis (Block C Connection)\n\nRobinhood faced a “margin call” when DTCC increased requirements from $700M to $3B. How is this similar to a bank failing to meet its LCR requirement (Block C)? What was Robinhood’s equivalent of HQLA?\nDTCC’s margin requirements are procyclical—they increased precisely when volatility spiked. How does this compare to the haircut increases in Block C Task 3.4? What does this reveal about liquidity stress in the financial system?\n\nTask 4.4: Liquidity Wrong-Way Risk and Black Holes\n\nShort sellers needed to BUY to close positions, but each purchase pushed prices higher, triggering more margin calls and more forced buying. Trace this feedback loop and explain why this is the perfect example of liquidity wrong-way risk.\nAt the peak, GME became a “liquidity black hole”—normal price discovery broke down, spreads were enormous, and market makers couldn’t provide continuous pricing. What could have broken this cycle? What actually did break it?\n\n\n\nTheory: Systemic Liquidity Concepts\n\nLiquidity Wrong-Way Risk\nWrong-way risk occurs when liquidity needs increase precisely when asset liquidity deteriorates. The GameStop short squeeze is a textbook example:\n\nFunding needs spiked: Short sellers faced massive margin calls requiring immediate cash\nAsset liquidity vanished: Bid-ask spreads exploded, market makers stepped back\nTiming correlation: Those who needed to buy the most faced the worst execution costs\nFeedback loop: Each forced purchase made the problem worse for remaining shorts\n\nThis adverse correlation amplifies losses beyond what separate funding and market risk models would predict.\n\n\nLiquidity Black Holes\nA liquidity black hole is an extreme state where market liquidity completely evaporates:\n\nNo continuous pricing: During GME’s peak, normal bid-ask quotes disappeared for seconds or minutes at a time\nSelf-reinforcing dynamics: Each forced purchase triggered more margin calls, creating more forced purchases\nMarket freeze: Normal price discovery broke down—prices jumped in $50-100 increments rather than pennies\n\nHistorical examples: October 1987 crash, September 2008 (Lehman), March 2020 (COVID panic), January 2021 (GameStop). Once triggered, standard risk measures become unreliable because they assume continuous markets and normal liquidity.\n\n\nThe GameStop Lesson\nThe GameStop crisis revealed several crucial insights:\n\nPosition limits matter: Short interest &gt;100% of float created structural vulnerability\nProcyclical margin requirements: DTCC’s margin increases amplified the crisis\nConcentration risk: Retail flow concentrated through few platforms (Robinhood) created single points of failure\nSocial media coordination: New dynamics in market manipulation and coordination\nLiquidity is not continuous: Even major stocks can become illiquid under stress",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Liquidity Risk</span>"
    ]
  },
  {
    "objectID": "week_08.html#key-takeaways",
    "href": "week_08.html#key-takeaways",
    "title": "8  Liquidity Risk",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThis module revealed how liquidity crises unfold through three interconnected dimensions, culminating in the GameStop short squeeze—a real-world laboratory for liquidity risk.\nThe Three Dimensions of Liquidity:\n\nAsset Liquidity (Blocks A & B): Markets can become illiquid from external conditions (exogenous risk) or from your own trades (endogenous risk). Time is a luxury—patient liquidation only works when you face no margin calls.\nFunding Liquidity (Block C): Banks survive on stable funding, not just assets. LCR measures whether you can survive 30 days of stress. The death spiral: rumors → funding dries up → forced asset sales → more rumors → failure.\nSystemic Liquidity (Block D): The GameStop case showed how all three dimensions interact catastrophically. Short sellers faced wrong-way risk (needed to buy when liquidity vanished), procyclical margins (DTCC requirements tripled overnight), and a liquidity black hole (spreads widened 1000x, price discovery failed).\n\nThe Bottom Line: Liquidity risk is not about having assets—it’s about converting them to cash when you need it most, which is precisely when markets least want to provide it.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Liquidity Risk</span>"
    ]
  },
  {
    "objectID": "week_09.html",
    "href": "week_09.html",
    "title": "9  Credit Risk",
    "section": "",
    "text": "Block A: Understanding Credit Risk Components\nYou are a loan officer at Regional Bank, evaluating a portfolio of 10 loans across different borrower types. Your task is to calculate the expected loss for each loan and understand which components drive credit risk.\nDownload Credit Risk Data\nThe Excel file contains comprehensive data for all activities today. For Block A, use the Loan_Portfolio sheet, which includes 10 loans across different categories.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Credit Risk</span>"
    ]
  },
  {
    "objectID": "week_09.html#block-a-understanding-credit-risk-components",
    "href": "week_09.html#block-a-understanding-credit-risk-components",
    "title": "9  Credit Risk",
    "section": "",
    "text": "Activity 1: The Three Components of Credit Risk\nCredit risk has three multiplicative components: Probability of Default (PD), Loss Given Default (LGD), and Exposure at Default (EAD). Calculate all three for each of the 10 loans.\nTask 1.1: Probability of Default (PD)\nUse the Historical_Default_Rate_Pct column as your PD estimate for each loan.\n\nWhich borrower type has the highest average PD? Which has the lowest?\nCompare Loan L002 (BB-rated Corporate) and Loan L006 (BB-rated SME). Same rating—same PD? Why or why not?\n\nTask 1.2: Loss Given Default (LGD)\nIf the borrower defaults, you can seize and sell the collateral. How much will you lose as a percentage of your total exposure?\n\nCompare Loans L007 and L009 (both residential mortgages). Which has higher LGD and why?\nWhich loan has the highest LGD? Is it secured or unsecured?\nWhat happens to LGD when Collateral Value &gt; Exposure Amount (e.g., Loan L002)?\n\n\n\n\n\n\n\nIndustry Context: Typical Recovery Rates\n\n\n\nIn practice, recovery rates vary significantly by collateral type due to differences in liquidity and value stability:\n\n\n\nCollateral Type\nTypical Recovery Rate\nTypical LGD\n\n\n\n\nCash & Government Bonds\n95%\n5%\n\n\nResidential Property\n70%\n30%\n\n\nCommercial Real Estate\n60%\n40%\n\n\nEquipment\n50%\n50%\n\n\nInventory\n40%\n60%\n\n\nUnsecured\n10%\n90%\n\n\n\nBanks often apply haircuts to collateral values (e.g., valuing real estate at 70-80% of market value) and add workout costs (legal fees, time value) when estimating LGD for regulatory purposes.\n\n\nTask 1.3: Exposure at Default (EAD)\nFor credit lines, borrowers draw additional amounts before defaulting:\n\\[\n\\text{EAD} = \\text{Current Exposure} + (\\text{Undrawn Commitment} \\times \\text{CCF})\n\\]\nwhere Undrawn Commitment = Committed Line - Current Exposure, and CCF (Credit Conversion Factor) = 50%.\n\nWhich loans have EAD &gt; Current Exposure? Why does this matter for risk management?\n\n\n\nActivity 2: Expected Loss Calculation\nNow combine the three components you’ve calculated (PD, LGD, and EAD) to determine the Expected Loss for each loan. For each loan, calculate:\n\nEL in dollars - The expected loss amount in currency\nEL as % of EAD - The expected loss as a percentage of your exposure\n\nThen analyze:\n\nWhich loan has the highest expected loss in dollar terms? As a percentage?\nRank the top 3 loans by dollar EL. What do they have in common?\nCompare Loan L001 (BBB Corporate, Equipment collateral) and Loan L010 (AAA Corporate, Government bond collateral): Which has higher dollar EL? Why?\n\n\n\nActivity 3: Portfolio-Level Analysis\nCalculate portfolio-level statistics:\n\nTotal EAD = Sum of all EADs\nTotal Expected Loss = Sum of all ELs\nAverage Expected Loss Rate = Total EL / Total EAD\n\nThen consider:\n\nYour bank estimates unexpected loss (95th percentile) is 4× the expected loss. What is the total Credit VaR at 95% confidence? (Hint: Credit VaR includes both expected and unexpected losses)\nExpected losses should be covered by loan pricing. Which loans likely have spreads too low to cover their expected losses?\nIf you could only monitor 3 loans intensively, which would you choose?\n\n\n\n\n\n\n\nCredit VaR vs Market VaR\n\n\n\nYou’ve calculated Market VaR in previous weeks. Credit VaR follows similar logic but with key differences:\n\nMarket VaR: Continuous price changes, typically 1-10 day horizon, observable daily\nCredit VaR: Discrete default events, typically 1-year horizon, highly skewed distribution\n\nBoth measure potential losses at a confidence level, but credit losses are asymmetric (small probability of large loss) while market losses are more symmetric.\nThe calculation above (UL = 4×EL) is a simplified Credit VaR approach. More sophisticated models (CreditMetrics, CreditRisk+) explicitly model default correlations and rating migrations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Credit Risk</span>"
    ]
  },
  {
    "objectID": "week_09.html#theory-block-a-credit-risk-fundamentals",
    "href": "week_09.html#theory-block-a-credit-risk-fundamentals",
    "title": "9  Credit Risk",
    "section": "Theory Block A: Credit Risk Fundamentals",
    "text": "Theory Block A: Credit Risk Fundamentals\nIn your calculations, you discovered how three distinct components combine multiplicatively to create expected loss. Let’s formalize this framework.\n\nThe Credit Loss Distribution\nCredit risk differs fundamentally from market risk in its asymmetric and discrete nature:\n\nMarket Risk: Continuous distribution, symmetric, observable in real-time (mark-to-market)\nCredit Risk: Discrete jumps (default or no default), asymmetric (maximum gain = coupon, maximum loss = principal), latent (deterioration not observable until default)\n\n\n\n\n\n\n\nCredit Risk Components\n\n\n\nProbability of Default (PD):\n\nLikelihood of default within a time horizon (typically 1 year), expressed as percentage\nEstimated from external ratings, internal models, or market signals (CDS spreads)\n\nLoss Given Default (LGD):\n\nPercentage of exposure lost if default occurs: LGD = 1 - Recovery Rate\nDepends on collateral quality, seniority, legal framework, and economic conditions\n\nExposure at Default (EAD):\n\nDollar amount outstanding when default occurs\nFor credit lines: \\(\\text{EAD} = \\text{Drawn} + (\\text{Undrawn} \\times \\text{CCF})\\)\nCredit Conversion Factor (CCF) calibrated empirically (higher for committed lines)\n\n\n\n\n\nExpected Loss Formula\n\\[\n\\text{EL} = \\text{PD} \\times \\text{LGD} \\times \\text{EAD}\n\\]\nThis multiplicative formula creates important effects:\n\nImproving any component reduces expected loss\nComponents interact: Low PD can offset high LGD\nDiversification helps: Different exposures have different component drivers\n\n\n\nExpected Loss vs. Unexpected Loss\nYour Activity 3 revealed a critical distinction that drives all credit risk management:\nExpected Loss (EL):\n\nThe mean of the loss distribution—predictable and statistically certain\nShould be priced into the loan via interest rate spread\nCovered by loan loss provisions and reserves\nNot surprising when it materializes; part of the business model\n\nUnexpected Loss (UL):\n\nThe volatility around the mean (standard deviation of losses)\nUnpredictable credit losses that exceed EL—the true risk\nCovered by bank capital (equity)\nKey driver of regulatory capital requirements\n\n\\[\n\\text{Credit VaR}_{95\\%} \\approx \\text{EL} + 1.645 \\times \\text{UL}\n\\]\nKey Insight: Banks earn profits by pricing expected loss into loans and using capital to absorb unexpected losses. If EL is mispriced, the bank loses money even without defaults occurring.\nThis distinction explains why Block D will examine Basel III capital requirements: regulatory capital must cover unexpected losses at 99.9% confidence, not just expected losses. The separation between pricing (covers EL) and capital (covers UL) is fundamental to banking.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Credit Risk</span>"
    ]
  },
  {
    "objectID": "week_09.html#block-b-credit-migration-and-mark-to-market",
    "href": "week_09.html#block-b-credit-migration-and-mark-to-market",
    "title": "9  Credit Risk",
    "section": "Block B: Credit Migration and Mark-to-Market",
    "text": "Block B: Credit Migration and Mark-to-Market\nYou are a fixed income portfolio manager at Investment Fund. This quarter, credit rating agencies have announced rating changes for several bonds in your portfolio. Your task is to understand how credit migration affects bond values—even when bonds don’t default. Use the Bond_Portfolio sheet from the data file.\n\nActivity 4: Bond Revaluation After Rating Changes\nTask 4.1: Calculate Current Portfolio Value\n\nCalculate the current market value of each bond and the total market value of the portfolio.\nFor each bond with a rating change, calculate the new bond price and the mark-to-market impact. For simplicity, assume annual coupon payments.\n\n\nTip: first calculate the new bond yield, then price the bond using the standard present-value formula. In Excel, you can use the PV function: =-PV(yield, maturity, coupon, 100) (note the negative sign to get positive price)\n\nTask 4.2: Portfolio Impact and Accounting Treatment\n\nWhich bond experiences the largest dollar loss from rating migration?\nBond B001 was downgraded from BBB to BB. If you hold B001 to maturity (5 years) and it doesn’t default, will you recover the mark-to-market loss?\nAre rating downgrades more problematic for bonds in the trading book or banking book? Why?\nIdentify the fallen angel: Which bond(s) crossed from investment grade to high yield? Why is this bond especially problematic for the portfolio?\n\nTask 4.3: Market Signals vs Rating Agencies\n\nWhat do CDS spreads measure, and how do they relate to credit risk?\nCompare each bond’s Current CDS Spread (from the data) to the rating-implied spread (from the Credit_Spreads table, using the bond’s current rating and maturity).\nWhich bonds have CDS spreads significantly higher than their current rating suggests? What might this signal about future credit deterioration?\n\n\n\nActivity 5: Portfolio Impact and Liquidity Connections\nTask 5.1: Liquidity Risk Connection (Week 8 Callback)\nRecall from Week 8 that banks must hold High-Quality Liquid Assets (HQLA) for the Liquidity Coverage Ratio (LCR): government bonds (100% value, no haircut); corporate bonds rated AA- or better (85% value, 15% haircut); below-investment-grade bonds (not eligible for HQLA).\n\nAssume bonds B003, B004, B005, and B007 (all investment-grade) were contributing to your LCR. What happens after the rating changes?\nWrong-way risk: Explain how credit downgrades create a ‘doom loop.’",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Credit Risk</span>"
    ]
  },
  {
    "objectID": "week_09.html#theory-block-b-credit-risk-beyond-default",
    "href": "week_09.html#theory-block-b-credit-risk-beyond-default",
    "title": "9  Credit Risk",
    "section": "Theory Block B: Credit Risk Beyond Default",
    "text": "Theory Block B: Credit Risk Beyond Default\nBlock A focused on default risk—the probability that borrowers fail to repay. But credit risk is not binary. You can lose money even without defaults occurring. This is crucial for understanding trading book risks and market-based credit exposures.\n\n\n\n\n\n\nCredit Losses\n\n\n\nCredit losses come from three distinct sources:\n\nDefault Risk: Borrower fails to repay (binary event: default or no default)\nMigration Risk: Credit rating changes affect bond values\nSpread Risk: Market-wide credit spread movements independent of ratings\n\nFor trading books (mark-to-market accounting), migration and spread risk dominate daily P&L. For banking books (held-to-maturity accounting), only default risk matters if the bank holds to maturity.\n\n\nThe relationship between bond yields, spreads, and prices:\n\\[\n\\text{Bond Yield} = \\text{Risk-Free Rate} + \\text{Credit Spread}\n\\]\nWhen credit spreads widen (e.g., due to downgrade), bond prices fall to bring yields up to the new market level. The approximate price change for small spread movements:\n\\[\n\\Delta \\text{Price} \\% \\approx -\\text{Modified Duration} \\times \\Delta \\text{Spread}\n\\]\n\nThe Fallen Angel Effect: When Investment Grade Disappears\nWhen a bond falls from investment grade (BBB- or better) to high yield (BB+ or worse), it becomes a fallen angel. This triggers:\n\nForced selling by investment-grade-only mandates (mutual funds, pension funds, insurance companies)\nPrice decline beyond spread widening due to selling pressure\nHQLA reclassification: Level 2A eligible → Not eligible for LCR (Basel III)\nLiquidity evaporates: Market makers widen bid-ask spreads\nWrong-way risk: Credit downgrades trigger liquidity needs precisely when liquidity is scarce\n\n\n\nCredit Rating Transition Matrix\nThe Credit Rating Transition Matrix shows one-year probabilities of rating changes. Row = Current Rating, Column = Rating in 1 Year.\nWhy Banks Use This:\n\nEstimate portfolio losses from both defaults AND rating migrations over 1-year horizon\nRegulatory stress testing: model worst-case rating scenarios\nDefault column provides 1-year PD estimates by rating (BBB: 0.18%, BB: 0.60%, B: 3.95%)\nCalculate MTM losses from potential downgrades in trading book\nDiagonal values = rating stability (investment grade: 87-91%, high yield: 83-84%)\nDowngrade bias: Ratings tend to drift downward, especially for lower-rated credits\nCliff risk: Identifies probability of fallen angel events (BBB → BB)\n\nKey Insight: Credit quality is sticky but asymmetric—downgrades are more common than upgrades. Banks price this migration risk into spreads and capital requirements.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Credit Risk</span>"
    ]
  },
  {
    "objectID": "week_09.html#block-c-historical-default-probabilities",
    "href": "week_09.html#block-c-historical-default-probabilities",
    "title": "9  Credit Risk",
    "section": "Block C: Historical Default Probabilities",
    "text": "Block C: Historical Default Probabilities\n\nTheory: Three Ways to Measure Default Risk\nWhen analyzing credit risk, we use three interconnected measures:\n1. Cumulative (Unconditional) Default Probability - \\(Q(t)\\)\n\nProbability that a company defaults within time period \\([0,t]\\) (from now until time \\(t\\))\nAnswers: “What’s the chance they default sometime in the next \\(t\\) years?”\nAlways increases with time\nExample: \\(Q(5) = 1.54\\%\\) means 1.54% of BBB companies default within 5 years\n\n2. Marginal (Conditional) Default Probability - \\(q(t)\\)\n\nProbability of default in period \\(t\\), given survival through period \\(t-1\\)\nAnswers: “If they survived this long, what’s the chance they default next period?”\nCan increase or decrease over time\nFormula: \\(q(t) = \\frac{Q(t) - Q(t-1)}{1 - Q(t-1)}\\)\n\n3. Hazard Rate - \\(h(t)\\)\n\nInstantaneous default intensity at time \\(t\\)\nContinuous-time version of marginal PD\nFor constant hazard rate: \\(Q(t) = 1 - e^{-h \\cdot t}\\)\nTherefore: \\(h = -\\frac{1}{t} \\ln(1 - Q(t))\\)\n\nKey relationship: All three measures are mathematically linked and provide equivalent information about default risk from different perspectives.\n\n\nActivity 6: Working with Historical Default Data\nHere is actual default data from rating agencies (1981-2020):\nCumulative Default Probabilities (%):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime (years)\n1\n2\n3\n4\n5\n7\n10\n15\n\n\n\n\nAAA\n0.00\n0.03\n0.13\n0.24\n0.34\n0.51\n0.70\n0.90\n\n\nAA\n0.02\n0.06\n0.11\n0.21\n0.30\n0.49\n0.70\n0.99\n\n\nA\n0.05\n0.13\n0.22\n0.33\n0.46\n0.76\n1.20\n1.84\n\n\nBBB\n0.16\n0.43\n0.75\n1.14\n1.54\n2.27\n3.24\n4.54\n\n\nBB\n0.63\n1.93\n3.46\n4.99\n6.43\n8.89\n11.64\n14.65\n\n\nB\n3.34\n7.80\n11.75\n14.89\n17.35\n20.99\n24.62\n28.24\n\n\nCCC/C\n28.30\n38.33\n43.42\n46.36\n48.58\n50.75\n52.76\n54.76\n\n\n\nTask 6.1: Basic Interpretation and Time Patterns\n\nWhat percentage of BBB companies default within 5 years?\nIf you hold 100 B-rated bonds for 2 years, how many defaults do you expect?\nHow much does cumulative PD increase from year 1 to year 2 for AAA and CCC/C companies?\nHow much does it increase from year 4 to year 5 for these same ratings?\n\nTask 6.2: Calculating Marginal Default Probabilities\n\nCalculate marginal PDs for BBB-rated companies for years 2 and 5.\nCalculate marginal PDs for B-rated companies for years 2 and 5.\nCompare the patterns between BBB and B ratings. What do you observe?\n\nTask 6.3: Calculating Hazard Rates and Non-Integer Maturities\nBanks need default probabilities for any time horizon, not just whole years.\n\nCalculate average constant hazard rates for AA and BB companies using 5-year cumulative PDs.\nUsing these constant hazard rates, calculate the cumulative default probabilities for maturities of 6-month, 2.5-year, and 7-year.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Credit Risk</span>"
    ]
  },
  {
    "objectID": "week_09.html#block-d-market-implied-default-probabilities",
    "href": "week_09.html#block-d-market-implied-default-probabilities",
    "title": "9  Credit Risk",
    "section": "Block D: Market-Implied Default Probabilities",
    "text": "Block D: Market-Implied Default Probabilities\n\nTheory: From Historical to Market-Based Measures\nWhile historical default probabilities tell us what happened in the past, market prices tell us what investors expect to happen in the future.\nCredit Default Swaps (CDS) and Default Probabilities\nA CDS is insurance against default. The CDS spread reflects the market’s assessment of default risk:\n\nCDS Spread: Annual premium (in basis points) paid for default protection\nKey relationship: Higher default probability → Higher CDS spread\n\nRisk-Neutral vs. Historical Probabilities\n\nHistorical (Real-World) PD: Actual default frequencies from past data\nRisk-Neutral (Market-Implied) PD: Default probabilities implied by market prices\nWhy they differ: Risk-neutral PDs include risk premiums. Investors demand extra compensation for bearing default risk.\n\nSimple Approximation\nFor a constant hazard rate \\(h\\) and recovery rate \\(R\\):\n\\[\\text{CDS Spread} \\approx h \\times (1 - R)\\]\nTherefore:\n\\[h \\approx \\frac{\\text{CDS Spread}}{1 - R}\\]\nThe risk-neutral cumulative default probability is:\n\\[Q^{RN}(t) = 1 - e^{-h \\cdot t}\\]\nMore Precise Formula (when spreads are high):\n\\[Q^{RN}(t) = 1 - e^{-\\frac{s \\cdot t}{1 - R}}\\]\nwhere \\(s\\) is the CDS spread expressed as a decimal (e.g., 180 bps = 0.0180).\n\n\nActivity 7: Extracting Default Probabilities from CDS Spreads\nCurrent Market CDS Spreads (5-year, in basis points):\n\n\n\nCompany\nRating\nCDS Spread\nHistorical 5Y PD\n\n\n\n\nApple\nAA\n35 bps\n0.30%\n\n\nIBM\nA\n65 bps\n0.46%\n\n\nFord\nBBB\n180 bps\n1.54%\n\n\nTesla\nBB\n420 bps\n6.43%\n\n\nAMC\nB\n850 bps\n17.35%\n\n\n\nTask 7.1: Basic CDS Interpretation\n\nWhat is the annual cost to insure $10 million of Ford debt?\nIf you buy 5-year protection on $100 million of Tesla debt, what’s your total premium over 5 years (ignoring discounting)?\nWhich company shows the biggest gap between market-implied risk (CDS spread) and historical default probability?\n\nTask 7.2: Calculating Market-Implied Default Probabilities\nAssume 40% recovery rate for all companies. For IBM, Ford, and Tesla, calculate:\n\nImplied hazard rates\nRisk-neutral PDs\nCompare risk-neutral PDs to historical PDs from Activity 6. What pattern emerges?\n\nTask 7.3: Bond vs. CDS Arbitrage Check\nFord’s 5-year bond yields 5.2% while the 5-year Treasury yields 3.5%.\n\nCredit spread from bond = 170 bps\nCDS spread = 180 bps\nWhy might these differ?\nWhat trade could you execute if the difference widened to 50 bps?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Credit Risk</span>"
    ]
  },
  {
    "objectID": "week_09.html#key-takeaways",
    "href": "week_09.html#key-takeaways",
    "title": "9  Credit Risk",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nExpected loss (PD × LGD × EAD) measures average credit losses, while Credit VaR captures unexpected losses that determine capital requirements\nRecovery rates vary significantly by seniority and collateral, with senior secured bonds recovering substantially more than subordinated debt\nCredit migration risk creates mark-to-market losses even without default, as rating downgrades widen spreads and reduce bond values\nDefault probabilities can be measured three ways (cumulative, marginal, hazard rate) and allow calculation of default risk for any time horizon\nHistorical default rates increase sharply with lower credit ratings, demonstrating the nonlinear relationship between rating quality and default risk\nMarket-implied probabilities from CDS spreads exceed historical default rates because they include risk premiums and reflect forward-looking investor expectations",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Credit Risk</span>"
    ]
  },
  {
    "objectID": "week_10.html",
    "href": "week_10.html",
    "title": "10  Operational Risk",
    "section": "",
    "text": "Block A: Understanding Operational Loss Patterns",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Operational Risk</span>"
    ]
  },
  {
    "objectID": "week_10.html#block-a-understanding-operational-loss-patterns",
    "href": "week_10.html#block-a-understanding-operational-loss-patterns",
    "title": "10  Operational Risk",
    "section": "",
    "text": "Theory: Operational Risk Definition\nOperational risk refers to potential losses from failed internal processes, human errors, system failures, or external events. Unlike credit or market risk, operational risk is challenging to quantify due to its diverse and unpredictable nature.\n\n\n\n\n\n\nBasel Definition\n\n\n\n“The risk of loss resulting from inadequate or failed internal processes, people, and systems, or from external events.”\n\nIncludes: Legal risk (lawsuits, regulatory penalties)\nExcludes: Reputation risk, strategic decision risks\n\n\n\n\n\nActivity 1: Loss Data Analysis\nYou are a newly appointed operational risk analyst at a major European bank. Your risk committee has asked you to analyze the bank’s operational loss history over the past decade.\n  Download Operational Risk Data \nOpen the Excel file and navigate to Activity_1_Loss_Data sheet.\nTask 1.1: Categorize each of the 30 operational losses using Basel’s seven categories (reference table provided in the sheet). Enter your category number (1-7) in the “Basel_Category” column.\nTask 1.2: Create a summary table showing:\n\nTotal losses (EUR millions) for each category\nNumber of loss events for each category\nAverage loss size for each category\n\nTask 1.3: Based on your analysis, identify:\n\nWhich categories are high-frequency, low-severity?\nWhich categories are low-frequency, high-severity?\nWhat implications do these patterns have for risk management?\n\nTask 1.4 Discussion: Looking at the distribution of loss amounts, how would you describe the shape of this distribution? What statistical challenges does this create for modeling operational risk?\n\n\nTheory: Loss Frequency and Severity Distributions\nYour analysis in Activity 1 revealed a fundamental characteristic of operational risk: losses are not normally distributed. Most losses are small, but a few are catastrophic—this is known as a heavy-tailed distribution.\n\nThe Seven Basel Categories\nBasel III classified operational risks into seven distinct types:\n\nInternal Fraud: Intentional acts by employees to deceive or circumvent laws\n\nExamples: Rogue trading, insider trading, embezzlement\n\nExternal Fraud: Fraudulent activities by third parties\n\nExamples: Cyber attacks, phishing, robbery\n\nEmployment Practices and Workplace Safety: Violations of employment laws\n\nExamples: Discrimination lawsuits, wrongful termination, workplace injuries\n\nClients, Products, and Business Practices: Failures to meet obligations or ethical standards\n\nExamples: Fiduciary breaches, data leaks, money laundering\n\nDamage to Physical Assets: Losses from external events damaging tangible assets\n\nExamples: Fires, floods, earthquakes, terrorism\n\nBusiness Disruption and System Failures: Disruptions due to technological failures\n\nExamples: Server outages, hardware malfunctions, power failures\n\nExecution, Delivery, and Process Management: Transaction processing failures\n\nExamples: Settlement errors, missed deadlines, documentation failures\n\n\n\n\nModeling Loss Frequency\nThe number of loss events in a given time period (typically one year) is commonly modeled using the Poisson distribution:\n\\[\nP(n) = e^{-\\lambda T} \\frac{(\\lambda T)^n}{n!}\n\\]\nwhere:\n\n\\(P(n)\\) = Probability of observing \\(n\\) losses in time period \\(T\\)\n\\(\\lambda\\) = Average annual frequency of losses\n\\(T\\) = Time period (usually \\(T = 1\\) year)\n\nInterpretation: If a bank historically experiences \\(\\lambda = 5\\) internal fraud events per year, the Poisson distribution tells us the probability of experiencing exactly 3, 7, or 10 events next year.\n\n\nModeling Loss Severity\nGiven that a loss occurs, its magnitude is often modeled using the lognormal distribution, which naturally captures:\n\nPositive values only (losses cannot be negative)\nRight-skewed shape (many small losses, few large ones)\nHeavy tail (extreme losses are rare but possible)\n\n\n\nMonte Carlo Simulation\nTo estimate the total annual operational risk loss, banks combine these distributions through simulation:\n\nSimulate frequency: Draw \\(n\\) from Poisson(\\(\\lambda\\)) → “How many losses this year?”\nSimulate severity: For each of the \\(n\\) losses, draw loss amount from lognormal distribution\nAggregate: Sum all simulated losses to get total annual loss\nRepeat: Run 10,000+ scenarios to build the total loss distribution\n\nKey Challenge: Banks have limited internal loss data, especially for rare but severe events. This necessitates:\n\nExternal data: Industry consortium databases (but must be scaled to institution size)\nScenario analysis: Expert judgment for events that haven’t occurred but could",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Operational Risk</span>"
    ]
  },
  {
    "objectID": "week_10.html#block-b-calculating-regulatory-capital-under-sma",
    "href": "week_10.html#block-b-calculating-regulatory-capital-under-sma",
    "title": "10  Operational Risk",
    "section": "Block B: Calculating Regulatory Capital under SMA",
    "text": "Block B: Calculating Regulatory Capital under SMA\n\nTheory: The Standardized Measurement Approach (SMA)\nRegulators require banks to hold capital against potential operational losses. The Standardized Measurement Approach (SMA) is Basel’s methodology for calculating this capital requirement. Introduced in December 2017 and implemented in January 2023, the SMA replaced earlier approaches (including the Advanced Measurement Approach, or AMA) with a single, consistent methodology applicable to all banks.\n\nOverview of the SMA Framework\nThe SMA combines two elements:\n\nSize-based capital: The Business Indicator Component (BIC) scales with bank size and complexity\nLoss-based adjustment: The Internal Loss Multiplier (ILM) adjusts capital based on actual loss experience\n\n\\[\n\\text{Operational Risk Capital} = \\text{BIC} \\times \\text{ILM}\n\\]\nFor small banks (Business Indicator &lt; €1 billion), capital equals BIC without adjustment (ILM = 1).\nLet’s examine each component systematically.\n\n\nComponent 1: The Business Indicator (BI)\nThe Business Indicator measures bank size and operational complexity through three categories of financial activity:\n\\[\n\\text{BI} = \\text{ILDC} + \\text{SC} + \\text{FC}\n\\]\nwhere:\nILDC (Interest, Leases, Dividend Component):\n\\[\n\\text{ILDC} = |\\text{Interest Income} - \\text{Interest Expense}| + \\text{Dividend Income}\n\\]\nSC (Services Component):\n\\[\n\\text{SC} = \\text{Fee Income} + \\text{Other Service Income}\n\\]\nFC (Financial Component):\n\\[\n\\text{FC} = |\\text{Trading Gains/Losses}| + |\\text{Investment Gains/Losses}|\n\\]\nKey principle: Absolute values (| |) ensure that losses don’t artificially reduce the BI. A bank with large trading losses has high operational complexity and should not receive lower capital requirements.\nInterpretation: The BI captures the scale of a bank’s operations across lending, service provision, and trading activities. Larger BI indicates greater operational exposure.\n\n\nComponent 2: The Business Indicator Component (BIC)\nThe Business Indicator Component converts the BI into a baseline capital requirement using a progressive three-tier structure:\nSmall banks (BI &lt; €1 billion):\n\\[\n\\text{BIC} = 0.12 \\times \\text{BI}\n\\]\nMedium banks (€1 billion ≤ BI &lt; €30 billion):\n\\[\n\\text{BIC} = 0.12 + 0.15 \\times (\\text{BI} - 1)\n\\]\nLarge banks (BI ≥ €30 billion):\n\\[\n\\text{BIC} = 4.47 + 0.18 \\times (\\text{BI} - 30)\n\\]\nUnderstanding the coefficients:\n\nSmall banks: 12% of BI\nMedium banks: €120M base + 15% marginal rate on amounts above €1 billion\nLarge banks: €4.47 billion base + 18% marginal rate on amounts above €30 billion\n\nWhy progressive rates? Operational risk doesn’t scale linearly with size. Larger banks face disproportionately higher risks due to:\n\nGreater organizational complexity\nMore interconnected systems\nHigher systemic importance\nMore extensive regulatory obligations\nIncreased reputational exposure\n\nThe increasing marginal rates (12% → 15% → 18%) reflect this nonlinear relationship.\n\n\nComponent 3: Historical Loss Data\nBanks must collect all operational loss events over a rolling 10-year period. This includes losses from all seven Basel categories, regardless of amount (though small losses below €20,000 may be excluded).\nAverage Annual Loss = (Sum of 10 years of losses) ÷ 10\nThis average captures the bank’s actual loss experience, providing a backward-looking measure of operational risk profile.\n\n\nComponent 4: The Loss Component (LC)\nThe Loss Component scales historical losses to reflect the capital needed for extreme outcomes:\n\\[\n\\text{LC} = 15 \\times \\text{Average Annual Loss (past 10 years)}\n\\]\nWhy multiply by 15?\nThe multiplier of 15 reflects several principles:\n\nConfidence level: Targets approximately 99.9% confidence over a one-year horizon\nHistorical calibration: Derived from empirical analysis of industry-wide loss distributions\nExpected + Unexpected losses: The 10-year average captures “normal” years; multiplying by 15 provides a buffer for rare but severe events\nIndustry standard: All banks apply the same multiplier, ensuring consistency\n\nInterpretation: If a bank averaged €10 million in annual losses over the past decade, LC = €150 million represents the capital needed to absorb a bad year (at 99.9% confidence).\n\n\nComponent 5: The Internal Loss Multiplier (ILM)\nThe Internal Loss Multiplier adjusts baseline capital (BIC) based on how the bank’s loss experience compares to its size:\nSmall banks (BI &lt; €1 billion):\n\\[\n\\text{ILM} = 1\n\\]\nSmall banks don’t receive adjustments—their capital equals BIC regardless of loss history.\nOther banks (BI ≥ €1 billion):\n\\[\n\\text{ILM} = \\ln \\left[ e - 1 + \\left( \\frac{\\text{LC}}{\\text{BIC}} \\right)^{0.8} \\right]\n\\]\nwhere:\n\n\\(e\\) = Euler’s constant (≈ 2.718)\n\\(\\ln\\) = Natural logarithm\n\nUnderstanding ILM behavior:\n\n\n\n\n\n\n\n\n\nScenario\nLC/BIC Ratio\nILM Value\nInterpretation\n\n\n\n\nAverage performance\nLC = BIC\nILM = 1.0\nNo adjustment to capital\n\n\nBetter than average\nLC &lt; BIC\nILM &lt; 1.0\nCapital reward (reduction)\n\n\nWorse than average\nLC &gt; BIC\nILM &gt; 1.0\nCapital penalty (increase)\n\n\nPerfect record\nLC = 0\nILM ≈ 0.54\nMaximum 46% capital reduction\n\n\nVery poor record\nLC &gt;&gt; BIC\nILM &gt;&gt; 1.0\nUnbounded penalty\n\n\n\nWhy this formula? The logarithmic structure ensures:\n\nDiminishing returns: Banks with excellent records can’t reduce capital too much (floor at ILM ≈ 0.54)\nEscalating penalties: Banks with poor records face increasing penalties (no ceiling on ILM)\nSmooth transitions: The function is continuous and mathematically stable\nIncentive alignment: Small improvements in loss performance yield tangible capital relief\n\n\n\nComponent 6: Final Capital Requirement\nThe SMA capital requirement combines BIC and ILM:\nSmall banks (BI &lt; €1 billion):\n\\[\n\\text{Operational Risk Capital} = \\text{BIC}\n\\]\nOther banks (BI ≥ €1 billion):\n\\[\n\\text{Operational Risk Capital} = \\text{BIC} \\times \\text{ILM}\n\\]\nInterpretation:\n\nSmall banks cannot reduce capital below BIC, even with excellent loss history\nMedium and large banks can earn capital reductions (ILM &lt; 1) or face penalties (ILM &gt; 1) based on performance\nThis structure rewards institutions that invest in operational risk management\n\n\n\n\nActivity 2: SMA Capital Calculation\nNavigate to Activity_2_SMA_Calculator sheet in your Excel file.\nYou have data for two banks:\n\nBank A: Small regional bank\nBank B: Large international bank\n\nTask 2.1: Calculate the complete SMA capital requirement for both banks by working through all components:\n\nBusiness Indicator (BI): Using the “Income Components” table, calculate ILDC, SC, and FC, then sum to get BI. Classify the bank by size (small/medium/large).\nBusiness Indicator Component (BIC): Apply the appropriate formula based on bank size classification.\nLoss Component (LC): Using the “Historical Operational Losses” table, calculate the 10-year average annual loss and multiply by 15.\nInternal Loss Multiplier (ILM): For small banks, record ILM = 1. For other banks, apply the ILM formula showing intermediate steps.\nFinal Operational Risk Capital: Apply the appropriate formula based on bank size.\n\nTask 2.2: Answer the following questions based on your calculations:\n\nWhich bank holds more operational risk capital in absolute terms (EUR millions)?\nWhich bank holds more operational risk capital as a percentage of its Business Indicator?\n\nTask 2.3: Explore how historical loss performance affects capital of Bank B:\n\nWhat is Bank B’s ILM with its actual loss history?\nIf LC = BIC (average performance), what would ILM be? What would capital be?\nIf Bank B’s losses doubled (LC = 2 × BIC), what would ILM and capital be?\nIf Bank B had zero losses over 10 years (LC = 0), what would ILM and capital be?\nBased on these scenarios, explain the economic incentive created by the ILM structure. How does it encourage better risk management?\nWhy does Bank A not receive any adjustment for its loss performance, even if it has an excellent record?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Operational Risk</span>"
    ]
  },
  {
    "objectID": "week_10.html#block-c-preventing-operational-risk-losses",
    "href": "week_10.html#block-c-preventing-operational-risk-losses",
    "title": "10  Operational Risk",
    "section": "Block C: Preventing Operational Risk Losses",
    "text": "Block C: Preventing Operational Risk Losses\nWhile capital protects against losses, prevention is far more valuable. Let’s explore how banks proactively manage operational risk.\n\nActivity 3: Designing Risk Controls and KRIs\nScenario: Your bank recently experienced an operational loss event:\nA trader in the commodities desk accumulated unauthorized positions over six months, resulting in a €42 million loss when market prices moved adversely. The trader concealed the positions by exploiting a gap in the trade confirmation system. The fraud was only discovered when the trader took sick leave and a colleague reviewed the portfolio.\nTask 3.1: Which of Basel’s seven categories does this loss fall under?\nTask 3.2: Identify at least three root causes that enabled this loss to occur. Consider:\n\nProcess failures (what controls were missing?)\nSystem failures (what technology gaps existed?)\nPeople failures (what behavioral red flags were missed?)\n\nTask 3.3: Design four Key Risk Indicators (KRIs) that could have provided early warning of this unauthorized trading. For each KRI:\n\nDefine the metric clearly\nSpecify a threshold that would trigger investigation\nExplain why this metric would detect similar problems\n\nTask 3.4 Your risk committee proposes three responses:\n\nInvest in prevention: Upgrade trade monitoring system (€5M upfront cost)\nAccept and insure: Purchase operational risk insurance with €10M deductible (€2M annual premium)\nHold capital only: Allocate SMA capital but no additional controls\n\nDiscuss the advantages and disadvantages of each approach. When might each be appropriate?\n\n\nTheory Block C: Risk Control and Self-Assessment (RCSA)\nRisk Control and Self-Assessment (RCSA) is a structured process where business units:\n\nIdentify risks: Map processes and identify failure points\nAssess impact: Estimate potential loss frequency and severity\nEvaluate controls: Review existing safeguards\nClose gaps: Implement additional controls where needed\nMonitor: Track effectiveness over time\n\nKey Risk Indicators (KRIs) serve as early warning signals for emerging operational risks. Effective KRIs have these characteristics:\n\nForward-looking: Predict problems before they materialize\nMeasurable: Objective, quantifiable metrics\nActionable: Clear threshold triggers investigation\nRelevant: Directly linked to specific risk types\n\nCommon KRI Examples:\n\n\n\n\n\n\n\n\nRisk Type\nKRI Example\nWarning Threshold\n\n\n\n\nInternal Fraud\n% employees not taking 10+ consecutive leave days\n&gt; 15%\n\n\nSystem Failures\nSystem downtime hours per month\n&gt; 8 hours\n\n\nProcess Errors\nFailed transaction rate\n&gt; 0.5%\n\n\nCyber Risk\nPhishing click rate in training\n&gt; 10%\n\n\nTurnover Risk\nStaff turnover in critical functions\n&gt; 20% annually\n\n\nOversight Risk\nSupervisor-to-staff ratio\n&lt; 1:12",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Operational Risk</span>"
    ]
  },
  {
    "objectID": "week_10.html#block-d-operational-risk-in-action",
    "href": "week_10.html#block-d-operational-risk-in-action",
    "title": "10  Operational Risk",
    "section": "Block D: Operational Risk in Action",
    "text": "Block D: Operational Risk in Action\nLet’s examine how operational risk manifests in real crises and interacts with other risk types.\n\nActivity 4: Case Study Analysis\nInstructions: Navigate to Activity_4_Case_Study sheet in your Excel file.\nBackground: In September 2023, GlobalBank International announced a €1,850 million loss from unauthorized trading activities. A senior trader had built large, hidden positions in equity derivatives over 18 months. When discovered, the bank was forced to unwind positions rapidly in adverse market conditions.\nReview the case study data in the Excel sheet.\nTask 4.1: Calculate the impact on GlobalBank’s SMA capital:\n\nWhat was the bank’s pre-loss ILM? (Use pre-loss average annual losses)\nWhat is the new average annual loss including this event?\nWhat is the new ILM and capital requirement?\nBy how much (EUR millions) did the capital requirement increase?\n\nTask 4.2: This single loss was classified as “Internal Fraud” (Basel Category 1). But identify at least three other risk types that were affected or amplified by this operational failure:\n\nConsider market risk, liquidity risk, credit risk, reputation risk\nExplain the connection for each\n\nTask 4.3: The trader exploited gaps in the trade confirmation system and supervisory oversight. Which of the following would have been most effective at preventing this loss?\n\nHigher capital requirements\nBetter Key Risk Indicators\nOperational risk insurance\nMandatory rotation of traders\nReal-time position monitoring system\n\nTask 4.4 (Discussion): After major operational losses, regulators often increase capital requirements for the entire industry (not just the affected bank). Is this fair? What is the regulatory logic?\n\n\nTheory Block D: Operational Risk as a Systemic Amplifier\nYour analysis of the GlobalBank case revealed a critical insight: operational risk rarely exists in isolation. It amplifies other risk types and can trigger systemic crises.\n\nWrong-Way Risk Connections\nOperational → Market Risk:\n\nTrading system failure during volatile markets\nInability to execute hedges when most needed\nForced liquidation of positions (as in GlobalBank case)\n\nOperational → Liquidity Risk:\n\nCyber attack disrupts payment systems → funding crisis\nSystem failure prevents access to central bank facilities\nReputation damage → depositor flight\n\nOperational → Credit Risk:\n\nData breach reveals weak controls → credit rating downgrade\nProcessing failure causes client defaults\nLoss of key personnel → deteriorating underwriting\n\n\n\nThe Cyber Risk Challenge\nCyber risk has emerged as the fastest-growing operational risk category:\nDistinctive characteristics:\n\nRapidly evolving: Attack methods change constantly\nAsymmetric threat: Low-cost attacks can cause massive losses\nInterconnected: One bank’s breach can affect the entire system\nHard to insure: Correlated losses make diversification difficult\n\nManagement approaches:\n\nContinuous employee education (humans are the weakest link)\nMulti-layered technical defenses\nIncident response planning\nIndustry information sharing\nRegulatory stress testing (e.g., ECB cyber resilience tests)\n\n\n\n\n\n\n\nKey Insight\n\n\n\nUnlike credit or market risk, operational risk cannot be diversified away or hedged. It must be prevented, monitored, and capitalized. This makes operational risk management fundamentally about organizational culture, processes, and systems—not just quantitative models.\n\n\n\n\nCapital Allocation and Incentives\nOperational risk capital must be allocated to business units strategically:\nGood allocation principles:\n\nReducing operational risk → Lower allocated capital → Higher ROE for the unit\nUnits bear the cost of their operational risk profile\nEncourages investment in prevention when cost-effective\n\nGovernance requirements (Basel Committee):\n\nActive board oversight of operational risk program\nRegular review of loss events and lessons learned\nIndependent operational risk function\nClear accountability for risk owners",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Operational Risk</span>"
    ]
  },
  {
    "objectID": "week_10.html#key-takeaways",
    "href": "week_10.html#key-takeaways",
    "title": "10  Operational Risk",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nAfter completing this week’s activities, you should understand these core principles:\n\nOperational risk is heterogeneous: Seven distinct categories with different frequency/severity profiles require tailored management approaches.\nHeavy-tailed distributions: Most operational losses are small, but extreme losses drive capital requirements. This challenges traditional statistical methods.\nSMA combines size and experience: Larger banks hold more capital (BIC), adjusted by historical loss performance (ILM). This rewards good risk management.\nPrevention beats capital: While capital protects against losses, early detection through KRIs and strong controls is far more valuable.\nOperational risk amplifies other risks: Failures rarely occur in isolation—they trigger market, liquidity, and credit consequences, making operational risk systemically important.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Operational Risk</span>"
    ]
  },
  {
    "objectID": "week_11.html",
    "href": "week_11.html",
    "title": "11  Bank Regulation, FRTB, and Economic Capital",
    "section": "",
    "text": "Block A: The Evolution of Bank Capital Regulation",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bank Regulation, FRTB, and Economic Capital</span>"
    ]
  },
  {
    "objectID": "week_11.html#block-a-the-evolution-of-bank-capital-regulation",
    "href": "week_11.html#block-a-the-evolution-of-bank-capital-regulation",
    "title": "11  Bank Regulation, FRTB, and Economic Capital",
    "section": "",
    "text": "Why Bank Regulation Evolved\nBank regulation ensures financial institutions maintain adequate capital to manage risks and safeguard economic stability. Before 1988, countries applied inconsistent capital-to-asset ratios, creating competitive imbalances and failing to address risks from international lending and OTC derivatives. The 1988 Basel Accord (Basel I) introduced the first international risk-based standards, but its oversimplification led to regulatory arbitrage and inadequate risk capture.\nThe 2007-2009 Global Financial Crisis exposed critical weaknesses in Basel II, which coincidentally was implemented just as the crisis began. Banks using internal models had underestimated risks, particularly in trading books. The crisis prompted a comprehensive regulatory overhaul: Basel II.5 (2011), Basel III (2010-2023), and the Fundamental Review of the Trading Book (FRTB).\nKey timeline:\n\n1988: Basel I - First international capital standards (8% minimum, risk-weighted assets)\n2007: Basel II - Three pillars (minimum capital, supervisory review, market discipline)\n2008: Global Financial Crisis - Revealed Basel II weaknesses\n2011: Basel II.5 - Stressed VaR, incremental risk charge\n2010-2023: Basel III - Higher capital quality, liquidity requirements, leverage ratio\n2019-TBD: FRTB - Expected Shortfall, liquidity horizons, stricter book boundaries\n\n\n\nBasel I: The Foundation and Its Flaws\nThe 1988 BIS Accord introduced risk-weighted assets (RWA) with five risk weight categories (0%, 10%, 20%, 50%, 100%). While groundbreaking, it had significant limitations:\nRisk-Weighted Assets Formula:\n\\[\n\\text{RWA} = \\sum_{i=1}^{N} w_i L_i\n\\]\nwhere \\(w_i\\) is the risk weight and \\(L_i\\) is the principal amount.\nCapital requirement: Banks must maintain capital ≥ 8% of total RWA, with at least 4% as Tier 1 (core) capital.\nCritical flaws:\n\nCrude risk buckets: All corporate loans = 100% risk weight regardless of credit quality (AAA same as B-rated)\nNo market risk: Only credit risk covered initially (1996 Amendment added market risk)\nRegulatory arbitrage: Banks could structure transactions to minimize capital while maintaining risk exposure\nNo operational risk: Completely ignored\n\nExample of regulatory arbitrage: A bank could securitize high-quality mortgages (50% risk weight) and retain low-quality ones, reducing regulatory capital while increasing actual risk.\n\n\nActivity A: Basel I Risk-Weighted Assets Calculation\nScenario: You are analyzing a bank’s loan portfolio under Basel I rules. Calculate the total risk-weighted assets and minimum capital requirement.\nBank’s balance sheet:\n\n\n\nAsset Type\nPrincipal ($ million)\nBasel I Risk Weight\n\n\n\n\nCash and reserves\n50\n0%\n\n\nGovernment bonds (OECD)\n200\n0%\n\n\nInterbank loans\n100\n20%\n\n\nResidential mortgages\n500\n50%\n\n\nCorporate loans (AAA-rated)\n300\n100%\n\n\nCorporate loans (B-rated)\n200\n100%\n\n\nTotal assets\n1,350\n\n\n\n\nQuestions:\n\nCalculate total risk-weighted assets\nCalculate minimum total capital requirement (8% of RWA)\nCalculate minimum Tier 1 capital requirement (4% of RWA)\nWhy is it problematic that AAA and B-rated corporate loans have the same risk weight?\nIf the bank securitizes $200M of residential mortgages (moves off balance sheet), how does RWA change? Does risk actually decrease?\n\nDiscussion points:\n\nHow might a bank exploit these rules to reduce capital while maintaining (or increasing) risk?\nWhy did Basel I fail to prevent the 2008 crisis?\nWhat improvements did Basel II introduce? (Internal ratings-based approach for credit risk)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bank Regulation, FRTB, and Economic Capital</span>"
    ]
  },
  {
    "objectID": "week_11.html#block-b-fundamental-review-of-the-trading-book-frtb",
    "href": "week_11.html#block-b-fundamental-review-of-the-trading-book-frtb",
    "title": "11  Bank Regulation, FRTB, and Economic Capital",
    "section": "Block B: Fundamental Review of the Trading Book (FRTB)",
    "text": "Block B: Fundamental Review of the Trading Book (FRTB)\n\nFrom VaR to Expected Shortfall\nYou’ve already studied Value-at-Risk (VaR) and Expected Shortfall (ES) as risk measures. The FRTB represents the Basel Committee’s shift from VaR to ES for regulatory capital calculations, recognizing that VaR failed to capture tail risk during the 2007-2009 crisis.\nEvolution of market risk capital:\n\nBasel I (1996 Amendment): 10-day VaR at 99% confidence\nBasel II.5 (2011): Added stressed VaR (VaR + sVaR) to capture crisis conditions\nFRTB (2019): Replaced VaR with Expected Shortfall at 97.5% confidence\n\nWhy Expected Shortfall?\n\nVaR answers: “What is the maximum loss in 99% of cases?”\nES answers: “What is the average loss in the worst 2.5% of cases?”\nES is coherent (subadditive), captures tail risk, and provides information about extreme losses\n\n\n\nLiquidity Horizons: The Key FRTB Innovation\nThe FRTB’s most significant innovation is liquidity horizons - the time required to exit or hedge a risk position in a stressed market. Different risk factors have different liquidity horizons, reflecting how quickly positions can be unwound without significant price impact.\nFRTB liquidity horizon structure:\n\n\n\nRisk Factor\nLiquidity Horizon\n\n\n\n\nLarge-cap equity, major FX, government bonds\n10 days\n\n\nSmall-cap equity, corporate bonds (IG)\n20 days\n\n\nVolatility (equity, FX)\n20 days\n\n\nCorporate bonds (high-yield)\n40 days\n\n\nStructured credit products\n60 days\n\n\nExotic products\n120 days\n\n\n\nCapital calculation under FRTB:\n\\[\n\\text{ES}_{\\text{total}} = \\sqrt{\\text{ES}_{10d}^2 + \\text{ES}_{20d}^2 + \\text{ES}_{40d}^2 + \\text{ES}_{60d}^2 + \\text{ES}_{120d}^2}\n\\]\nwhere each \\(\\text{ES}\\) term represents risk factors within that liquidity horizon.\nPractical implication: Illiquid positions (exotic derivatives, structured credit) now require significantly more capital than liquid positions (government bonds, major FX pairs), properly reflecting the higher risk of being unable to exit in stressed markets.\n\n\nTrading Book vs. Banking Book\nFRTB introduced stricter criteria to prevent regulatory arbitrage between books:\n\nTrading book: Instruments held for short-term profit, marked-to-market daily, subject to market risk capital (ES-based)\nBanking book: Instruments held to maturity, marked-to-model or amortized cost, subject to credit risk capital (probability of default-based)\n\nWhy this matters: Pre-FRTB, banks could shift positions between books to minimize capital. For example, moving illiquid credit instruments to the trading book (lower capital under Basel II.5) even though they couldn’t actually be traded.\nFRTB restrictions:\n\nClear intent required for trading book classification\nPresumptive lists of instruments for each book\nOnce assigned, transfers between books severely restricted\nAny capital benefit from transfer is disallowed\n\n\n\nActivity B: FRTB Capital Calculation with Liquidity Horizons\nScenario: A trading desk holds the following positions. Calculate the capital requirement under FRTB, considering liquidity horizons.\nRisk factor exposures (10-day 97.5% ES):\n\n\n\nRisk Factor\n10-day ES ($M)\nLiquidity Horizon\n\n\n\n\nS&P 500 equity\n5.0\n10 days\n\n\nUSD/EUR FX\n2.0\n10 days\n\n\nInvestment-grade corporate bonds\n8.0\n20 days\n\n\nHigh-yield corporate bonds\n12.0\n40 days\n\n\nEquity volatility (VIX)\n3.0\n20 days\n\n\nCollateralized loan obligations (CLOs)\n15.0\n60 days\n\n\n\nQuestions:\n\nFor risk factors with liquidity horizons &gt; 10 days, scale the ES appropriately:\n\n20-day ES = 10-day ES × √2\n40-day ES = 10-day ES × √4 = 10-day ES × 2\n60-day ES = 10-day ES × √6\n\nGroup risk factors by liquidity horizon and sum ES within each group\nCalculate total ES using the formula: \\[\\text{ES}_{\\text{total}} = \\sqrt{\\sum_{h} \\text{ES}_h^2}\\] where \\(h\\) represents each liquidity horizon\nCompare to simple sum of 10-day ES values (old approach). What percentage reduction does the FRTB formula provide?\nWhich position contributes most to capital? Why?\n\nDiscussion points:\n\nA bank wants to classify a portfolio of BBB-rated corporate bonds as “trading book.” They claim they can sell them any time. Should the regulator allow this? What evidence would you require?\nHow do liquidity horizons address the lesson from 2008 when “liquid” markets suddenly froze?\nWhat unintended consequences might FRTB create? (Hint: Will banks avoid illiquid products entirely?)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bank Regulation, FRTB, and Economic Capital</span>"
    ]
  },
  {
    "objectID": "week_11.html#block-c-basel-iii-capital-framework",
    "href": "week_11.html#block-c-basel-iii-capital-framework",
    "title": "11  Bank Regulation, FRTB, and Economic Capital",
    "section": "Block C: Basel III Capital Framework",
    "text": "Block C: Basel III Capital Framework\n\nCapital Quality and Quantity\nBasel III, finalized in 2017 and fully implemented in January 2023, dramatically increased both the quality and quantity of required capital. The framework eliminated Tier 3 capital and redefined capital tiers to ensure loss-absorbing capacity.\nCapital tier definitions:\n\nCommon Equity Tier 1 (CET1) - Highest quality:\n\nShare capital and retained earnings\nMinus deductions: goodwill, intangible assets, deferred tax assets, pension deficits\nMinimum: 4.5% of RWA\n\nAdditional Tier 1 (AT1):\n\nNon-cumulative perpetual preferred stock\nContingent convertible bonds (CoCos) meeting strict criteria\nTotal Tier 1 minimum: CET1 + AT1 ≥ 6% of RWA\n\nTier 2:\n\nSubordinated debt (minimum 5-year maturity)\nTotal capital minimum: Tier 1 + Tier 2 ≥ 8% of RWA\n\n\n\n\nCapital Buffers\nBeyond minimum requirements, Basel III introduced buffers to ensure banks can absorb losses during stress:\n1. Capital Conservation Buffer (CCB):\n\nAdditional 2.5% of RWA in CET1 capital\nBrings effective CET1 minimum to 7% (4.5% + 2.5%)\nIf breached, automatic restrictions on dividends, bonuses, and share buybacks\n\n2. Countercyclical Buffer (CCyB):\n\n0% to 2.5% of RWA, set by national regulators\nActivated during credit booms to build extra capital\nReleased during downturns to allow lending\nMust be met with CET1 capital\n\nTotal CET1 requirement: 4.5% (minimum) + 2.5% (CCB) + 0-2.5% (CCyB) = 7% to 9.5%\n\n\nLeverage Ratio\nThe leverage ratio provides a non-risk-weighted backstop to prevent excessive leverage:\n\\[\n\\text{Leverage Ratio} = \\frac{\\text{Tier 1 Capital}}{\\text{Total Exposure}}\n\\]\n\nMinimum: 3% (some jurisdictions require higher)\nTotal Exposure includes: on-balance sheet assets, derivatives, securities financing, off-balance sheet commitments\nPurpose: Prevents underestimation of risk-weighted assets and limits absolute leverage\n\nWhy both ratios matter: A bank can be compliant on RWA-based ratios but fail leverage ratio if it holds many low-risk-weight assets (e.g., government bonds). The binding constraint determines actual lending capacity.\n\n\nGlobal Systemically Important Banks (G-SIBs)\nG-SIBs are banks whose failure could destabilize the global financial system. Designation is based on size, interconnectedness, substitutability, complexity, and cross-jurisdictional activity.\nG-SIB capital surcharges:\n\nBucket 1: +1.0% CET1\nBucket 2: +1.5% CET1\nBucket 3: +2.0% CET1\nBucket 4: +2.5% CET1\nBucket 5: +3.5% CET1\n\nCurrent G-SIBs: The Financial Stability Board (FSB) publishes an annual list at www.fsb.org. As of 2023, approximately 30 banks are designated as G-SIBs, including JPMorgan Chase, HSBC, Bank of America, Citigroup, and others.\n\n\nContingent Convertible Bonds (CoCos)\nCoCos are hybrid debt instruments that convert to equity or are written down when a bank’s capital falls below a trigger level. They provide automatic recapitalization during stress.\nKey features:\n\nHigher interest rates than regular debt (compensates for conversion risk)\nTrigger event: Typically CET1 ratio falling below 5.125% or 7%\nConversion mechanism: Convert to equity (diluting shareholders) or write-down (investors lose principal)\nCan count as AT1 capital if meeting Basel III criteria\n\nBenefits:\n\nFor banks: Enhances equity during stress while offering cheaper funding during normal times\nFor regulators: Automatic recapitalization reduces need for taxpayer bailouts\nFor investors: Higher yield compensates for tail risk\n\nNotable example: Credit Suisse CoCos were completely written down to zero during its 2023 rescue by UBS, while equity holders received some value - a controversial inversion of typical capital hierarchy.\n\n\nActivity C: Basel III Capital Calculation\nScenario: You are a bank analyst evaluating whether a regional bank meets Basel III requirements.\nBank’s capital structure ($ million):\n\n\n\nCapital Component\nAmount\n\n\n\n\nCommon stock\n500\n\n\nRetained earnings\n300\n\n\nGoodwill\n(50)\n\n\nDeferred tax assets\n(30)\n\n\nNon-cumulative perpetual preferred stock\n100\n\n\nCoCo bonds (qualifying AT1)\n80\n\n\nSubordinated debt (7-year maturity)\n150\n\n\n\nBank’s exposure:\n\n\n\n\n\n\n\n\nExposure Type\nAmount\nRisk Weight\n\n\n\n\nGovernment bonds\n2,000\n0%\n\n\nResidential mortgages\n4,000\n35%\n\n\nCorporate loans\n5,000\n100%\n\n\nTotal balance sheet\n11,000\n\n\n\nOff-balance sheet commitments\n1,000\n50% (credit conversion factor)\n\n\n\nLeverage ratio exposure: $12,000M (includes derivatives and securities financing)\nRegulatory parameters:\n\nCountercyclical buffer: 1.0%\nBank is NOT a G-SIB\n\nQuestions:\n\nCalculate CET1, Total Tier 1, and Total Capital amounts\nCalculate risk-weighted assets (RWA)\nCalculate CET1 ratio, Tier 1 ratio, and Total Capital ratio\nCalculate leverage ratio\nCheck compliance with:\n\nMinimum CET1: 4.5%\nMinimum Tier 1: 6%\nMinimum Total Capital: 8%\nCapital Conservation Buffer: 2.5% (CET1)\nCountercyclical Buffer: 1.0% (CET1)\nLeverage Ratio: 3%\n\nWhat is the bank’s binding constraint - RWA-based ratios or leverage ratio?\nIf the bank wants to increase lending by $1,000M (100% risk weight), how much additional CET1 capital is needed?\nThe bank holds $2,000M in government bonds (0% risk weight). If it replaces these with corporate loans (100% risk weight), how do the ratios change?\n\nDiscussion points:\n\nWhy might a bank with high capital ratios still be risky? (Hint: What risks don’t RWA capture?)\nShould CoCo bondholders have been wiped out before equity in Credit Suisse? Why or why not?\nA bank CEO argues: “We should hold only high-risk-weight assets since the leverage ratio is binding anyway.” Is this rational? What are the implications?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bank Regulation, FRTB, and Economic Capital</span>"
    ]
  },
  {
    "objectID": "week_11.html#block-d-economic-capital-and-raroc",
    "href": "week_11.html#block-d-economic-capital-and-raroc",
    "title": "11  Bank Regulation, FRTB, and Economic Capital",
    "section": "Block D: Economic Capital and RAROC",
    "text": "Block D: Economic Capital and RAROC\n\nBeyond Regulatory Minimum: Economic Capital\nEconomic capital represents an internal estimate of capital needed to absorb unexpected losses over a one-year horizon at a confidence level aligned with the institution’s target credit rating. Unlike regulatory capital (compliance-driven), economic capital reflects the bank’s actual risk profile.\nKey distinction:\n\nRegulatory capital: “What regulators require us to hold” (Basel III rules)\nEconomic capital: “What we actually need to hold” (internal risk models)\n\nA well-managed bank should ensure economic capital ≥ regulatory capital. If economic capital &lt; regulatory capital, the firm is taking less risk than regulators assume. If economic capital &gt; regulatory capital, the firm may be undercapitalized despite regulatory compliance.\nConfidence level and credit rating:\n\n\n\nTarget Rating\nDefault Probability\nConfidence Level\n\n\n\n\nAAA\n0.01%\n99.99%\n\n\nAA\n0.02%\n99.98%\n\n\nA\n0.05%\n99.95%\n\n\nBBB\n0.20%\n99.80%\n\n\n\n\n\nComponents of Economic Capital\nEconomic capital must cover all risk types, integrating the concepts from previous weeks:\n1. Market Risk (Weeks 5-6): - Assessed using 1-year horizon VaR or ES at target confidence level - Derived from daily VaR scaled: \\(\\text{VaR}_{1\\text{year}} = \\text{VaR}_{1\\text{day}} \\times \\sqrt{250}\\)\n2. Credit Risk (Week 9): - Models: CreditMetrics, Credit Risk+, KMV - Captures default risk and credit migration risk - Can be cycle-specific (conditional) or cycle-neutral (unconditional)\n3. Operational Risk (Week 10): - Loss distribution approach - Captures frequency and severity of operational losses - Highly skewed distribution (many small losses, few catastrophic)\n4. Liquidity Risk (Week 8): - Cost of emergency funding during stress - Often embedded in other risk types or handled separately\n5. Business Risk: - Strategic risk, reputational risk, competitive pressures - Difficult to quantify but important for strategic initiatives\nLoss distribution characteristics:\n\n\n\nRisk Type\nStandard Deviation\nSkewness\nKurtosis\nImplication\n\n\n\n\nMarket Risk\nHigh\nZero (normal)\nLow\nVaR ≈ ES\n\n\nCredit Risk\nModerate\nModerate\nModerate\nSome tail risk\n\n\nOperational Risk\nLow\nHigh\nHigh\nFat tails matter\n\n\n\n\n\nAggregating Economic Capital\nSimply summing economic capital across risk types assumes perfect correlation (overly conservative):\n\\[E_{\\text{total}} = \\sum_{i=1}^{n} E_i\\]\nThis overestimates by ~40% in practice.\nCorrelation-based aggregation (hybrid approach):\n\\[\nE_{\\text{total}} = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{n} E_i E_j \\rho_{ij}}\n\\]\nwhere \\(\\rho_{ij}\\) is the correlation between risk types \\(i\\) and \\(j\\).\nTypical correlations:\n\n\n\n\nMarket\nCredit\nOperational\n\n\n\n\nMarket\n1.00\n0.30\n0.10\n\n\nCredit\n0.30\n1.00\n0.10\n\n\nOperational\n0.10\n0.10\n1.00\n\n\n\nAdvanced approach: Copulas\n\nMaps each risk type’s loss distribution to a standard distribution\nApplies correlation structure using copula function\nGaussian copula is simplest; t-copula captures tail dependence\n\n\n\nRAROC: Risk-Adjusted Return on Capital\nRAROC measures risk-adjusted profitability, enabling capital allocation and performance evaluation:\n\\[\n\\text{RAROC} = \\frac{\\text{Revenues} - \\text{Costs} - \\text{Expected Losses}}{\\text{Economic Capital}}\n\\]\nComponents:\n\nRevenues: Interest income, fees, trading profits\nCosts: Operating expenses, funding costs\nExpected Losses (EL): Average losses over time (e.g., credit loss provisions)\nEconomic Capital (EC): Capital required for unexpected losses\n\nInterpretation:\n\nRAROC &gt; Cost of Equity → Business creates value\nRAROC &lt; Cost of Equity → Business destroys value\nCompare RAROC across business units to allocate capital efficiently\n\nApplications:\n\nEx-ante (forward-looking): Should we expand this business? Enter this market?\nEx-post (backward-looking): How did each business unit perform? Bonus allocation?\nPricing: What interest rate should we charge on this loan to achieve target RAROC?\n\nExample: A trading desk generates $200M revenue, $80M costs, $10M expected losses, and requires $800M economic capital:\n\\[\n\\text{RAROC} = \\frac{200 - 80 - 10}{800} = \\frac{110}{800} = 13.75\\%\n\\]\nIf the bank’s cost of equity is 12%, this desk creates value.\n\n\nActivity D: Business Unit RAROC Analysis and Strategic Decisions\nScenario: You are the CFO of a diversified financial institution. The firm has $3 billion in economic capital to allocate across business units. Your cost of equity is 12%, and target is AA rating (99.98% confidence).\nBusiness Unit Performance (all figures $ millions):\n\n\n\n\n\n\n\n\n\n\nBusiness Unit\nRevenues\nOperating Costs\nExpected Losses\nEconomic Capital (Current)\n\n\n\n\nCorporate Lending\n240\n80\n40\n1,000\n\n\nTrading Desk\n320\n140\n20\n1,200\n\n\nRetail Banking\n180\n90\n30\n600\n\n\nAsset Management\n80\n30\n5\n200\n\n\nTotal\n820\n340\n95\n3,000\n\n\n\nPart 1: Current Performance Analysis\n\nCalculate RAROC for each business unit\nRank business units by RAROC\nWhich units create value (RAROC &gt; 12%)? Which destroy value?\nCalculate total firm RAROC\n\nPart 2: Strategic Decisions\nYou are considering four strategic initiatives. Analyze each using RAROC:\nOption A: Expand Corporate Lending\n\nIncrease loans by 20% → Increase EC by $200M\nExpected incremental revenues: +$50M\nExpected incremental costs: +$18M\nExpected incremental losses: +$8M\n\nOption B: Grow Asset Management\n\nLaunch new funds → Increase EC by $100M (operational risk)\nExpected incremental revenues: +$25M\nExpected incremental costs: +$10M\nExpected incremental losses: +$2M\n\nOption C: Reduce Trading Desk Size\n\nCut illiquid positions → Reduce EC by $300M\nExpected revenue reduction: -$60M\nExpected cost reduction: -$25M\nExpected loss reduction: -$5M\n\nOption D: Exit Retail Banking (Divest)\n\nSell retail division → Release $600M EC for other uses\nOne-time gain on sale: $50M\n\nQuestions:\n\nCalculate RAROC for each initiative (Options A-C)\nWhich initiative has the highest RAROC?\nIf you could implement only one initiative, which would you choose and why?\nFor Option D: If you reinvest the $600M freed capital into Corporate Lending (at current RAROC), what is the net impact on firm profits?\n\nPart 3: Risk Aggregation\nAssume the business units have the following correlations:\n\n\n\n\nCorp Lending\nTrading\nRetail\nAsset Mgmt\n\n\n\n\nCorp Lending\n1.00\n0.40\n0.50\n0.20\n\n\nTrading\n0.40\n1.00\n0.30\n0.15\n\n\nRetail\n0.50\n0.30\n1.00\n0.25\n\n\nAsset Mgmt\n0.20\n0.15\n0.25\n1.00\n\n\n\n\nUsing the correlation-based aggregation formula, calculate the diversified total economic capital:\n\n\\[\nEC_{\\text{total}} = \\sqrt{\\sum_{i=1}^{4} \\sum_{j=1}^{4} EC_i \\cdot EC_j \\cdot \\rho_{ij}}\n\\]\n\nCompare to the simple sum ($3,000M). What is the diversification benefit?\nIf the firm is subject to Basel III CET1 requirement of 7% on $40B RWA = $2,800M, which is binding - regulatory capital or economic capital?\n\nPart 4: Integration with Previous Weeks\n\nThe trading desk’s EC comes from:\n\nMarket risk (VaR): $800M\nCredit risk (CVA): $200M\nOperational risk: $200M\n\nIf FRTB increases market risk capital by 50% due to liquidity horizons, what happens to trading desk RAROC?\nRetail banking faces a potential operational risk loss (data breach) with:\n\nExpected loss: $30M (already included above)\n99.98% VaR: $150M\n\nIs the current EC allocation ($600M) sufficient? If not, how should it change?\nCorporate lending portfolio:\n\nAverage loan: $10M\nNumber of loans: 500\nAverage PD: 2%\nLGD: 40%\nCorrelation between borrowers: 0.25\n\nUsing the Vasicek formula from Week 9, estimate economic capital for credit risk. Does it match the $1,000M allocation?\n\nDiscussion Points:\n\nA business unit has the highest absolute profit but lowest RAROC. Should bonuses be based on profit or RAROC? Why?\nYour trading desk argues: “FRTB penalizes illiquid products too much. Competitors will take this business.” How do you respond?\nEconomic capital shows the bank needs $3.5B, but Basel III requires only $2.8B. Should you hold the higher amount? What if shareholders demand capital return?\nHow would you explain RAROC to a business unit head who believes their high revenues justify any risk-taking?\nCompare regulatory capital (rules-based, public) vs. economic capital (model-based, private). Which matters more for: (a) preventing bank failures, (b) creating shareholder value?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bank Regulation, FRTB, and Economic Capital</span>"
    ]
  },
  {
    "objectID": "week_11.html#key-takeaways",
    "href": "week_11.html#key-takeaways",
    "title": "11  Bank Regulation, FRTB, and Economic Capital",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nRegulatory Evolution\n\nBasel I (1988) introduced risk-weighted assets but was overly simplistic and enabled regulatory arbitrage\nBasel II (2007) improved risk sensitivity but was implemented just as the crisis revealed its weaknesses\nBasel II.5 (2011) added stressed VaR and incremental risk charge for trading books\nBasel III (2023) increased capital quality (CET1) and quantity (buffers), added leverage ratio and liquidity requirements\nFRTB (ongoing) replaces VaR with Expected Shortfall and introduces liquidity horizons\n\n\n\nFRTB Key Innovations\n\nExpected Shortfall at 97.5% captures tail risk better than VaR at 99%\nLiquidity horizons (10 to 120 days) reflect true time to exit positions in stress\nStricter trading vs. banking book boundaries prevent regulatory arbitrage\nCapital requirements for illiquid products increased significantly\n\n\n\nBasel III Capital Framework\n\nCET1 (4.5%) + Capital Conservation Buffer (2.5%) + Countercyclical Buffer (0-2.5%) = 7-9.5% total CET1 requirement\nLeverage ratio (3%) provides non-risk-weighted backstop\nG-SIBs face additional 1-3.5% CET1 surcharge (FSB publishes annual list)\nCoCo bonds automatically recapitalize banks when CET1 falls below trigger\n\n\n\nEconomic Capital and RAROC\n\nEconomic capital is internal estimate based on target rating; regulatory capital is compliance minimum\nAggregate risks using correlations to capture diversification benefits\nRAROC measures risk-adjusted performance: \\(\\text{RAROC} = \\frac{\\text{Rev} - \\text{Cost} - \\text{EL}}{\\text{EC}}\\)\nUse RAROC for capital allocation, pricing, performance evaluation, and strategic decisions\nIntegrate market, credit, operational, and liquidity risk into unified framework\n\n\n\nCritical Connections\n\nFRTB liquidity horizons reflect lessons from Week 8 (liquidity risk freezing in 2008)\nEconomic capital aggregates Week 5-6 (market), Week 8 (liquidity), Week 9 (credit), and Week 10 (operational) risks\nBinding constraint (RWA vs. leverage ratio) determines actual lending capacity\nEconomic capital &gt; Regulatory capital → Bank may be undercapitalized despite compliance\n\n\n\nPractical Implications\n\nBanks must satisfy both RWA-based ratios AND leverage ratio (dual constraint)\nFRTB makes illiquid trading strategies much more expensive (liquidity horizons)\nRAROC enables comparing business units with different risk profiles\nG-SIB designation adds 1-3.5% capital requirement but also provides funding advantages (“too big to fail” perception)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bank Regulation, FRTB, and Economic Capital</span>"
    ]
  }
]