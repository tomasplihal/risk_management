[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Risk Management and Financial Institutions",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Risk Management and Financial Institutions",
    "section": "",
    "text": "Course Overview\nThis course provides a comprehensive introduction to risk management principles and their practical applications in banking and insurance sectors. Students will gain deep insights into how financial institutions identify, assess, and mitigate various types of risk while exploring the critical role risk management plays in ensuring organizational stability and success.\nThrough a blend of theoretical frameworks and real-world case studies, participants will develop the analytical skills and strategic thinking necessary to navigate the complex risk landscape of modern financial services.\n\n\nLearning Outcomes\nUpon successful completion of this course, students will be able to:\n\nMaster fundamental risk management concepts, terminology, and theoretical frameworks\nUnderstand regulatory requirements and industry best practices in financial risk management\nApply quantitative and qualitative methods to measure and evaluate risk profiles\nCompare and contrast various risk identification methodologies\nEvaluate existing risk management systems and processes within financial institutions\nPropose innovative risk mitigation strategies using financial market instruments\nAssess risk transfer mechanisms including derivatives, securitization, and reinsurance\nIntegrate risk management considerations into broader business decision-making\n\n\n\nKey Topics Covered\nThe course explores risk management from both banking and insurance perspectives, examining how these industries approach common challenges while addressing their unique risk exposures. Students will engage with current industry developments, emerging risks, and evolving regulatory landscapes that shape modern risk management practices.\n\n\nReferences\nThe whole course is based on the following textbooks:\n\nHull, J. C. 2023. Risk Management and Financial Institutions. Wiley Finance.\nPirie, W. L., and M. P. Kritzman. 2017. Derivatives. CFA Institute Investment Series.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "week_01.html",
    "href": "week_01.html",
    "title": "1  Introduction to Risk Management",
    "section": "",
    "text": "1.1 Foundational Concepts",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_01.html#foundational-concepts",
    "href": "week_01.html#foundational-concepts",
    "title": "1  Introduction to Risk Management",
    "section": "",
    "text": "1.1.1 What is Risk?\nRisk is the exposure to uncertainty that can lead to variation in outcomes, particularly adverse outcomes that result in losses or failure to achieve objectives. Simply put: risk is the possibility that bad things might happen.\nThe key distinction to understand:\n\nUncertainty means we don’t know what will happen\nRisk means we can estimate the probabilities of different outcomes\n\nWhen we talk about risk in practice, we’re actually referring to three interconnected components:\n\nRisk Driver - The underlying source of uncertainty (what could change?)\nRisk Position - The amount exposed to that uncertainty (how much is at stake?)\nRisk Exposure - The potential impact from that position (what could we gain or lose?)\n\n\n\n\n\n\n\nExample\n\n\n\nConsider a company holding ¥1,000,000 when news from Japan could move the currency by ±1%:\n\nThe risk driver is the ±1% currency movement\nThe risk position is ¥1,000,000\nThe risk exposure is ±¥10,000 potential impact\n\n\n\n\n\n1.1.2 Risk Management: Optimization, Not Elimination\nRisk management is the process of defining acceptable risk levels, measuring current risk, and adjusting exposure to achieve organizational objectives. The critical insight: risk management aims to optimize value, not minimize risk.\nWhy? Because risk and return are inseparable. We cannot directly control returns—we can only manage risk to influence expected outcomes. This leads to the fundamental principle of finance: there is no free lunch. Higher returns require accepting higher risks.\n\n\n\n\n\n\nExample\n\n\n\nConsider these investment options:\n\nGovernment Bonds: ~2-3% annual return with minimal risk\nCorporate Bonds: ~5-7% return with moderate default risk\nStartup Equity: 20%+ potential returns with high probability of total loss\n\nThe goal isn’t to avoid the startup investment—it’s to understand the risk-return tradeoff and choose appropriately based on objectives and risk tolerance.\n\n\n\n\n1.1.3 The Risk Landscape: Financial and Non-Financial Risks\nOrganizations face two broad categories of risk that frequently interact and amplify each other.\n\nFinancial Risks\nMarket Risk stems from changes in market prices and rates. Market risk includes equity price movements, interest rate changes, foreign exchange fluctuations, and commodity price volatility.\nCredit Risk is the possibility that a counterparty won’t meet their obligations. The challenge with credit risk is its rarity—defaults are infrequent events with limited historical data, requiring estimation through credit spreads, rating models, and financial ratio analysis.\nLiquidity Risk manifests in two forms:\n\nFunding liquidity risk: inability to meet payment obligations\nMarket liquidity risk: inability to exit positions without significant losses\n\nThe 2008 crisis demonstrated this dramatically when mortgage-backed securities became essentially unsellable at any reasonable price. The risk isn’t just wider bid-ask spreads (a known cost) but the uncertainty of spreads widening unpredictably, especially during stress periods.\n\n\nNon-Financial Risks\nOperational Risk arises from failed processes, people, systems, or external events. These events are typically low-frequency but high-impact, making them difficult to predict and quantify.\nStrategic Risk emerges from poor business decisions or flawed execution.\nReputation Risk can destroy value rapidly, especially in our social media age. It often results from other risk events and spreads faster than organizations can respond.\nModel Risk occurs when we use wrong models or misapply correct ones. All models simplify reality—the danger comes when we forget these limitations. Models fail in predictable ways.\nTail Risk represents extreme “black swan” events that occur more frequently than normal distributions suggest. These outlier events in the probability distribution’s tails can devastate unprepared organizations.\n\n\nThe Critical Distinction: Systematic vs. Idiosyncratic Risk\nSystematic risk affects entire markets or economies—interest rate changes, recessions, geopolitical events. It cannot be diversified away.\nIdiosyncratic risk is specific to individual companies or assets. While diversification can reduce it, the 2008 crisis showed how idiosyncratic risks can evolve into systematic ones when individual firm failures trigger system-wide contagion.\n\n\n\n1.1.4 Risk Interactions: When 1+1 = 3\nRisks rarely occur in isolation. They interact, amplify each other, and cascade through organizations in non-linear ways. The 2008 Financial Crisis provides a textbook cascade:\n\nHousing prices decline (market risk)\nMortgage defaults surge (credit risk)\n\nMBS securities become illiquid (liquidity risk)\nBanks face funding crises (funding liquidity risk)\nCounterparty concerns explode (credit risk amplification)\nSystem-wide crisis emerges (systemic risk)\n\nThis cascade illustrates wrong-way risk—when exposure increases precisely as counterparty credit quality deteriorates. Banks buying credit protection from AIG, which was itself exposed to the same mortgage risks, exemplified this fatal correlation.\n\n\n\n\n\n\nImportant\n\n\n\nThe lesson: risk models often fail to capture these interactions, making scenario planning and stress testing essential for identifying potential risk cascades before they occur.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_01.html#the-risk-management-process",
    "href": "week_01.html#the-risk-management-process",
    "title": "1  Introduction to Risk Management",
    "section": "1.2 The Risk Management Process",
    "text": "1.2 The Risk Management Process\n\n1.2.1 The Five-Step Risk Management Cycle\nRisk management isn’t a one-time exercise—it’s a continuous cycle that integrates risk considerations into every organizational decision. The process flows through five interconnected steps, each building on the previous while feeding back into a continuous loop of improvement.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis cycle operates within a broader framework that provides structure and support:\n\nRisk Governance establishes the top-down direction, ensuring risk management aligns with organizational objectives.\nRisk Infrastructure provides the people, systems, and analytical tools needed to track and quantify risks.\nPolicies and Processes translate high-level risk appetite into operational guidelines and limits.\nThe Three Lines of Defense create organizational accountability—business operations own and manage risks, risk management provides oversight, and internal audit offers independent assurance.\n\n\n\n\n\n\n\nImportant\n\n\n\nA critical insight: while organizations face countless potential risks, they’re typically driven by a small number of key risk factors. Effective risk management focuses on identifying and managing these critical drivers.\n\n\n\n\n1.2.2 Risk Identification—Finding What Could Hurt You\nThe first challenge is discovering what risks you face. This requires systematic exploration beyond the obvious, because the risks that destroy organizations are often the ones they didn’t see coming.\nEffective identification combines multiple approaches. Risk workshops bring together diverse perspectives to brainstorm potential threats. Historical loss analysis examines past failures—both internal and at peer organizations—to identify patterns. Scenario analysis asks “what could go wrong?” and follows the implications through to their logical conclusions. Risk registers provide systematic taxonomies of known risk types, while external benchmarking reveals risks other organizations have identified.\nThe challenge lies in navigating three categories of risk knowledge:\n\nKnown Knowns: Risks we understand and can quantify (market volatility, credit defaults)\nKnown Unknowns: Risks we’re aware of but can’t fully measure (cyber attack severity, pandemic impacts)\nUnknown Unknowns: “Black Swan” events we haven’t imagined\n\nThe goal isn’t to identify every conceivable risk—that’s impossible. It’s to cast a wide enough net to catch the risks that could materially impact objectives while remaining alert to signals of emerging unknown risks.\n\n\n1.2.3 Risk Assessment—Prioritizing What Matters\nNot all risks deserve equal attention. Assessment evaluates both the likelihood and potential impact of identified risks to focus resources where they matter most.\nThe classic risk matrix provides a simple but powerful prioritization tool:\n\n\n\nLikelihood\nLow Impact\nMedium Impact\nHigh Impact\n\n\n\n\nHigh\nMedium\nHigh\nCritical\n\n\nMedium\nLow\nMedium\nHigh\n\n\nLow\nLow\nLow\nMedium\n\n\n\nHigh-likelihood, high-impact risks demand immediate attention. Low-likelihood, low-impact risks might be accepted as a cost of doing business. The challenging decisions involve the corners: high-impact but unlikely events (prepare contingency plans) and likely but low-impact events (automate controls).\nAssessment isn’t purely mechanical. It requires judgment about probability and impact, consideration of risk velocity (how quickly a risk materializes), and understanding of risk interconnections that could amplify impacts.\n\n\n1.2.4 Risk Measurement—Quantifying the Unquantifiable\nMeasurement transforms vague concerns into actionable metrics. Different risk types require different measurement approaches, and choosing the right metric is crucial for effective management.\nFoundation metrics apply across risk types:\n\nProbability distributions show the range of potential outcomes\nStandard deviation measures dispersion, though it assumes normal distributions that rarely exist in practice\nValue-at-Risk (VaR) estimates maximum likely loss at a confidence level\nConditional VaR reveals average losses beyond the VaR threshold\nStress testing explores performance under extreme scenarios\n\nSpecialized metrics target specific risks:\n\nFor market risk, we use beta to measure systematic risk exposure, duration for interest rate sensitivity, and the “Greeks” for derivatives—delta (price sensitivity), gamma (delta’s rate of change), vega (volatility sensitivity), theta (time decay), and rho (interest rate sensitivity).\nFor credit risk, we estimate Probability of Default (PD), Loss Given Default (LGD), and Exposure at Default (EAD). Credit ratings synthesize multiple factors into standardized risk grades.\n\nKey Risk Indicators (KRIs) serve as early warning signals—metrics that change before risks materialize, providing time to respond.\nThe challenge in measurement is balancing precision with practicality. Perfect measurement is impossible, but approximate measurement that acknowledges uncertainty is far better than no measurement at all.\n\n\n1.2.5 Risk Management—Choosing Your Response\nOnce risks are identified, assessed, and measured, organizations must decide how to respond. The four fundamental strategies can be combined for optimal results:\nRisk Avoidance eliminates risk by not engaging in the activity. A company might avoid entering politically unstable markets or decline to develop products with uncertain liability. This provides certainty but foregoes potential opportunities. The key question: is what we’re giving up worth the risk we’re avoiding?\nRisk Acceptance consciously retains risk, either because it’s unavoidable or because the cost of mitigation exceeds potential losses. This takes two forms. Self-insurance means bearing losses as they occur or setting aside reserves. Diversification spreads risk across uncorrelated exposures, reducing concentration while accepting that some risk remains.\nRisk Transfer shifts risk to parties better equipped to bear it. Insurance is the classic mechanism—insurers pool uncorrelated risks from many sources, using the law of large numbers to make aggregate losses predictable. The limitation: insurance becomes expensive or unavailable for risks that affect many parties simultaneously (pandemic, financial crisis).\nRisk Shifting uses derivatives to alter risk distribution without transferring ownership. A company might use currency forwards to lock in exchange rates or buy options to limit downside while preserving upside. The choice between forwards (obligation) and options (right but not obligation) depends on whether you want certainty or flexibility.\nThe decision process starts with a fundamental question: is the risk within our appetite? If yes, we optimize management costs. If no, we must reduce exposure through some combination of the four strategies, considering costs, benefits, risk tolerance, and operational capabilities.\n\n\n1.2.6 Risk Monitoring—Maintaining Vigilance\nRisk management isn’t “set and forget.” Continuous monitoring ensures controls remain effective and strategies stay aligned with changing conditions.\nEffective monitoring tracks KRIs against predetermined limits, providing early warning when risks approach unacceptable levels. It regularly updates risk assessments as business conditions change, tests control effectiveness through audits and reviews, runs stress scenarios to verify resilience, and adjusts strategies based on lessons learned.\nRisk reporting transforms monitoring data into actionable intelligence. Effective reports share six characteristics:\n\nTimely—delivered frequently enough for decision-making, which varies by risk type (market risk might need daily reports, strategic risk quarterly)\nAccurate—based on validated data and calculations stakeholders can trust\nRelevant—focused on material risks for the specific audience, avoiding information overload\nClear—understandable by intended recipients, avoiding unnecessary jargon\nActionable—enabling informed decisions with clear implications and recommendations\nComprehensive—covering all material risk areas without gaps that could hide emerging problems\n\nThe monitoring phase feeds back into identification, creating a continuous cycle of improvement. New risks emerge, existing risks evolve, and yesterday’s controls become today’s vulnerabilities. Only through constant vigilance can organizations maintain resilience in an uncertain world.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_01.html#governance-and-organization",
    "href": "week_01.html#governance-and-organization",
    "title": "1  Introduction to Risk Management",
    "section": "1.3 Governance and Organization",
    "text": "1.3 Governance and Organization\nRisk governance transforms risk management from compliance burden into strategic advantage through clear organizational structure and accountability. The hierarchy ensures risk awareness flows from boardroom to operations while information travels upward to inform decisions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRisk appetite defines what risks an organization willingly accepts pursuing objectives, while risk tolerance sets acceptable variation boundaries. Organizations operationalize these through cascading limits:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmart organizations set appetite well below capacity, maintaining buffers for unexpected shocks.\nRisk budgeting transforms abstract risk appetite into concrete allocations across the organization. Like financial budgeting allocates scarce capital, risk budgeting allocates scarce risk capacity to its most productive uses.\nRisk culture—shared attitudes and behaviors around risk—determines whether governance structures actually influence behavior. Strong culture exhibits four pillars: leadership tone from the top, clear accountability with consequences, effective challenge without retaliation, and risk-aligned incentives favoring long-term value over short-term profits. Poor culture reveals itself through profit-at-any-cost focus, suppressed concerns, and risk management viewed as impediment rather than enabler.\nModern risk governance creates value by optimizing risk-return trade-offs, enabling profitable activities others fear, and building competitive advantage through superior risk management. Success requires integration with business strategy, dynamic adaptation to changing conditions, and investment in capabilities matching strategic importance. The goal: risk management so embedded it becomes invisible yet omnipresent, influencing every decision without constraining innovation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_01.html#modern-applications-and-enterprise-risk-management",
    "href": "week_01.html#modern-applications-and-enterprise-risk-management",
    "title": "1  Introduction to Risk Management",
    "section": "1.4 Modern Applications and Enterprise Risk Management",
    "text": "1.4 Modern Applications and Enterprise Risk Management\nEnterprise Risk Management (ERM) represents a paradigm shift from isolated risk silos to holistic risk management. Traditional approaches created blind spots at intersections where risks interact and amplify—credit crises become liquidity crises become market crises. ERM acknowledges that organizations are driven by a small number of key risk factors cutting across traditional boundaries.\nERM rests on five interconnected pillars:\n\nTargets align risk-taking with strategic objectives through enterprise-wide appetite statements and risk-adjusted metrics.\nStructure creates accountability through board oversight and cross-functional committees.\nIdentification and Metrics establish common risk language and aggregate measures.\nStrategies optimize risk-return trade-offs at portfolio level.\nCulture embeds risk awareness as organizational mindset rather than isolated function.\n\nModern risk management looks forward through stress testing that explores extreme conditions. Sensitivity analysis changes single factors, scenario analysis combines multiple changes, and reverse stress testing identifies what could cause failure. Testing uses both historical scenarios (2008 crisis, COVID-19) and hypothetical events (cyber attacks, climate catastrophes) to reveal vulnerabilities that normal metrics miss.\nNew risk categories demand attention as the world evolves. Climate risk operates through physical damage, transition costs, and stranded assets. Cyber risk grows more sophisticated with cascading systemic impacts. Artificial intelligence creates accountability gaps and amplifies model risk. Regulatory evolution accelerates with dynamic requirements across multiple frameworks (Basel III, Solvency II, COSO).\n\n\n\n\n\n\nImportant\n\n\n\nThe fundamental insight: ERM transforms risk from threat to opportunity, from cost to investment, from necessary evil to competitive advantage. Organizations that master ERM don’t eliminate uncertainty—they harness it for sustainable value creation through intelligent risk-taking.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_01.html#practice-questions-and-problems",
    "href": "week_01.html#practice-questions-and-problems",
    "title": "1  Introduction to Risk Management",
    "section": "1.5 Practice Questions and Problems",
    "text": "1.5 Practice Questions and Problems\n\nExercise 1: Risk Components\nIdentify the risk components:\n\nRisk Driver: What uncertain factor is creating the risk?\nRisk Position: What is your stake/exposure in this situation?\nRisk Exposure: What is the dollar amount potentially at stake?\n\n\n\n\n\n\n\nScenario A: Currency Risk\n\n\n\nYou work for a German company that just signed a contract to receive $1,000,000 from a US client in 3 months. Today’s exchange rate is €1 = $1.10, but it could move to anywhere between $1.00 and $1.20.\n\n\n\n\n\n\n\n\nScenario B: Interest Rate Risk\n\n\n\nYour company has a €5,000,000 loan with a variable interest rate, currently at 4%. The rate is reviewed every 6 months and could move ±2%.\n\n\nBased on these two examples, how do the three risk components (driver, position, exposure) work together to create overall risk?\n\n\nExercise 2: Risk Identification\nReview the real events from recent years provided below. For each event, complete the following:\n\nIdentify all types of risks present in each event. Consider multiple perspectives - what could potentially go wrong for various stakeholders involved?\nClassify the identified risks into these four categories:\n\nFinancial/Market Risks\nOperational/Process Risks\nHuman/Behavioral Risks\nTechnology/System Risks\n\n\nThis exercise helps develop pattern recognition skills for identifying and categorizing different risk types across real-world scenarios.\n\n\n\n\n\n\nEvent 1: Silicon Valley Bank Collapse (March 2023)\n\n\n\nSVB focused heavily on tech startups and held many long-term government bonds. When interest rates rose, bond values fell. The bank announced losses, customers panicked, and $42 billion was withdrawn in one day via mobile banking.\n\n\n\n\n\n\n\n\nEvent 2: Ever Given Ship (March 2021)\n\n\n\nA container ship got stuck in the Suez Canal for 6 days, blocking 12% of global trade. Oil prices rose, supply chains were disrupted, and some companies couldn’t deliver products.\n\n\n\n\n\n\n\n\nEvent 3: GameStop Stock Surge (January 2021)\n\n\n\nReddit users coordinated to buy GameStop stock, driving the price from $20 to $347. Hedge funds that bet against the stock lost billions. Some trading apps restricted purchases.\n\n\n\n\nExercise 3: Risk Dominos\nPick ONE of the scenarios below and trace how problems might spread. Think about how one problem leads to the next:\n\nWeek 1: Original event happens\nWeek 2: What happens next?\nWeek 3: What new problems emerge?\nWeek 4: How does it spread further?\nWeek 5: What’s the final impact?\n\nCritical Thinking Questions:\n\nWhich step was hardest to predict?\nAt what point could the company have stopped the cascade?\nWhat does this tell you about managing risk?\n\n\n\n\n\n\n\nScenario A: Cyber Attack\n\n\n\nA major bank’s computer systems are hacked, and customer data is stolen.\n\n\n\n\n\n\n\n\nScenario B: CEO Scandal\n\n\n\nThe CEO of a major company is arrested for fraud.\n\n\n\n\n\n\n\n\nScenario C: Factory Fire\n\n\n\nYour company’s main manufacturing plant burns down.\n\n\n\n\nExercise 4: The Coffee Shop Dilemma\nYou’re opening a coffee shop near campus. You’ve identified these potential problems:\n\nCompetitor opens next door: High chance, medium impact\nCoffee prices double: Low chance, high impact\nStudents switch to energy drinks: Medium chance, medium impact\nYour espresso machine breaks: High chance, low impact\nCampus shuts down (like COVID): Very low chance, very high impact\n\nSuggest suitable solutions:\n\nRank these risks from 1-5 (1 = deal with first, 5 = worry about last)\nFor your top 3 risks, what would you do and why? For each one, you can:\n\nAvoid it (don’t do things that create the risk)\nAccept it (live with it, maybe save money just in case)\nTransfer it (get someone else to bear the risk - like insurance)\nReduce it (take action to make it less likely or less damaging)\n\n\n\n\nExercise 5: The Risk Management System\nImagine you’re hired as “Chief Risk Officer” for a tech startup. The CEO says: “Just make sure nothing bad happens to us.”\n\nWhat would you tell the CEO about their request? What’s realistic and what isn’t?\n\nThe CEO is more reasonable now. They want you to “manage risks systematically.” Design your approach:\n\nWhat’s the very first thing you’d do?\nWhat comes next?\nHow would you measure things?\nWhat actions would you take?\nHow would you keep watching?\n\nWho should be responsible for risk management in the company?\n\nJust the risk manager\nSenior management\nAll employees\nBoard of directors\nExternal consultants\n\n\n\nExercise 6: Putting It All Together\nThink about everything you’ve worked through today. In your own words, define:\n\nRisk\nRisk Management\n\nThink of a major financial decision you’ve made (or will make soon) - buying a car, choosing a university, taking a job, etc.\n\nWhat risks did/do you face?\nHow did/will you manage them?\n\nYou’re in a job interview for a finance position. The interviewer asks: “Why should companies care about risk management?” Explain your opinion in 60s.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Risk Management</span>"
    ]
  },
  {
    "objectID": "week_02.html",
    "href": "week_02.html",
    "title": "2  Revision of Basic Concepts",
    "section": "",
    "text": "2.1 Introduction\nThis session reviews core concepts from your previous coursework, now examined specifically through a risk management lens. These aren’t just academic theories—they’re the foundation of how practitioners assess portfolio exposures, set capital requirements, and evaluate systemic vulnerabilities. Understanding these concepts deeply is crucial for grasping why certain risks materialize and how traditional models can fail under stress.\nDon’t just find answers—understand why these concepts matter and how they break down. Be able to explain your findings to the class, including points of disagreement or uncertainty. Recent events (from the 2008 crisis to SVB’s 2023 collapse) show that even well-understood principles can fail spectacularly.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#introduction",
    "href": "week_02.html#introduction",
    "title": "2  Revision of Basic Concepts",
    "section": "",
    "text": "Use available resources: Textbook, your previous notes, online sources\nAI tools can help with general concepts, but verify specific facts and formulas\nFocus on connections between concepts and their practical limitations\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember: In risk management, questioning assumptions is essential. The most dangerous phrase remains: “We’ve always done it this way!”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#risk-return-trade-offs-and-portfolio-theory",
    "href": "week_02.html#risk-return-trade-offs-and-portfolio-theory",
    "title": "2  Revision of Basic Concepts",
    "section": "2.2 Risk-Return Trade-offs and Portfolio Theory",
    "text": "2.2 Risk-Return Trade-offs and Portfolio Theory\n\nHow does the correlation between assets affect portfolio risk, and why is perfect negative correlation rare in practice?\nWhat changes when we introduce a risk-free asset to the efficient frontier? How does this lead to the concept of the market portfolio?\nIn practice, why might investors choose portfolios that are not on the efficient frontier?\nHow do leverage constraints affect the risk-return choices available to different types of investors?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#the-capital-asset-pricing-model",
    "href": "week_02.html#the-capital-asset-pricing-model",
    "title": "2  Revision of Basic Concepts",
    "section": "2.3 The Capital Asset Pricing Model",
    "text": "2.3 The Capital Asset Pricing Model\n\nWhat distinguishes systematic from nonsystematic risk? Why can only systematic risk command a risk premium?\nHow should we interpret negative beta assets, and what role might they play in portfolio construction?\nIf CAPM holds perfectly, what should be the weighted average alpha across all investors? Why?\nWhat are the main violations of CAPM assumptions you observe in real markets, and do they invalidate the model’s usefulness?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#banking-and-risk-management",
    "href": "week_02.html#banking-and-risk-management",
    "title": "2  Revision of Basic Concepts",
    "section": "2.4 Banking and Risk Management",
    "text": "2.4 Banking and Risk Management\n\nCompare the risks faced by commercial banks versus investment banks. How did the repeal of Glass-Steagall change these risk profiles?\nWhat is the “originate-to-distribute” model, and how did it contribute to the 2007-2008 crisis?\nHow do Basel III capital requirements differ from Basel II? What gaps were they designed to address?\nWhy might a bank choose to use internal models versus standardized approaches for calculating regulatory capital?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#insurance-and-pension-plans",
    "href": "week_02.html#insurance-and-pension-plans",
    "title": "2  Revision of Basic Concepts",
    "section": "2.5 Insurance and Pension Plans",
    "text": "2.5 Insurance and Pension Plans\n\nHow do life insurance companies manage the opposing risks of mortality and longevity across their product lines?\nWhat makes property-casualty insurance fundamentally different from life insurance in terms of risk management?\nHow do CAT bonds work, and why might investors be interested in them despite their risks?\nWhat is the definition of moral hazard and adverse selection?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#fund-management",
    "href": "week_02.html#fund-management",
    "title": "2  Revision of Basic Concepts",
    "section": "2.6 Fund Management",
    "text": "2.6 Fund Management\n\nWhat are the key differences in risk profile between open-end funds, closed-end funds, and ETFs?\nHow do hedge fund fee structures (e.g., “2 and 20”) create option-like payoffs for managers? What conflicts might this create?\nWhat is the evidence on persistence in fund manager performance? What does this imply for investors?\nCompare the risks of late trading, market timing, and front running. How do regulations attempt to prevent these?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#derivatives-markets-and-central-clearing",
    "href": "week_02.html#derivatives-markets-and-central-clearing",
    "title": "2  Revision of Basic Concepts",
    "section": "2.7 Derivatives Markets and Central Clearing",
    "text": "2.7 Derivatives Markets and Central Clearing\n\nHow does daily settlement in futures markets differ from end-of-period settlement in forward markets? What are the risk implications?\nWhat is the role of central counterparties (CCPs), and how do they change counterparty risk?\nHow has netting evolved from bilateral agreements to multilateral clearing? What are the benefits and limitations?\nPost-2008, what OTC derivatives must be centrally cleared, and why might some market participants prefer bilateral clearing?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_02.html#systemic-risk-and-the-2007-2008-crisis",
    "href": "week_02.html#systemic-risk-and-the-2007-2008-crisis",
    "title": "2  Revision of Basic Concepts",
    "section": "2.8 Systemic Risk and the 2007-2008 Crisis",
    "text": "2.8 Systemic Risk and the 2007-2008 Crisis\n\nWhat factors transformed the U.S. housing bubble into a global financial crisis?\nHow did securitization and CDOs amplify rather than distribute risk?\nWhat role did credit rating agencies play, and what conflicts of interest existed?\nCompare “too big to fail” banks with “too interconnected to fail” - which poses greater systemic risk?\nWhat key regulatory changes (Dodd-Frank, Volcker Rule, Basel III) emerged from the crisis, and what risks might they have missed?\n\n\n\n\n\n\n\nCase Study Exercise\n\n\n\nSelect one major financial institution failure from 2007-2008 (e.g., Lehman Brothers, Bear Stearns, AIG, Northern Rock). Analyze:\n\nWhat risks were inadequately managed?\nHow did their failure impact other institutions?\nWhat regulatory changes specifically addressed their type of failure?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision of Basic Concepts</span>"
    ]
  },
  {
    "objectID": "week_03.html",
    "href": "week_03.html",
    "title": "3  Understanding VaR and ES",
    "section": "",
    "text": "3.1 Traditional vs. Coherent Risk Measures",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding VaR and ES</span>"
    ]
  },
  {
    "objectID": "week_03.html#traditional-vs.-coherent-risk-measures",
    "href": "week_03.html#traditional-vs.-coherent-risk-measures",
    "title": "3  Understanding VaR and ES",
    "section": "",
    "text": "3.1.1 Why We Need Risk Measures in Modern Finance\nModern financial institutions manage portfolios of staggering complexity. A typical trading floor handles thousands of positions across global markets - equity derivatives in Tokyo, currency swaps in London, structured products in New York. When the chief risk officer asks “How much could we lose today?”, human intuition alone cannot provide the answer.\nCore Challenges:\n\nPortfolio complexity beyond human comprehension\nNeed to aggregate diverse risks into actionable numbers\nBalance between risk-taking and risk control\nRegulatory requirements for quantitative risk reporting\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Insight: Risk management isn’t about avoiding losses - it’s about understanding what risks you’re taking and ensuring they’re intentional and compensated.\n\n\n\n\n3.1.2 The Journey from Variance to VaR\nHarry Markowitz revolutionized finance in 1952 by making risk mathematically tractable. For the first time, risk wasn’t just a vague concept but something that could be calculated and optimized.\n\nMarkowitz’s Framework (Modern Portfolio Theory)\n\nRisk measure: Variance \\(\\sigma^2\\) (or standard deviation \\(\\sigma\\))\nKey formula: Portfolio variance accounts for correlations\n\n\\[\n\\sigma_p^2 = \\sum_i w_i^2\\sigma_i^2 + \\sum_i \\sum_{j \\neq i} w_iw_j\\rho_{ij}\\sigma_i\\sigma_j\n\\]\n\nMain insight: Diversification reduces risk\nOptimization: Maximize return/risk ratio (\\(\\mu_p/\\sigma_p\\))\n\n\n\nWhy Variance Failed as a Risk Measure\nSymmetry Problem:\n\nTreats upside and downside equally\n50% gain = same “risk” as 50% loss\nInvestors care more about losses than gains\n\nEmpirical Failures:\n\nFinancial returns exhibit “fat tails”\nExample: 1987 crash (-22% in one day)\n\nUnder normal distribution: Should occur once per billion years\nReality: Happened in our lifetime\n\nExtreme events far more common than theory predicts\n\nDerivatives Revolution (1980s):\n\nOptions have asymmetric payoffs\nPortfolio of options: Low variance but huge downside potential\nVariance cannot capture nonlinear risks\n\n\n\n\n3.1.3 The P/L Convention - Setting Our Standard\nOne of the most persistent sources of confusion in risk management is whether to work with P/L (Profit and Loss) or L/P (Loss and Profit) data. This seemingly simple choice affects every formula, every interpretation, and every risk system. Different textbooks, papers, and systems use different conventions, often without clearly stating which they’ve chosen.\n\n\n\n\n\n\n\n\nCharacteristic\nP/L (Returns as-is)\nL/P (Inverted)\n\n\n\n\nSign of Profits\nPositive\nNegative\n\n\nSign of Losses\nNegative\nPositive\n\n\nInterpretation\n+5% is good, -5% is bad\n+5% loss is bad, -5% loss (profit) is good\n\n\nVaR/ES Focus\nLeft tail (negative returns)\nRight tail (positive losses)\n\n\nUsed By\nMost trading systems, Bloomberg, many practitioners\nMany textbooks, Basel documents, some risk systems\n\n\n\n\n\n\n\n\n\nOur Course Standard\n\n\n\nFor all future lectures, we will use the P/L convention for these reasons:\n\nIntuitive: Positive returns are good, negative are bad - matches human thinking\nIndustry Standard: Most trading and portfolio systems use P/L\nDirect Interpretation: No mental sign-flipping when reading results\nConsistency: P/L statements, risk reports, and models all align\n\nRemember: VaR and ES are always reported as positive numbers representing potential losses, regardless of the convention used internally.\n\n\nThe choice of convention doesn’t affect the risk measurement - a 5% loss is a 5% loss. But being consistent and clear about which convention you’re using is essential for correct implementation and interpretation. When reading any risk management material, always check: Are they using returns (P/L) or losses (L/P)?\n\n\n3.1.4 Value-at-Risk: The Revolutionary Simple Answer\nThe breakthrough came from J.P. Morgan in the early 1990s. CEO Dennis Weatherstone’s daily request for a single risk number at 4:15 PM spawned an industry standard that would transform risk management globally.\n\n\n\n\n\n\nVaR Definition\n\n\n\n\\[\\text{VaR}_\\alpha(L) = \\inf\\{l : F_L(l) \\geq \\alpha\\}\\]\nWhere:\n\n\\(\\text{VaR}_\\alpha(L)\\) = Value-at-Risk at confidence level \\(\\alpha\\) for loss variable \\(L\\)\n\\(F_L(l)\\) = Cumulative distribution function of losses, i.e., \\(P(L \\leq l)\\)\n\\(l\\) = A specific loss value\n\\(\\inf\\) = Infimum (smallest value) of the set\n\nInterpretation: “We are \\(\\alpha\\)% confident we won’t lose more than VaR over time horizon \\(h\\)”\n\n\nThree Essential Components:\n\nConfidence level (\\(\\alpha\\)): Typically 95% or 99%\nTime horizon (\\(h\\)): 1 day, 10 days, etc.\nLoss amount: In currency units (USD, EUR, etc.)\n\n\n\n\n\n\n\nExample\n\n\n\n\nPortfolio value: $100 million\nDaily volatility: \\(\\sigma = 2\\%\\)\nMean return: \\(\\mu = 0\\)\nAssume normal distribution\n\n\\[\\text{VaR}_{95\\%} = \\$100M \\times 0.02 \\times 1.645 = \\$3.29M\\] \\[\\text{VaR}_{99\\%} = \\$100M \\times 0.02 \\times 2.326 = \\$4.65M\\]\nThe \\(\\text{VaR}_{99\\%}\\) indicates that we are 99% confident that the portfolio will not lose more than $4.65 million tomorrow.\n\n\nWhy VaR Conquered the Financial World:\n\nIntuitive: Speaks the language of senior management (dollars)\nUniversal: Same framework for any portfolio\nRegulatory blessing: Basel II Market Risk Amendment (1996)\nFlexible: Historical simulation, parametric, Monte Carlo methods\n\n\n\n3.1.5 VaR’s Dangerous Blind Spot\nDespite its success, VaR harbors a fundamental flaw that would contribute to the 2008 financial crisis. It answers “what’s the minimum loss in bad scenarios?” but says nothing about the magnitude of losses beyond that point.\n\nThe Tail Risk Problem\n\n\n\nScenario\nPortfolio A\nPortfolio B\n\n\n\n\n99% VaR\n$10 million\n$10 million\n\n\nActual loss when VaR breached\n$11-12 million\n$100+ million\n\n\nVaR Assessment\n“Equal risk”\n“Equal risk”\n\n\nReality\nManageable\nCatastrophic\n\n\n\n\n\n\n\n\n\nReal Trading Floor Example\n\n\n\n\nStrategy: Sell deep out-of-the-money options\n99% of days: Collect premium (looks great!)\n1% of days: Massive losses (firm-threatening)\nVaR perspective: Low risk strategy\nReality: “Picking up nickels in front of a steamroller” (Joe Ritchie, CRT, 2002)\n\n\n\n\n\nMathematical Failure: Violation of Subadditivity\nVaR can suggest that diversification increases risk: \\[\\text{VaR}(A + B) &gt; \\text{VaR}(A) + \\text{VaR}(B)\\]\nConsequences:\n\nBanks could reduce regulatory capital by splitting into subsidiaries\nNo real risk reduction, just regulatory arbitrage\nIncentivizes behavior that increases systemic risk\n\nEvidence from Academic Literature:\n\nBasak and Shapiro (2001): VaR constraints may increase risk-taking\nTraders optimize to stay just within VaR limits\nIgnore severity of losses beyond VaR threshold\nRegulators care most about extreme losses - exactly what VaR ignores\n\n\n\n\n3.1.6 Key Takeaways\nThe evolution from variance to VaR solved real problems but created new ones:\n\nProgress: From abstract variance to intuitive dollar losses\nSuccess: Global adoption, regulatory standard\nFailure: Blind to tail risk, mathematically incoherent\nNext: Need coherent risk measures that capture extreme losses",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding VaR and ES</span>"
    ]
  },
  {
    "objectID": "week_03.html#the-axiomatic-foundation-of-coherent-risk-measures",
    "href": "week_03.html#the-axiomatic-foundation-of-coherent-risk-measures",
    "title": "3  Understanding VaR and ES",
    "section": "3.2 The Axiomatic Foundation of Coherent Risk Measures",
    "text": "3.2 The Axiomatic Foundation of Coherent Risk Measures\n\n3.2.1 What Makes a “Good” Risk Measure?\nIn 1999, Philippe Artzner (Artzner et al. 1999) and his colleagues revolutionized risk management by asking a fundamental question: What mathematical properties should any sensible risk measure possess? Their answer wasn’t based on empirical observation or market practice, but on pure logical reasoning about what risk management should achieve. They proposed four axioms that any “coherent” risk measure must satisfy - creating a mathematical definition of common sense.\n\n\n\n\n\n\nCoherent Risk Measure Axioms\n\n\n\nA risk measure \\(R\\) is called coherent if it satisfies four axioms:\n\nSubadditivity (Diversification should reduce risk) \\[R(L_1 + L_2) \\leq R(L_1) + R(L_2)\\]\nPositive Homogeneity (Scaling positions scales risk proportionally) \\[R(\\lambda L) = \\lambda R(L), \\text{ for every } \\lambda &gt; 0\\]\nMonotonicity (Bigger losses mean bigger risk) \\[R(L_1) \\leq R(L_2) \\text{ if } L_1 \\leq L_2\\]\nTranslation Invariance (Adding cash reduces risk by that amount) \\[R(L + a) = R(L) - a\\]\n\n\n\nWhy These Axioms Matter:\n\nThey formalize intuitive risk management principles\nProvide mathematical consistency across an organization\nEnable decentralized risk management\nPrevent regulatory arbitrage\n\n\n\n3.2.2 Understanding Each Axiom\nLet’s examine what each axiom means in practice, using concrete examples from a trading desk.\n\nSubadditivity - The Diversification Principle\nIntuition: Merging two portfolios shouldn’t increase risk\n\n\n\n\n\n\nExample: Why Subadditivity Matters\n\n\n\n\nTrading Desk A: Risk = $10M\nTrading Desk B: Risk = $8M\nCombined Desks: Risk should be \\(\\leq \\$18M\\)\n\nIf \\(\\text{Risk}(A+B) &gt; \\$18M\\), something is wrong with the risk measure!\n\n\nThree Critical Implications (McNeil, Frey, and Embrechts 2015):\n\nPortfolio optimization: Non-subadditive measures may lead to concentrated, risky portfolios\nRegulatory gaming: Banks could split into subsidiaries to reduce capital requirements\nDecentralized management: Risk manager can allocate limits \\(M_1\\) and \\(M_2\\) to desks, knowing total risk \\(\\leq M_1 + M_2\\)\n\n\n\nPositive Homogeneity - The Scaling Property\nIntuition: Double the position, double the risk\n\n\n\n\n\n\nExample: Linear Scaling\n\n\n\n\nPosition: 1000 shares of Apple Risk: $50,000\nPosition: 2000 shares of Apple Risk: Must be $100,000 (exactly \\(2\\times\\))\n\nNote: This assumes no liquidity effects or market impact\n\n\n\n\nMonotonicity - Worse is Riskier\nIntuition: If one portfolio always loses more than another, it’s riskier\n\n\n\n\n\n\nExample: Comparing Portfolios\n\n\n\n\nPortfolio X: Loses $5M in all bad scenarios\nPortfolio Y: Loses $10M in all bad scenarios\nTherefore: \\(R(Y) \\geq R(X)\\) must hold\n\n\n\n\n\nTranslation Invariance - Cash Reduces Risk\nIntuition: Adding $1M cash reduces risk by exactly $1M\n\n\n\n\n\n\nExample: Cash Buffer\n\n\n\n\nPortfolio risk: $10M\nAdd $3M cash to portfolio\nNew risk: Must be $7M (= $10M - $3M)\n\n\n\n\n\n\n3.2.3 VaR’s Fatal Flaw: Violation of Subadditivity\nVaR satisfies three of the four coherence axioms. Its critical and fatal flaw is the violation of subadditivity - the principle that diversification should reduce risk. This single failure means VaR is not coherent, leading to situations where combining portfolios appears to increase risk and enabling regulatory capital arbitrage.\n\n\n\n\n\n\nClassic Textbook Example\n\n\n\nConsider two corporate bonds:\n\nBond A: Default probability = 0.6%, Loss if default = $100M\nBond B: Default probability = 0.6%, Loss if default = $100M\nDefault correlation = 0 (independent)\n\n\\[\\text{VaR}_{99\\%}(A) = \\$0 \\text{ (no default in 99\\% of cases)}\\] \\[\\text{VaR}_{99\\%}(B) = \\$0 \\text{ (no default in 99\\% of cases)}\\]\nCombined portfolio:\n\\[P(\\text{at least one defaults}) = 0.6\\% + 0.6\\% - (0.6\\% \\times 0.6\\%) \\approx 1.2\\%\\] \\[\\text{VaR}_{99\\%}(A+B) = \\$100M \\text{ (one default occurs)}\\]\nViolation: \\[\\text{VaR}(A+B) &gt; \\text{VaR}(A) + \\text{VaR}(B)\\]\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Insight: The fatter the tails, the more likely VaR violates subadditivity\n\n\nThe mathematical failure of VaR coherence has profound practical implications that contributed to the 2008 financial crisis.\n\nRegulatory Capital Arbitrage: Bank splits into subsidiaries, lower capital requirements with no real risk reduction\nPerverse Portfolio Optimization: Traders sell deep OTM options: 99% VaR shows profit, but hidden 1% chance of HUGE loss\nSystemic Risk Amplification: Pre-2008 - Banks created CDOs that passed VaR tests but concentrated extreme tail risk, contributing to system-wide crisis when tails materialized\n\n\n\n3.2.4 The Search for Better Measures\nThe failure of VaR to satisfy coherence requirements led to an urgent search for better risk measures. The requirements were clear:\nDesired Properties:\n\nMathematically coherent (satisfy all four axioms)\nCapture tail risk severity\nComputationally tractable\nIntuitive interpretation\nSuitable for regulation\n\nThis search led to Expected Shortfall (ES) and the broader class of spectral risk measures.\n\n\n3.2.5 Key Takeaways\nThe axiomatic approach to risk measures revealed fundamental flaws in VaR:\nMathematical Foundation:\n\nFour axioms define coherent risk measures\nVaR violates subadditivity - the diversification principle\nViolation more severe with fat-tailed distributions (i.e., real markets)\n\nPractical Consequences:\n\nRegulatory arbitrage opportunities\nPerverse incentives for traders\nSystemic risk accumulation\nContributed to 2008 financial crisis\n\nThe Path Forward:\n\nNeed measures that are both coherent AND practical\nExpected Shortfall emerges as the solution\nRegulators moving from VaR to ES (Basel III/FRTB)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding VaR and ES</span>"
    ]
  },
  {
    "objectID": "week_03.html#expected-shortfall-as-the-solution",
    "href": "week_03.html#expected-shortfall-as-the-solution",
    "title": "3  Understanding VaR and ES",
    "section": "3.3 Expected Shortfall as the Solution",
    "text": "3.3 Expected Shortfall as the Solution\n\n3.3.1 ES Definition and Intuition\nExpected Shortfall emerged from a simple yet profound question: If VaR tells us the minimum loss in bad scenarios, what about the average loss when things go really wrong? ES answers this by looking beyond the VaR threshold and calculating the expected loss in the tail. This seemingly modest adjustment fixes VaR’s mathematical flaws while providing risk managers with crucial information about extreme scenarios.\n\n\n\n\n\n\nES Intuitive Definition\n\n\n\nExpected Shortfall (ES): The average of all losses that exceed the VaR threshold\n\n\nAlternative Names (all referring to the same concept):\n\nConditional Value-at-Risk (CVaR)\nTail Conditional Expectation (TCE)\nAverage Value-at-Risk (AVaR)\nExpected Tail Loss (ETL)\n\n\n\n\n\n\n\nExample: Simple Illustration\n\n\n\n\n100 daily loss scenarios, ranked from best to worst\nLosses: -2, -1, 0, 1, 2, …, 95, 96, 97, 120, 150 (in $millions)\n\n\\[\\text{VaR}_{95\\%} = \\$95M \\text{ (the 95th worst loss)}\\] \\[\\text{ES}_{95\\%} = \\frac{95+96+97+120+150}{5} = \\$111.6M\\]\nKey insight: ES captures the $120M and $150M extreme losses that VaR ignores!\n\n\n\n\n3.3.2 Mathematical Formulation\nThe mathematical elegance of ES lies in its dual representation: as an integral of tail VaRs and as a conditional expectation. Both formulations provide different insights into why ES succeeds where VaR fails.\n\n\n\n\n\n\nFormal Definition (Integral Form)\n\n\n\nFor a loss random variable \\(L\\) with distribution function \\(F_L\\) and confidence level \\(\\alpha \\in (0,1)\\):\n\\[ES_\\alpha = \\frac{1}{1-\\alpha} \\int_\\alpha^1 VaR_u(L) du\\]\nInterpretation: ES is the average of all VaR levels from \\(\\alpha\\) to 100%\n\n\n\n\n\n\n\n\nConditional Expectation Form (for continuous distributions)\n\n\n\n\\[ES_\\alpha = E[L | L \\geq VaR_\\alpha(L)]\\]\nInterpretation: ES is the expected loss, given that the loss exceeds VaR\n\n\nComputational Approach - “Tail VaR” Method:\n\nSlice the tail into \\(n\\) equal-probability segments\nCalculate VaR for each segment\nAverage these VaRs\n\n\n\n\n\n\n\nExample: Computing ES for Normal Distribution\n\n\n\n\nDistribution: Normal(\\(\\mu=0\\), \\(\\sigma=1\\))\nConfidence: 95%\n\nCalculation process:\n\nStep 1: 95% VaR = 1.645\nStep 2: Slice tail [95%, 100%] into segments\nStep 3: Calculate VaRs\n\n\n\n\nSegment\nConfidence\nVaR\n\n\n\n\n1\n95.5%\n1.695\n\n\n2\n96.0%\n1.751\n\n\n3\n96.5%\n1.812\n\n\n4\n97.0%\n1.881\n\n\n5\n97.5%\n1.960\n\n\n6\n98.0%\n2.054\n\n\n7\n98.5%\n2.170\n\n\n8\n99.0%\n2.326\n\n\n9\n99.5%\n2.576\n\n\n\n\n95% ES \\(\\approx\\) Average = 2.025\nExact value: 2.063\n\n\n\nKey Properties of ES:\n\nAlways \\(\\geq\\) corresponding VaR (captures tail severity)\nContinuous in \\(\\alpha\\) (no “cliff effects”)\nAccounts for the shape of the entire tail\nConverges smoothly as more data becomes available\n\n\n\n3.3.3 Why ES is Coherent: Proving the Axioms\nExpected Shortfall satisfies all four coherence axioms, making it mathematically consistent and economically sensible. Let’s verify each axiom.\n1. Subadditivity ✓\n\\[ES(L_1 + L_2) \\leq ES(L_1) + ES(L_2)\\]\nProof: The average of combined tail losses cannot exceed the sum of individual average tail losses due to diversification effects.\n2. Positive Homogeneity ✓\n\\[ES(\\lambda L) = \\lambda \\cdot ES(L) \\text{ for } \\lambda &gt; 0\\]\nProof: Scaling losses by \\(\\lambda\\) scales both the VaR threshold and all tail losses by \\(\\lambda\\)\n3. Monotonicity ✓\n\\[\\text{If } L_1 \\leq L_2 \\text{ everywhere, then } ES(L_1) \\leq ES(L_2)\\]\nProof: If one portfolio always loses more, its average tail loss must be higher\n4. Translation Invariance ✓\n\\[ES(L + c) = ES(L) + c\\]\nProof: Adding constant \\(c\\) to all losses shifts the entire distribution by \\(c\\)\n\n\n3.3.4 ES vs VaR: Comparative Analysis\nWhile ES solves VaR’s theoretical problems, it comes with its own practical challenges. Understanding the trade-offs is essential for implementation.\n\n\n\n\n\n\n\n\nAspect\nVaR\nES\n\n\n\n\nTail Risk\nIgnores severity beyond threshold\nCaptures full tail severity\n\n\nCoherence\nFails (not subadditive)\nSatisfies all axioms\n\n\nInterpretation\n“Minimum loss in worst α% cases”\n“Average loss in worst α% cases”\n\n\nBacktesting\nSimple (count violations)\nComplex (need loss magnitudes)\n\n\nData Requirements\nRobust to outliers\nSensitive to extreme observations\n\n\nComputational Cost\nFast\nSlower (need full tail)\n\n\nRegulatory Status\nBeing phased out\nFRTB standard (Basel III)\n\n\n\nWhen to Use Each:\n\nUse VaR: For liquid markets, short horizons, robust systems\nUse ES: For risk capital, stress testing, regulatory compliance\nUse both: VaR for limits, ES for capital (common practice)\n\n\n\n\n\n\n\nExample: Comparing VaR and ES\n\n\n\n\nPortfolio analysis over 1000 days:\nDistribution: Heavy-tailed (Student-t with 4 df)\n\n\\[\\text{VaR}_{95\\%} = \\$4.2M\\] \\[\\text{ES}_{95\\%} = \\$6.8M\\]\nInterpretation:\n\n95% VaR: “95% confident won’t lose more than $4.2M”\n95% ES: “In the worst 5% of cases, average loss is $6.8M”\nES/VaR ratio = 1.62 (indicates fat tails)\nFor normal distribution: ES/VaR ≈ 1.25\n\n\n\n\n\n3.3.5 Spectral Risk Measures: The General Framework\nExpected Shortfall belongs to a broader class called spectral risk measures (SRMs), which allow for flexible weighting of different parts of the loss distribution. This generalization provides a unified framework for risk measurement.\n\n\n\n\n\n\nDefinition of Spectral Risk Measures\n\n\n\n\\[SRM = \\int_0^1 \\phi(p) \\cdot VaR_p(L) dp\\]\nwhere \\(\\phi(p)\\) is a weighting function satisfying:\n\n\\(\\phi(p) \\geq 0\\) (non-negative weights)\n\\(\\int_0^1 \\phi(p) dp = 1\\) (weights sum to 1)\n\\(\\phi\\) is increasing (larger losses get more weight)\n\n\n\n\n\n\n\n\n\n\n\nRisk Measure\nWeight Function \\(\\phi(p)\\)\nInterpretation\n\n\n\n\nVaR at level \\(\\alpha\\)\n\\(\\delta(p-\\alpha)\\)\nAll weight at one point\n\n\nES at level \\(\\alpha\\)\n\\(\\frac{1}{1-\\alpha} \\cdot \\mathbf{1}_{p \\geq \\alpha}\\)\nEqual weight in tail\n\n\nExponential spectral\n\\(\\gamma e^{\\gamma p}/(e^\\gamma - 1)\\)\nExponentially increasing weight\n\n\n\nWhy Spectral Measures Matter:\n\nUnifying framework for all coherent risk measures\nCan tailor risk aversion profile to institution’s preferences\nSmooth transition between different risk levels\nAvoid arbitrary confidence level choice\n\n\n\n3.3.6 The Regulatory Shift: From VaR to ES\nThe 2008 financial crisis catalyzed a fundamental shift in regulatory thinking, culminating in the Fundamental Review of the Trading Book (FRTB).\nTimeline of Transition:\n\n1996: Basel II adopts VaR for market risk\n2008: Crisis reveals VaR’s tail blindness\n2012: Basel Committee proposes move to ES\n2016: FRTB finalized with ES as standard\n2022: Full implementation begins globally\n\nKey Changes in FRTB:\n\nReplace 99% 10-day VaR with 97.5% 10-day ES\nLiquidity horizons: 10, 20, 40, 60, 120 days\nStressed ES calibration\nDesk-level approval process\n\nWhy 97.5% ES \\(\\approx\\) 99% VaR:\n\nMaintains similar capital levels\nES at lower confidence more stable than VaR at high confidence\nBetter captures tail risk while remaining practical\n\n\n\n3.3.7 Key Takeaways\nExpected Shortfall represents the evolution of risk measurement from ad-hoc to axiomatic:\nTheoretical Victory:\n\nSatisfies all coherence axioms\nCaptures tail risk severity\nProvides consistent risk aggregation\n\nPractical Reality:\n\nMore complex to implement and backtest\nGreater data sensitivity\nComputational challenges remain\n\nThe Future:\n\nES becoming global regulatory standard\nSpectral measures offer further refinement\nIntegration with machine learning methods emerging\n\n\n\n\n\n\n\nImportant\n\n\n\nFinal Thought: ES isn’t perfect, but it’s mathematically sound and economically sensible - a vast improvement over VaR for capturing the risks that matter most.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding VaR and ES</span>"
    ]
  },
  {
    "objectID": "week_03.html#references",
    "href": "week_03.html#references",
    "title": "3  Understanding VaR and ES",
    "section": "3.4 References",
    "text": "3.4 References",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding VaR and ES</span>"
    ]
  },
  {
    "objectID": "week_03.html#practice-questions-and-problems",
    "href": "week_03.html#practice-questions-and-problems",
    "title": "3  Understanding VaR and ES",
    "section": "3.5 Practice Questions and Problems",
    "text": "3.5 Practice Questions and Problems",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding VaR and ES</span>"
    ]
  },
  {
    "objectID": "week_04.html",
    "href": "week_04.html",
    "title": "4  Calculating VaR and ES",
    "section": "",
    "text": "4.1 Parametric Approaches",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculating VaR and ES</span>"
    ]
  },
  {
    "objectID": "week_04.html#parametric-approaches",
    "href": "week_04.html#parametric-approaches",
    "title": "4  Calculating VaR and ES",
    "section": "",
    "text": "4.1.1 The Normal Distribution Assumption\nParametric approaches to risk measurement begin with a crucial assumption: we know (or can reasonably approximate) the probability distribution of our portfolio returns. The normal distribution serves as the natural starting point - not because markets are actually normal, but because it provides closed-form solutions and intuitive results. Like Newtonian physics in a relativistic world, the normal assumption works well enough in many situations to be useful, while being simple enough to implement in real-time.\nKey Advantages of Parametric Methods:\n\nClosed-form solutions (instant calculations)\nSmooth risk estimates (no jagged histograms)\nCan extrapolate beyond available data\nStandard statistical theory applies\n\n\n\n\n\n\n\nVaR Formula with Normal Distribution\n\n\n\n\nCore Assumption: Portfolio returns follow Normal distribution: \\(R \\sim N(\\mu, \\sigma^2)\\)\nOne-period VaR at confidence level \\(\\alpha\\) (e.g., 95%):\n\n\\[\\text{VaR}_\\alpha = -\\mu + \\sigma \\cdot z_\\alpha\\]\nWhere:\n\n\\(\\mu\\) = Expected return per period (often \\(\\approx\\) 0 for daily)\n\\(\\sigma\\) = Standard deviation of returns per period\n\\(z_\\alpha\\) = Positive quantile value (e.g., \\(z_{0.95} = 1.645\\), \\(z_{0.99} = 2.326\\))\n\n\n\n\n\n\n\n\n\nES Formula with Normal Distribution\n\n\n\n\nExpected Shortfall = Average loss when loss exceeds VaR\nOne-period ES at confidence level \\(\\alpha\\):\n\n\\[\\text{ES}_\\alpha = -\\mu + \\sigma \\cdot \\frac{\\phi(z_\\alpha)}{1-\\alpha}\\] \\[\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-(z^2/2)}\\]\nWhere:\n\n\\(\\phi(z)\\) = Standard normal PDF function\n\\(z_\\alpha\\) = Same quantile as in VaR (e.g., 1.645 for 95%)\n\\((1-\\alpha)\\) = Tail probability (e.g., 0.05 for 95% confidence)\n\nKey Property: For normal distribution, ES/VaR ratio is constant:\n\nAt 95% confidence: ES ≈ 1.25 × VaR\nAt 99% confidence: ES ≈ 1.14 × VaR\n\n\n\n\n\n\n\n\n\nExample: Basic Normal VaR and ES Calculation\n\n\n\n\nPortfolio: $100 million\nDaily returns: \\(\\mu = 0.05\\%\\), \\(\\sigma = 2\\%\\)\nCalculate 95% and 99% one-day VaR and ES\n\nVaR Calculations:\n\\[\\text{VaR}_{95\\%} = \\$100M \\times (-0.0005 + 0.02 \\times 1.645) = \\$3.24M\\]\n\\[\\text{VaR}_{99\\%} = \\$100M \\times (-0.0005 + 0.02 \\times 2.326) = \\$4.60M\\]\nInterpretation: “We are 95% confident we won’t lose more than $3.24M tomorrow”\nES Calculations:\n\\[\\text{ES}_{95\\%} = \\$100M \\times (-0.0005 + 0.02 \\times 2.062) = \\$4.07M\\]\n\\[\\text{ES}_{99\\%} = \\$100M \\times (-0.0005 + 0.02 \\times 2.67) = \\$5.29M\\]\nInterpretation: “In the 5% (1%) worst case scenarios, the average daily loss is $4.07M ($5.29) times the VaR”\n\n\n\n\n4.1.2 Time Scaling: The Square-Root Rule\nThe square-root scaling rule is ubiquitous in risk management, but its validity depends on strong assumptions that often break down precisely when we need them most. Under normal distribution, both VaR and ES are linear functions of volatility \\(\\sigma\\). Since volatility scales with \\(\\sqrt{h}\\), both risk measures inherit this scaling.\n\\[\\text{VaR}_{h\\text{-day}} = \\text{VaR}_{1\\text{-day}} \\times \\sqrt{h}\\]\n\\[\\text{ES}_{h\\text{-day}} = \\text{ES}_{1\\text{-day}} \\times \\sqrt{h}\\]\nRequired Assumptions:\n\nReturns are independent across time (no autocorrelation)\nReturns are identically distributed (no volatility clustering)\nNo mean reversion or momentum\nNormal distribution maintained across horizons\n\n\n\n\n\n\n\n\nWhen It Works\nWhen It Fails\n\n\n\n\nLiquid markets\nCrisis periods (correlations spike)\n\n\nShort horizons (&lt; 10 days)\nIlliquid assets (autocorrelation)\n\n\nNormal market conditions\nLong horizons (mean reversion kicks in)\n\n\nWell-diversified portfolios\nConcentrated positions (jump risk)\n\n\n\n\nCritical Warning for ES: ES scaling is even more fragile than VaR scaling.\n\nBasel/Regulatory Approach:\n\nFRTB uses different liquidity horizons (10-120 days) rather than naive scaling\nRecognizes that scaling assumptions fail for longer horizons\nES scaling particularly scrutinized for illiquid assets\n\nKey Observations:\n\nNormal vs Lognormal: Nearly identical for daily horizons\n\nLognormal prevents returns &lt; -100% (realistic constraint)\nDifferences only matter for longer horizons or high volatility\n\nFat Tails Impact: Student-t dramatically increases tail risk\n\n99% VaR doubles (Normal: $4.65M → Student-t(4): $9.20M)\nES/VaR ratio increases (1.25 → 1.62 → 2.01)\nLower ν = fatter tails = higher risk\n\nPractical Implications:\n\nNormal: Underestimates extreme risk by 2-3×\nLognormal: Minor adjustment, mainly theoretical\nStudent-t: Essential for crisis-aware risk management\n\n\n:::{.callout-important} Critical Insight: The ES/VaR ratio reveals tail behavior\n\nNormal: 1.25 (thin tails)\nStudent-t(3): 2.01 (fat tails)\n\nThis ratio is a quick “health check” for your distribution assumption! :::\nWhich to Use?\n\nNormal: Quick calculations, well-diversified portfolios\nLognormal: Equity portfolios, longer horizons\nStudent-t: Crisis risk, regulatory stress testing, concentrated portfolios\n\n\n\n4.1.3 Beyond Normality: Lognormal, Fat Tails, EVT\nReal financial returns exhibit features that the normal distribution cannot capture: they’re skewed, have fat tails, and show volatility clustering. Moving beyond normality is essential for accurate risk measurement.\nLognormal Distribution: Appropriate when returns can’t go below -100% (realistic for stocks)\n\\[\\text{VaR}_\\alpha^{\\text{log}} = 1 - e^{\\mu - \\sigma \\cdot z_\\alpha}\\]\nStudent-t Distribution (Fat Tails): Captures extreme events better than normal\n\\[\\text{VaR}_\\alpha^t = \\mu + \\sigma \\cdot \\sqrt{\\frac{\\nu-2}{\\nu}} \\cdot t_{\\nu,\\alpha}\\]\nWhere:\n\n\\(\\nu\\) = Degrees of freedom (lower = fatter tails)\n\\(t_{\\nu,\\alpha}\\) = Student-t quantile\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Insight: As tails get fatter, VaR increases dramatically\n\n\nWhen normal assumptions break spectacularly (think 2008), we need tools designed specifically for extreme events. Extreme Value Theory (EVT) provides a mathematical framework for the impossible becoming possible.\nThe EVT Philosophy:\n\nDon’t model the entire distribution\nFocus only on the tail behavior\nLet the extremes speak for themselves\nTwo Main Approaches: Block Maxima (GEV Distribution), Peaks Over Threshold (GPD)\n\nWhen to Use EVT:\n\nExtreme risk assessment (99.9%+ VaR)\nStress testing\nRegulatory capital (Basel III)\nInsurance/catastrophe modeling\n\n\n\n4.1.4 Estimation Uncertainty: Standard Errors and Confidence Intervals\nRisk measures are estimates, not certainties. Understanding their precision is as important as calculating the point estimate itself. For normal VaR estimated from historical data, we can calculate Standard Error of VaR Estimate and Confidence Interval:\n\\[SE(\\widehat{\\text{VaR}}_\\alpha) \\approx \\frac{\\sigma}{\\sqrt{2n}} \\sqrt{1 + \\frac{z_\\alpha^2}{2}}\\]\nWhere:\n\n\\(SE\\) = Standard error (uncertainty of our VaR estimate)\n\\(\\sigma\\) = Standard deviation of returns\n\\(n\\) = Number of observations in sample\n\\(z_\\alpha\\) = Normal quantile at confidence level \\(\\alpha\\) (e.g., 1.645 for 95%)\n\n\\[CI = \\widehat{\\text{VaR}}_\\alpha \\pm z_{CI} \\times SE(\\widehat{\\text{VaR}}_\\alpha)\\]\nWhere:\n\n\\(CI\\) = Confidence interval for the true VaR value\n\\(\\widehat{\\text{VaR}}_\\alpha\\) = Our estimated VaR value\n\\(z_{CI}\\) = Z-score for desired confidence interval (e.g., 1.645 for 90%, 1.96 for 95%)\n\\(SE(\\widehat{\\text{VaR}}_\\alpha)\\) = Standard error of the VaR estimate\n\n\n\n\n\n\n\nExample: Uncertainty in VaR Estimates\n\n\n\n\nData: 1000 daily returns\nSample \\(\\sigma = 2\\%\\)\n95% VaR estimate = $100M \\(\\times\\) 0.02 \\(\\times\\) 1.645 = $3.29M\n\nStandard error:\n\\[SE = \\frac{0.02}{\\sqrt{2000}} \\times \\sqrt{1 + \\frac{1.645^2}{2}} = 0.00061\\]\nSE in dollars = $100M \\(\\times\\) 0.00061 = $61,000\n90% CI for VaR:\n\\[CI = \\$3.29M \\pm 1.645 \\times \\$0.061M = [\\$3.19M, \\$3.39M]\\]\nImplication: Our “precise” $3.29M VaR has \\(\\pm 3\\%\\) uncertainty!\n\n\nFactors Affecting Precision:\n\n\n\n\n\n\n\nFactor\nEffect on Precision\n\n\n\n\nSample size\nDoubles \\(n\\) → Reduces SE by \\(\\sqrt{2}\\)\n\n\nConfidence level\n99% VaR → 50% larger SE than 95% VaR\n\n\nDistribution\nFat tails → 2-3\\(\\times\\) larger SE\n\n\nES vs VaR\nES standard error \\(\\approx\\) 2\\(\\times\\) VaR standard error\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nKey Message: Every risk measure is an estimate with uncertainty. Ignoring this uncertainty is itself a major source of risk.\n\n\n\n\n4.1.5 Key Takeaways\nParametric Methods Summary:\n\nNormal provides baseline: Quick, simple, often adequate\nTime scaling needs caution: Square-root rule fails in crisis\nFat tails matter: Student-t or EVT for realistic extremes\nUncertainty is real: Always estimate confidence intervals\n\nDecision Framework:\n\nDaily trading limits → Normal often sufficient\nRisk capital → Fat-tailed distributions essential\nExtreme scenarios → EVT methodology required\nAll cases → Quantify estimation uncertainty",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculating VaR and ES</span>"
    ]
  },
  {
    "objectID": "week_04.html#historical-simulation-methods",
    "href": "week_04.html#historical-simulation-methods",
    "title": "4  Calculating VaR and ES",
    "section": "4.2 Historical Simulation Methods",
    "text": "4.2 Historical Simulation Methods\n\n4.2.1 Basic Historical Simulation\nHistorical Simulation represents the philosophical opposite of parametric methods: instead of assuming we know the distribution, we let the data tell its own story. This approach, pioneered by major banks in the 1990s, remains the most widely used VaR methodology in practice. Its appeal lies in a simple premise - the best predictor of tomorrow’s risk is yesterday’s experience.\nCore Philosophy: “The future will be sufficiently like the recent past that historical data can forecast risk”\nThe Basic HS Algorithm:\n\nCollect historical returns (typically 250-500 days)\nApply today’s portfolio weights to historical returns\nGenerate hypothetical P&L distribution\nRead VaR/ES directly from empirical distribution\n\nMathematical Formulation:\nFor \\(n\\) historical observations ordered from worst to best:\n\\[\\text{VaR}_\\alpha^{HS} = -L_{[\\alpha \\cdot n]}\\]\nWhere:\n\n\\(L_{[k]}\\) = The k-th order statistic (k-th worst return in sorted data)\n\\([\\alpha \\cdot n]\\) = Index position, rounded (e.g., [0.05 × 1000] = 50th worst)\n\\(n\\) = Total number of historical observations\n\n\\[\\text{ES}_\\alpha^{HS} = -\\frac{1}{m} \\sum_{i=1}^{m} L_{[i]}\\]\nWhere:\n\n\\(m = [(1-\\alpha) \\times n]\\) = Number of observations in the tail\n\\(L_{[i]}\\) = The i-th worst return (sorted from worst to best)\n\n\n\n\n\n\n\nExample: Basic Historical Simulation\n\n\n\nConsider a 500-day time window of daily returns for the SPY ETF to calculate the 1-day VaR and ES at a 99% confidence level. The table below lists the 10 worst daily returns from this period:\n\n\n\nNo.\nDate\nLog Returns\n\n\n\n\n1.\n2024-08-05\n-0.029556\n\n\n2.\n2022-11-02\n-0.025416\n\n\n3.\n2022-12-15\n-0.024766\n\n\n4.\n2024-07-24\n-0.022923\n\n\n5.\n2022-11-09\n-0.020817\n\n\n6.\n2024-09-03\n-0.020794\n\n\n7.\n2023-02-21\n-0.020265\n\n\n8.\n2024-08-02\n-0.018794\n\n\n9.\n2023-03-09\n-0.018622\n\n\n10.\n2022-12-05\n-0.018153\n\n\n\n\n1% of 500 observations equals 5, meaning the 5th worst loss represents the 1-day 99% VaR.\nVaR: The daily 99% VaR is the 5th worst return, which corresponds to -0.020817.\nES: The daily 99% ES is the average of the 4 worst returns, -0.025665, providing an estimate of the average loss beyond the VaR threshold.\n\n\n\n\n\n\n\n\n\n\nCritical Advantages\nCritical Limitations\n\n\n\n\nNo distributional assumptions\nLimited by available history\n\n\nCaptures fat tails, skewness automatically\nCannot exceed worst historical loss\n\n\nIncludes all complex dependencies\nAssumes past = future\n\n\nSimple to explain to management\nPoor for new products\n\n\nHandles derivatives naturally\n“Fighting the last war” problem\n\n\n\n\n\n4.2.2 The Bootstrap Revolution\nThe bootstrap, introduced by Bradley Efron in 1979, transforms historical simulation from a simple empirical method into a sophisticated resampling framework. By treating our historical data as the “population” and resampling with replacement, we can generate thousands of alternative histories and better understand the uncertainty in our risk estimates.\nBootstrap Historical Simulation Algorithm:\n\nOriginal sample: \\(\\{r_1, r_2, ..., r_n\\}\\)\nResample: Draw \\(n\\) returns WITH replacement\nCalculate: VaR/ES for bootstrap sample\nRepeat: Generate B bootstrap samples (typically 1000+)\nAnalyze: Distribution of risk measures\n\nWhy Bootstrap Works:\n\nCreates new “possible histories”\nBreaks temporal dependencies\nProvides confidence intervals\nSmooths empirical distribution\n\n\n\n\n\n\n\n\n\nAdvanced Bootstrap Techniques\nDescription\nWhen to Use\n\n\n\n\nBlock Bootstrap\nResample blocks to preserve autocorrelation\nVolatility clustering present\n\n\nStationary Bootstrap\nRandom block lengths\nUnknown dependency structure\n\n\nFiltered Bootstrap\nBootstrap residuals from GARCH\nTime-varying volatility\n\n\n\n\n\n4.2.3 Advanced Historical Simulation Methods\nBasic historical simulation treats all observations equally, but markets evolve. Several enhancements address this limitation:\nAge-Weighted HS (Exponential Weighting)\nRecent observations get higher weights: \\[w_i = \\lambda^{i-1}(1-\\lambda)\\]\nWhere \\(\\lambda \\approx 0.94-0.99\\) determines decay speed. Yesterday matters more than last year.\nVolatility-Weighted HS (Hull-White)\nScale historical returns to current volatility: \\[r_i^{adjusted} = r_i \\times \\frac{\\sigma_{current}}{\\sigma_i}\\]\nThis prevents calm-period returns from understating risk in volatile times (and vice versa).\nFiltered Historical Simulation (FHS)\nThe most sophisticated approach:\n\nFit GARCH model to capture volatility dynamics\nExtract standardized residuals (innovations)\nBootstrap these residuals (not raw returns)\nReconstruct returns using current volatility regime\n\nKey Insight: FHS bootstraps the “shocks” while respecting today’s volatility level.\n\n\n\nMethod\nNormal Market\nVolatility Spike\nRegime Change\n\n\n\n\nBasic HS\nGood\nToo slow\nFails\n\n\nAge-weighted\nGood\nBetter\nSlow\n\n\nVol-weighted\nGood\nExcellent\nGood\n\n\nFHS\nExcellent\nExcellent\nExcellent\n\n\n\n\n\n4.2.4 Choosing the Right Approach\nImplementation Tip: Start simple (basic HS), add complexity only if backtesting shows need for improvement. Perfect is the enemy of good enough in risk management.\n\n\n\n\n\n\n\n\nMethod\nBest For\nKey Advantage\n\n\n\n\nBasic HS\nRegulatory reporting, stable markets\nSimple, no assumptions\n\n\nBootstrap HS\nLimited data, confidence intervals\nMore scenarios\n\n\nWeighted HS\nRegime shifts, recent stress\nResponsive to change\n\n\nFiltered (FHS)\nTrading desks, volatile markets\nCaptures current volatility\n\n\n\nBest Practice: Run multiple approaches in parallel\n\nBasic HS for regulatory reporting\nFHS for internal risk management\nBootstrap for uncertainty assessment\nCompare results for validation\n\nData Minimums:\n\n250 observations (1 year) = bare minimum\n500-1000 observations (2-4 years) = recommended\nMust include at least one stress period\n\n\n\n4.2.5 Key Takeaways\nHistorical Simulation Summary:\n\nBasic HS: Simple, robust, but backward-looking\nBootstrap: Adds sophistication, provides confidence intervals\nWeighting: Makes HS adaptive to market conditions\nFHS: Optimal blend of parametric and non-parametric\n\nThe Fundamental Trade-off:\n\nSimplicity vs. Sophistication\nStability vs. Responsiveness\nTransparency vs. Accuracy",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculating VaR and ES</span>"
    ]
  },
  {
    "objectID": "week_04.html#backtesting-and-model-validation",
    "href": "week_04.html#backtesting-and-model-validation",
    "title": "4  Calculating VaR and ES",
    "section": "4.3 Backtesting and Model Validation",
    "text": "4.3 Backtesting and Model Validation\n\n4.3.1 The Backtesting Framework\nBacktesting is where theory meets reality - the moment of truth for any risk model. As Alan Greenspan noted, “Disclosure of quantitative measures of market risk, such as value-at-risk, is enlightening only when accompanied by a thorough discussion of how the risk measures were calculated and how they related to actual performance.” Without rigorous backtesting, VaR is merely an expensive random number generator.\nCore Concept: Backtesting systematically compares predicted risk (VaR forecasts) with realized outcomes (actual P&L) to validate model accuracy.\nThe Fundamental Question: “Are we experiencing the number and magnitude of losses our model predicts?”\n\n\n\n\n\n\nBasic Framework\n\n\n\nFor each day \\(t\\):\n\nCalculate \\(\\text{VaR}_t\\) using data up to \\(t-1\\)\nObserve actual loss \\(L_t\\)\nRecord violation indicator:\n\n\\[\nI_t = \\begin{cases}\n1 & \\text{if } L_t &gt; \\text{VaR}_t \\text{ (violation)} \\\\\n0 & \\text{if } L_t \\leq \\text{VaR}_t \\text{ (no violation)}\n\\end{cases}\n\\]\nKey Metrics:\n\nViolation Rate: \\(\\hat{p} = \\frac{1}{T}\\sum_{t=1}^T I_t\\)\nExpected: \\(p = 1 - \\alpha\\) (e.g., 5% for 95% VaR)\nTest: Is \\(\\hat{p}\\) significantly different from \\(p\\)?\n\n\n\nTesting whether observed violations are statistically significant requires careful consideration of both Type I errors (rejecting a correct model) and Type II errors (accepting an incorrect model).\n\nKupiec’s Unconditional Coverage Test (1995)\nTests if violation frequency matches expected level:\n\\[LR_{uc} = -2\\ln\\left[\\frac{p^n(1-p)^{T-n}}{\\hat{p}^n(1-\\hat{p})^{T-n}}\\right] \\sim \\chi^2(1)\\]\nWhere:\n\n\\(n\\) = Number of violations observed\n\\(T\\) = Total observations\n\\(p\\) = Expected violation rate\n\\(\\hat{p} = n/T\\) = Observed violation rate\n\nDecision Rule: Reject if \\(LR_{uc} &gt; 3.841\\) (95% confidence)\n\n\n\n\n\n\nExample: Kupiec Test\n\n\n\n\nBank’s 99% VaR over 250 trading days\nViolations observed: 8\nExpected violations: 250 \\(\\times\\) 1% = 2.5\nExpected rate \\(p = 0.01\\)\nObserved rate \\(\\hat{p} = 8/250 = 0.032\\)\n\n\\[LR_{uc} = -2\\ln\\left[\\frac{(0.01)^8(0.99)^{242}}{(0.032)^8(0.968)^{242}}\\right] = -2 \\times (-7.53) = 15.06\\]\nDecision: Since \\(15.06 &gt; 3.841\\), reject the model!\n\n\n\n\nChristoffersen’s Conditional Coverage Test (1998)\nTests both frequency AND independence of violations:\n\\[LR_{cc} = LR_{uc} + LR_{ind} \\sim \\chi^2(2)\\]\nThe independence test checks for violation clustering:\n\\[LR_{ind} = -2\\ln\\left[\\frac{(1-p)^{n_{00}+n_{10}} \\cdot p^{n_{01}+n_{11}}}{(1-p_0)^{n_{00}} \\cdot p_0^{n_{01}} \\cdot (1-p_1)^{n_{10}} \\cdot p_1^{n_{11}}}\\right]\\]\nWhere:\n\n\\(p\\) = overall violation probability (should be 0.05 for 95% VaR)\n\\(p_0\\) = prob(violation today | no violation yesterday)\n\\(p_1\\) = prob(violation today | violation yesterday)\n\\(n_{ij}\\) = count of transitions from state i to j\n\nState 0 = no violation\nState 1 = violation\n\n\nWhere \\(n_{ij}\\) = transitions from state \\(i\\) to state \\(j\\)\nWhy Independence Matters:\n\nViolations should be randomly distributed\nClustering indicates model misses volatility dynamics\nCommon in models without time-varying volatility\n\n\n\n\n\n\n\nExample: Violation Clustering\n\n\n\n\nPattern A: Violations on days 50, 100, 150, 200\nPattern B: Violations on days 98, 99, 100, 101\n\nBoth have 4 violations, but Pattern B shows dangerous clustering!\n\n\n\n\n\n4.3.2 The Basel Traffic Light System\nThe Basel Committee created a framework that translates VaR backtesting results into regulatory capital requirements. Banks must calculate capital using:\n\\[\\text{Market Risk Capital} = k \\times \\text{VaR}\\]\nWhere \\(k\\) is a multiplier (minimum 3) that increases with backtesting violations.\nHow It Works:\nBanks backtest their 99% VaR over 250 trading days (1 year). With 99% confidence:\n\nExpected violations = 250 × 1% = 2.5\nActual violations determine the multiplier \\(k\\)\n\n\n\n\n\n\n\n\n\n\n\nZone\nViolations (out of 250)\nMultiplier \\(k\\)\nAction\nProbability if Model Correct\n\n\n\n\nGreen\n0-4\n3.0\nModel acceptable\n89.2%\n\n\nYellow\n5-9\n3.0-4.0\nInvestigate\n10.7%\n\n\nRed\n10+\n4.0\nModel inadequate\n0.1%\n\n\n\nBasel requires banks to explain yellow zone violations:\n\nModel Problems: Incorrect implementation, missing risk factors\nPosition Issues: Intraday trading not captured, wrong positions\nMarket Issues: Exceptional moves, correlation breakdowns\nBad Luck: Statistically possible even with correct model\n\nCritical Weaknesses:\n\nLow Statistical Power: With only 2.5 expected violations per year, distinguishing good from bad models is difficult\nGaming Incentive: Banks may use conservative models to stay green rather than accurate models\nProcyclicality: Penalties increase capital requirements when losses occur (when capital is scarce)\n\nPractical Reality:\n\nMost banks target 3-4 violations to stay safely green\nModels are often deliberately conservative (overstate risk)\nTrue 99% accuracy is sacrificed for regulatory safety\n\n\n\n4.3.3 Backtesting Expected Shortfall\nES presents unique backtesting challenges because it’s about average tail losses, not just frequency. You can’t simply count violations - you need to assess their magnitude.\nWhy ES Backtesting is Harder:\n\nNo single threshold to check\nRequires modeling entire tail\nMore sensitive to extreme outliers\nLower statistical power\n\n\nAcerbi-Székely Test (2014)\nFirst rigorous ES backtest that checks if average losses in the tail match ES predictions:\n\\[Z_{ES} = \\frac{1}{n \\cdot ES_\\alpha} \\sum_{t: L_t &gt; VaR_t} L_t - (1-\\alpha)\\]\nWhere:\n\n\\(Z_{ES}\\) = Test statistic (should be \\(\\approx\\) 0 if ES is correct)\n\\(n\\) = Number of violations (days when loss &gt; VaR)\n\\(ES_\\alpha\\) = Expected Shortfall prediction\n\\(L_t\\) = Actual loss on day \\(t\\) (positive value)\n\\(VaR_t\\) = Value at Risk on day \\(t\\)\n\\((1-\\alpha)\\) = Tail probability (e.g., 0.05 for 95% confidence)\nSum includes only days where losses exceeded VaR\n\nIntuition: If ES is correct, the average loss when VaR is breached should equal ES.\n\n\nPractical Approaches\n\nJoint VaR-ES Testing:\n\nFirst verify VaR has correct number of violations\nThen check if violation magnitudes match ES\nIf VaR is wrong, ES is likely wrong too\n\nExceedance Residuals Method:\n\nWhen loss exceeds VaR, calculate: \\[e_t = \\frac{L_t - \\text{VaR}_t}{\\text{ES}_t - \\text{VaR}_t}\\]\nWhere:\n\n\\(e_t\\) = Exceedance residual for day \\(t\\)\n\\(L_t\\) = Actual loss (when &gt; VaR)\n\nKey insight: Average of \\(e_t\\) should equal 1.0 if ES is correctly calibrated.\n\n\nConnection Between Mentioned Methods\nNote that both detect the same problem (underestimation) but express it differently:\n\nAcerbi: Absolute deviation from expected tail behavior\nResiduals: Relative scaling of excess losses\n\n\n\n\n\n\n\nES Backtesting\n\n\n\n\nGiven: VaR = $6M, ES = $10M, α = 0.95\nViolations: $8M, $12M, $14M (3 violations out of 250 days)\n\nAcerbi-Székely Test:\n\\[Z_{ES} = \\frac{1}{3 \\times 10} \\times (8 + 12 + 14) - (1-0.95) = 1.083\\]\n\nResult: Z = 1.083 (should be 0 if ES correct)\nInterpretation: Actual average tail loss is higher than ES predicts\n\nExceedance Residuals Method:\n\\[e_1 = \\frac{8 - 6}{10 - 6} = 0.50\\] \\[e_2 = \\frac{12 - 6}{10 - 6} = 1.50\\] \\[e_3 = \\frac{14 - 6}{10 - 6} = 2.00\\] \\[\\bar{e} = \\frac{0.50 + 1.50 + 2.00}{3} = 1.33\\]\n\nResult: Average = 1.33 (should be 1.0 if ES correct)\nInterpretation: Excess losses are 33% higher than ES-VaR gap predicts\n\n\n\n\n\nModel Selection and Validation\nBeyond formal backtesting, practitioners check for these red flags:\n\nViolation clustering: Failures bunch together (model misses volatility dynamics)\nMonth-end spikes: Violations around reporting dates (possible manipulation)\nParameter instability: Small changes → big differences (overfitted model)\nUnexplained losses: Can’t explain big losses after they happen (missing risk factors)\n\nModel Comparison in Practice:\nBanks typically run 2-3 models in parallel and compare:\n\nAccuracy (fewest violations)\nStability (consistent results)\nSpeed (computational efficiency)\n\nMost use simple models for daily reporting and complex models for capital calculations.\n\n\n\n\n\n\nImportant\n\n\n\nKey Insight: The best model isn’t always the most sophisticated - it’s the one that reliably captures your actual risks and can be explained to management.\n\n\n\n\n\n4.3.4 Key Takeaways\nBacktesting Summary:\n\nStatistical rigor required: Not just counting violations\nIndependence matters: Clustering reveals model deficiencies\nBasel framework: Practical but statistically weak\nES challenges: Harder to backtest but necessary\nHolistic validation: Multiple criteria beyond backtesting\n\nBest Practices:\n\nTest at multiple confidence levels (not just 99%)\nCheck for violation clustering\nAnalyze magnitude of exceptions\nRun parallel models for comparison\nDocument all exceptions thoroughly\nRegular model review cycle\n\n\n\n\n\n\n\nImportant\n\n\n\nThe Ultimate Test: “Can you explain to senior management why the model failed when it did?”\nIf not, the model needs work.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculating VaR and ES</span>"
    ]
  },
  {
    "objectID": "week_04.html#references",
    "href": "week_04.html#references",
    "title": "4  Calculating VaR and ES",
    "section": "4.4 References",
    "text": "4.4 References",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculating VaR and ES</span>"
    ]
  },
  {
    "objectID": "week_04.html#practice-questions-and-problems",
    "href": "week_04.html#practice-questions-and-problems",
    "title": "4  Calculating VaR and ES",
    "section": "4.5 Practice Questions and Problems",
    "text": "4.5 Practice Questions and Problems",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculating VaR and ES</span>"
    ]
  }
]